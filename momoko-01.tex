% -*- coding: utf-8 -*-

\chapter{パターン認識と機械学習の「応用」}
% * (アスタリスク)付きの \chapter* コマンドは原則不可とする

\begin{flushright}
 早水　桃子 % ペンネーム
\end{flushright}

\begin{spacing}{0.6}
\noindent
{\footnotesize{本稿は、データサイエンスや統計学の専門家ではない筆者が
The Database Times vol.2のために気儘に執筆し、無責任に寄稿したものである。
本稿で述べる内容は、出典が明記されていない限りは全て
筆者の私見であり、それは他の個人や団体の主張を採用したものではなく、また
各分野の専門家の見識と一致するとは当然限らないということを予め宣言しておく。}}
\end{spacing}
 
\section{来し方行く末}
\subsection{ブームとしての学問}
『解体新書』は、江戸時代の医師たちの試行錯誤によって『ターヘル・アナトミア』というオランダ語の解剖学書から翻訳され、1774年に出版された歴史的書物である。まだオランダ語の辞書すらなかった時代であることを考えると、本文・図表合わせて五冊から成る医学書をわずか数年で翻訳・出版したというだけで大変な偉業である。しかし、この本の功績は、当時鎖国政策をとっていた日本に初めて西洋の医学知識を伝導し、それまでの日本の医学の常識を覆したところにある。
『解体新書』以前の日本においては解剖はほとんど行われていなかったため、人体は「五臓六腑」という東洋医学の概念によって捉えられていた。今でこそ我々は人体を色々な臓器の解剖学的な構造と生理学的な機能が作るシステムとして捉えているが、この枠組みを日本に初めてもたらしたのは、『解体新書』に他ならない。

『解体新書』は医学にとどまらず、ヨーロッパで発展した様々な学問から新しい知識や技術を吸収するという「蘭学（洋学）」ブームの契機になり、近代日本科学史に金字塔を打ち立てることとなった。
しかし、『解体新書』の翻訳者の一人である杉田玄白は、彼が85歳で亡くなる2年前に著した『蘭学事始』（1815年）という回想録の冒頭において、このブームを少々冷めた視線で眺めている。
%蘭学草創期の出来事を正しく伝えるために、85歳で亡くなる2年前の1815年に『蘭学事始』という回想録を著し、その冒頭でこのように述べている。

\begin{quote}
今時（きんじ）、世間に蘭学といふ事専ら行（おこな）はれ、志を立つる人は篤く学び、無識なる者は漫（みだ）りにこれを誇張す。
\end{quote}

時代を問わず、ブームには功罪がある。昨今のデータサイエンスをとりまく流行も、恐らく例外ではない。『解体新書』が現代医学への道を切り拓き、やがて蘭学が社会現象の域にまで達したように、
初の網羅的な解剖学アトラスのような『パターン認識と機械学習』の大ヒットもまた、機械学習・データマイニングといった分野の大流行を物語る。
ジョン・ネイスビッツは『メガトレンド』（1982年）という著作の中で「私たちは情報の海に溺れ、知識に飢えている」と見事に評したが、
%We are drowning in information but starved for knowledge. 
「情報爆発」や「ビッグデータ」へのソリューションが声高に語られ、それに煽られるかのようにバズワードに群がる烏合の衆は今日も騒がしいツイートをやめない。

無論、目新しい分野に期待を寄せるのは当然あるべき知的好奇心に違いないのだが、
パターン認識と機械学習の学習\footnote{特定の書籍に言及しているわけではない、と白々しくトボケておく。}をしなければ何となく乗り遅れたような気分にさせられたり、とりあえず機械学習で何かをすればそれが何であっても人目を引くことが出来たり、データの海に溺れる者にデータマイニングを謳った藁を差し伸べる有象無象の輩が現れるのは、最先端の花形的研究分野の活気と言うよりはむしろ、ゴールドラッシュ\footnote{金が採掘された場所に一攫千金を目論む採掘者が殺到すること。1848年にカリフォルニアの川で砂金が発見されたことによるカリフォルニア・ゴールドラッシュが有名であるが、金の鉱脈が見つかるたびに世界各地で幾度となく繰り返し見られる現象である。}を彷彿とさせる熱病と呼ぶべきものだろう。

\subsection{メルトダウンするブームと心中しないために}
人並みの繊細さを持ち合わせている人間であれば、その熱気にのぼせて付和雷同してはいられないはずだ。現在のデータサイエンスをとりまく熱気は単なる空騒ぎに浪費されるかもしれないし、これまでに提案されてきた数々の理論や技法は、現実の問題解決に対する有用性や意義が不明瞭なまま忘れ去られ、それを実装したコードも無用の長物となるかもしれない。データ解析に求められるソフトウェア・ハードウェアに投資しても望む結果が何も得られなかった場合、ユーザーの少々無理のある過剰な期待はやがて失望に変わり、それが集団的な絶望へ変わる時にブームは終焉を迎え、最悪の場合は学問分野そのものが廃れていくかもしれない。

また、新しい考えや技術は受け入れられるどころか脅威と見なされて排除されたり、新たな問題を生み出すことも少なくない。実際、蘭学ブームは蘭方医学と漢方医学の深刻な対立を招き、蘭学は蘭書翻訳取締令などによって思想弾圧の憂き目を見ることになったし、産業革命は機械による自動化・作業効率化をもたらした一方で、失業を恐れた労働者集団は機械を破壊するという行動に出た\footnote{この機械打ち壊し運動（ラッダイト運動）になぞらえて、雇用を不安定にするIT化・自動化に反対する思想はネオ・ラッダイトと呼ばれている。}し、産業廃棄物の処理という新たな問題を生じさせた。

だからといって、ブームに対する危機感に振り回されて時代の潮流に乗ることを拒絶するのも賢明ではないだろう。したがって、本稿において貫きたいスタンスはまさしく「自ら『濁れる世』の只中をうろつき、危険に身をさらしつつ、しかも、批判的な姿勢を崩さぬことである。対象と深くかかわり全面的に没入すると同時に、対照を容赦なく突き放し切って捨てること。同化と異化のこの鋭い緊張こそ、真に知と呼ぶに値するすぐれてクリティカルな体験の境位であることは、いまさら言うまでもない。簡単に言ってしまえば、シラケつつノリ、ノリつつシラケること、これである」\footnote{浅田彰の『構造と力―記号論を超えて』（1983年）のあまりにも有名な一節。}。

持て囃されている道具を信仰したり、道具を使うこと自体が目的になってしまったり、道具を数式やプログラムの上だけで理解したり、あるいは実装して再生産しただけで満足してしまったり、どの道具が優れているのかという格付けに一喜一憂したりするような態度は全てこのスタンスに反する。肝に銘じておくべきは、どんなに高尚な理論や技法も、現実世界の問題に挑む人間にとっては目的を叶えるために使える道具の一つにすぎず、目的のために道具を使う人間があくまでも主役ということである。道具の長所や短所や使い道、他の道具とどのように関わると有用か、どのような道具がどのような局面で必要とされているのかは、ショーケースの中の道具を鑑賞しているだけではちっとも分からず、実際に使ってみて漸く見えてくるものである。

これはあまりにも当然なことであるのだが、データベースからの知識発見\footnote{Knowledge discovery in databasesの頭文字を取ってKDDとも呼ばれる。}という舞台を観客として眺めていると、大規模データベース・統計学・機械学習という華やかな役者たちが全てであるかのように錯覚しがちである。しかしそうした「道具」たちは何ができて何ができないのか、どのように互いに関わり、どこでどのように応用すれば現実世界で真価を発揮するものになるのかというストーリーは、
人間や社会の問題意識やゴール設定なしに始まるはずがないし、もちろん終わるはずもない。

\section{データマイニング再考}
まずはJerome H. Friedmanの"Data mining and Statistics: What's the connection?"を下敷きにして、データマイニングとは何であるのかを
批判的に捉え直してみよう。

\subsection{データマイニングとデータベース}
データから知識を得ようとするというのは、全然目新しくない。では、何が今更データマイニング「特需」をもたらしたのだろうか。この問いかけに答える鍵の一つは、データベースマネジメントシステム（DBMS）の変遷である。
%Imielinski（1995年）

かつてのデータベースマネジメントシステムにおいては、銀行のATMで利用されているようなオンライントランザクション処理（OLTP）と呼ばれる情報処理が主流であった。つまり、データベースマネジメントシステムに求められていたのはデータを貯蔵し、多くのユーザーからの定型クエリーを高速に処理し、小さなデータを高速に読み書きして更新するという能力だった。
%http://otndnld.oracle.co.jp/deploy/performance/pdf/Oracle8i-tuning2.pdf
しかしデータベースに蓄積されたデータを様々な切り口で分析したいエンドユーザーたちが
データベースに意思決定支援を求めるにつれて、データベースマネジメントシステムにおいて
オンライン分析処理（OLAP）が注目を集めるようになった。OLAPは、ユーザーが対話式の試行錯誤を繰り返すことによって、データウェアハウスに格納された膨大なデータを集計したり、多次元的に分析したりすることを可能にした。

一見するとこれはデータマイニングツールの原型のように見えるかもしれない。しかし、OLAPによるデータ解析によってデータウェアハウスと「対話」できるのは、的確な仮説を考え続けるような不断の洞察力とクエリーを書き続ける技術力（および時間と体力）を持ち合わせた選ばれし人間だけである。データに潜むパターンやモデルの叩き台を自動的に見つけてくれるものではないし、ビジネスの場における迅速な意思決定に向いているものでもない。だからこれは
データマイニングツールの原型と言うよりはむしろ、人々がデータマイニングツールを求める素地を作ったと考えたほうが良いだろう。
%大規模なデータからパターンを見出す試行錯誤のプロセスが人間の手を煩わせずに実現するならばそれが効率的であるし、data analysis for everyoneみたいなものへの需要は高まっていった。

\subsection{プロパガンダとしてのデータマイニング}
データマイニングもまた他のバズワードの例にもれず定義が曖昧な言葉であるが、
複雑で膨大なデータ集合の中に埋もれていて人間の力では見いだせないような有益な知識をコンピューターの力でデータベースの中から探索的に発見し、ビジネスにおける意思決定に役立てるような色々な手法（ないし、それらを使ったプロセス）という意味で専ら使われており、
多くの場合においてはIBM Intelligent miner、SAS Enterprise Miner、SPSS Clementineなどの汎用データマイニングツールを使うことによって実践されるものである。

データマイニングの目論見は壮大なものであるが、
%データから新たな知見を得ようとする試みは全然斬新なものではない。
ユーザーが小難しいデータベースの構造やデータ解析の中身を知らなくてもデータベースに格納されたデータにアクセスし、それを分析したり視覚化することを可能にするというデータマイニングツールというものは、
誰のためのものなのだろうか。
「データ自身に語らせる」といえば聞こえは良いが、ビジネス戦略のヒントとなるような知識を囁くかどうかは別の話であるし、あまりそういううまい話は聞かない。Friedmanが指摘するように、汎用データマイニングツールは非常に高額な商用ソフトウェアであり、導入するとなるれば当然相応の高価なハードウェアやストレージも要求する。その意味で、データマイニングはもはや金脈を掘り当てようとする宝探しのような試みと言うよりはむしろ、データマイニングと聞いてソフトウェアやハードウェアに投資してくれるような、データを持て余している企業の意思決定者や潜在的データマイナーを発掘するための商業的プロパガンダと化しているところがある。

\subsection{データマイニングと統計解析の違い}
Friedmanは汎用データマイニングツールが提供する主要な手法の数々を
%\footnote{Decision tree induction・rule induction・nearest neighbors・clustering・association rules・feature extraction・visualisation、neural networks・graphical models（Bayesian belief networks）・遺伝的アルゴリズム・SOM（自己組織化マップ）・neuro-fuzzy systemsと}
を列挙し、多くのデータマイニングツールにおいて
仮説検定や判別分析など、統計学者にとって見慣れた手法は
殆ど無視されていると評している。一言で「データを解析する」と言っても、データマイニングと統計解析は目的も手段も大きく異なるものであり、両者を混同してはならない。

統計学的なデータ解析は、「データ自身に語らせる」という大らかなものではない。
「実験が終わった後に統計学者にコンサルトするのは、死後に解剖を依頼するようなもので、統計学者はその実験の死因を教えることぐらいしかできない」というロナルド・フィッシャーの言葉の通り、
そもそも統計学者にとってはデータは綿密な実験デザインに基づいて注意深く収集されたり吟味されるべきものであり、
鉱山のように元々そこにあるものではないのだ。
それに、データを説明するための仮説やモデルがなければ、仮説検定\footnote{帰無仮説を棄却するか否かという仮説検定は無味乾燥なものだと思うかもしれないが、少なくとも学術研究の分野においては最もよく使われる意思決定アルゴリズムである。}もモデル選択\footnote{モデルのパラメータを最尤推定する「モデル推定」とともに、どのモデルを選ぶべきかという「モデル選択」は機械学習（特に教師あり学習）においては最も重要なテーマの一つである。モデル選択基準はAICやBICなど色々なものがあり、クロスバリデーションもよく使われる。}も始めようがない。だから、良質なデータと洗練されたモデルこそ、統計学的なデータ解析の要であるのだ。その営みにおいてアルゴリズムというのはそのモデルを採用するか否かの基準を計算するための方法にすぎない。だから、モデルが正しければ良い結果を出し、正しくなければ悪い結果を出すのがここでのアルゴリズムのあるべき姿であるし、解析結果の善し悪しはアルゴリズムではなく、モデルが決めていると考えるのが筋である。

これに対しデータマイニングは、良く言えば従来の統計学が扱えないような整理されていない大規模データを何とか使える形にし、色々な切り口で分析して意思決定のヒントになる知識を見出そうとする果敢な取り組みであり、悪く言えば産業廃棄物のような膨大なデータの山をゴミだと認めるわけにいかないからといって、往生際悪くデータのフォーマットを整え、クレンジング\footnote{データの誤りや重複を洗い出し、使えるデータにするという泥臭い（しかし最も重要な）プロセス。}を施して場当たり的に試行錯誤することにより、有象無象の仮説やモデルの叩き台をあれこれ捻り出すという作業である。

いずれにせよデータマイニングは統計学とは全く異なる思想を持つものであり、仮説やモデルなしに鉱山の中を探索するデータマイニングにおいてよく登場する手法が、統計学の手法と異なるのは驚くべきことではないし、データマイニングが語られる文脈においてはテクニックやアルゴリズムありきの話になりがちであるのも、当然の成り行きなのかもしれない。

\subsection{データマイニングの意義}
しかしながら
%データマイニングに失望したり、ベンダーやコンサルティングファームに対して猜疑的になったり、
データマイニングの努力は水泡に帰するのかと悲観する必要はないし、それは本稿における我々のスタンスではない。むしろそこでの努力を無にしないために、試行錯誤の中に埋もれた教訓を探し、データマイニングが人間にとって有用であるためにはどうあるべきかを考えなければならない。

データマイニングは大規模なデータセットになるほど前処理が重要であることを実証してきたし、そのためのプラクティカルな方法を確立してきた。データの可視化は迅速な意思決定の取っ掛かりとして確かに強力であるし、現にそういったツールが多くの人々に求められているということは、人間が扱わなければならないデータが確かに複雑になっているということを物語っている。
また、様々なテクニックを集めたツールとデータ解析の専門家だけでは解析結果は荒唐無稽なものになりがちであったが、それはデータマイニングは役立たないと切り捨てる理由にはならず、むしろデータ解析には現場の知識を持った専門家の知識が必須であるということを改めて思い知らせてくれたと考えるべきであるし、その方がよほど建設的である。

また、大規模データベースを持っているのは企業だけではないし、
後述するように公開されている大規模なデータベースも数多く存在する。
そのようなデータベースから知識を発見する試みにおいて、データマイニングは単なる商業的なプロパガンダに堕したものではなく、真理の探求を目指す学術研究において仮説やモデルのプロトタイプを見つける手段となったり、
蓄積されたデータから合理的に最適な政策を決定するという国家や国際社会の意思決定を支援するためのツールとなり得るものである。

データマイニングは、アソシエーションルール・クラスタリング・決定木学習など、実に様々なパターン認識・機械学習の理論的な枠組みを現実の大規模データに応用してきた。そうした理論と現実の橋渡しをする取り組みは軽視されてはならないし、その価値は商業的な実用性や理論的な厳密さや一般性だけで評価されるべきではない。そうでなければデータマイニングバブル崩壊とともに、データベースから知識を発見することの重要性も失われかねないからだ。

\section{データベースの外の世界}
データベースのことばかり考えていると、統計学もパターン認識も機械学習もデータベースを掘り起こす道具にしか見えなくなるかもしれないから、それ以外の側面についても少しだけ触れてから先へ進むことにしよう。
\subsection{医療機器}
統計学や機械学習で登場するアルゴリズムの幾つかは、医療の現場で人々の健康な生活や生命を支えている計算手法と言っても過言ではない。医用画像を得るためには画像再構成という重要なプロセスがあり、そこではバックプロパゲーションやEMアルゴリズムといった機械学習・統計学でよく知られる手法が一般的に用いられている。
また、病気の検査のためだけでなく、
がんの放射線治療においてはモンテカルロ法が古くから線量分布推定に用いられ、
今もなお放射線治療計画に必須の計算手法であり続けている。

医療現場で使われる装置やプログラムに関わるごく一部の例であるが、
統計学や機械学習の、データマイニングで期待される役割とは少し違う一面を窺い知ることが出来るだろう。

\subsection{神経科学}
もともと機械学習は人工知能研究の一分野であることから分かるように、理論神経科学と深く関係しており、その理論的な枠組みそのものが、人間の脳のメカニズムや、社会における人間の行動などを理解する一助となり得るものである。パーセプトロンは古くから小脳のモデルとして知られ、人間の運動学習の神経基盤を理論的にも実験的にも明らかにし、ロボットの運動制御という工学的な応用にもつながることとなった代表的な例である。
より最近の例としては、やや趣味的になるが、ドーパミンニューロンが報酬予測誤差によって強化学習を行っているのではないかという仮説が提唱され、強化学習の中でも特にTDモデルが大脳基底核の計算モデルとして注目を集め、報酬に基づく意思決定の神経基盤が理論的にも実験的にも精力的に研究されている\footnote{そしてその実験のためには医用画像処理・画像の統計解析が必要不可欠なのである。}。これらは薬物やギャンブルなどの依存症と密接に関係し、今後は人間の衝動制御にもつながる研究がますます進むと思われる。

こうした人間の脳につながる研究はロマンティックで興味深く、また実際に有益でもあるのだが、本稿ではもう少し現実的で差し迫った問題を題材にして、データベースやデータマイニング・機械学習・統計学について考えていこう。

\section{人生の明暗を分ける}
%日本人の平均寿命は世界の先進国の中でもトップクラスであり、それは高度に発達した現代の医学と、充実した医療サービスを誰でも受けることが出来るようにするための国民皆保険制度の賜物である。しかし
周知の通り、日本を筆頭に世界各国（特に先進国）では高齢化が社会問題となっている。増え続ける認知症患者は医療費の増加により国の財政を圧迫し、若い世代にも大きな負担を強いることになっている。これは社会を悩ませるだけではなく、個人の人生を脅かす問題である。

日本では現在既に65歳以上の高齢者のうち約10\%が認知症であると見積もられており、しかも今後さらに増加していくと推定されている。
つまり、我々のうち少なくとも10人に1人は認知症患者になる。生涯現役が当たり前になりつつある中で自分の本来のパフォーマンスを発揮できなくなっていくのは個人にとっても悲劇であるし、もちろんその個人を取りまく人々の人生にもかなりの打撃を与える。

\subsection{認知症：背景知識の要点}
認知症というのは単一の疾患ではなく、認知機能低下を呈する多くの疾患の総称である。その中でも一番多くて重要なのが、誰でも一度は聞いたことがあるであろうアルツハイマー病だ。

\subsubsection{アルツハイマー病はハードウェアの異常である}
コンピュータの調子が悪くなったら、まずはハードウェアレベルの異常なのかソフトウェアレベルの異常なのかを考えるのが自然な流れであろう。それと同様に、長年使ってきた脳がうまく動かなくなる場合も、原因はソフトウェアのレベルとハードウェアのレベルとに分けられる。

認知「機能」の障害と聞くとソフトウェアレベルの問題のように思うかもしれないが、アルツハイマー病は正常な脳の「構造」が壊れることにより機能に支障が出る病気で、ハードウェアの著しい劣化による脳の故障と考えるべきものである。具体的には、正常な人でも加齢で蓄積するゴミのようなもの\footnote{ゴミの正体について少しだけ詳しく述べれば、アルツハイマー病は脳内に$\beta$アミロイドとよばれるタンパク質が蓄積し、異常リン酸化タウタンパクが蓄積してしまうことで神経細胞の変性・機能不全が起こり、神経細胞死を招く疾患である。それが肉眼的・画像的な脳萎縮という初見となる。}が異常に脳に溜まってしまうために神経細胞がどんどん死んでしまい、脳のボリュームが目に見えて減ってしまうのである。

\subsubsection{アルツハイマー病の根本治療薬とワクチン開発への動き}
ハードウェアの障害であると繰り返したのは、病気が進行して重症化してからでは有効な治療法がないことと、MRI画像で脳の形やボリュームを見ることが診断に有効であることの二点を強調するためである。
壊れかけの初期のうちに食い止めることが出来るなら画期的であるし、もっと言えば最初から認知症症状が出ないように出来たら理想的である。現在多くの巨大な製薬会社や医療機器メーカーがベンチャー企業とも協力し、アルツハイマー病の根本治療薬やワクチン開発を目指して研究開発に取り組んでおり、実際に日本でも複数の治験が進行中である。

\subsection{ADNIデータベース}
根本治療薬・ワクチン開発への動きとともに、脳の異常の客観的な評価方法や、早期診断手法の重要性は急速に高まった。
2004年にアメリカはUS-ADNI（米国アルツハイマー病神経画像イニシアチブ）をスタートさせた。
これは認知症研究を推進するための大規模な公開データベースを構築するという、
開始当初の予算だけでも6000万ドルという規模の国家的な臨床研究プロジェクトである。

US-ADNIは800人以上の被験者を6ヶ月毎にスキャンし、
経時的な画像データ
\footnote{
MRIの構造画像だけでなく、FDG-PETという機能画像や発症前診断のためのPiB-PETによるアミロイドイメージングも含む}のみならず、認知機能検査の点数や色々な生物学的マーカー（重要なタンパク質の遺伝子型など）、臨床的な因子などの様々な情報をデータベース化している。
医用画像データの質は施設ごとの撮像方法や装置そのものに大きく影響されるため、多施設から得られたデータをそのまま解析することは出来ないのだが、ADNIでは
撮像された画像データはそのような違いを補正するための前処理を施され、匿名化された後、翌日にはwebサイトにアップロードされ、世界中に公開される。

このADNIというイニシアチブはヨーロッパ・オーストラリア・日本・台湾・韓国でもスタートし、日本でもJ-ADNIという名前で2007年からプロジェクトがスタートしている。
US-ADNIはさらにアルツハイマー病の早期診断・発症予測に焦点を当てたADNI 2に乗り出し、また被験者の全ゲノムシークエンスをも公開するビッグデータプロジェクトに変貌しつつある\footnote{もし800人の被験者の全ゲノム配列が得られたらデータベースに追加されるデータのサイズは少なくとも165テラバイトになると見込まれている。}。

\subsection{ADNIデータベースと機械学習}
US-ADNIのデータは医師や研究者ではなくても、誰でも自由に無料でダウンロードすることができる。
つまり、医療と何ら関係のないデータマイニングや機械学習の研究者であっても、最先端の検査を駆使して得られた大規模な患者データを得ることが出来るのである。データはどのように利用しても良いし、「データベースからの知識発見」に成功したならば当然それを自由に論文として発表することが出来る。アルツハイマー病の画像診断研究に機械学習分野の研究者が続々と参入しているのはそのためであり、ADNIデータベースはそうした学際的な研究の基盤になっているのだ。

機械学習を応用するという視点で見れば、確実に正しい教師データが入手できるというのも特筆すべき点である。実は日常的に行われている臨床診断は、厳密には100\%正しいラベル付けとは限らない。それはアルツハイマー病を確定診断をするためには顕微鏡で実際に「ゴミ」が溜まっているのを見なければならないため、手術室で生きている脳から組織のサンプルを採取することが出来た場合を除き、真の答えは死後の解剖なしには確かめようがないからである。ADNIは人間の医師が付けたラベルだけでなく、判明している限りの「真のラベル」も公開している。だから訓練データの中にあるかもしれないノイズに悩まされることなく、安心して教師あり学習アルゴリズムを使うことが出来る。

そのようなわけでこの分野への機械学習の応用は枚挙に暇がないのだが、
代表的なものを幾つか紹介しよう。
\subsection{機械学習の応用}

\subsubsection{どのアルゴリズムが良いのか}
2008年にStefan Kloppelらはアルツハイマー病のMRI自動診断という論文を発表した。これはサポートベクターマシン（SVM）はMRI画像だけで重症なアルツハイマー病の患者と健康な高齢者を95\%ほどの感度・特異度で分類できたというものであった。つまり、非常に高次元なMRI画像を数万次元の長いベクトルとみなしてその内積を計算して得られる類似度から病気か否かを判断するだけのアルゴリズムが、認知症の症状の度合いや年齢といった情報を一切使わずして認知症を診断出来たのである。

ADNIデータベースは、そうして提案された手法の性能を同一データセットで追試し、比較できるというメリットもある。Kloppelらの研究を含め、色々な手法を提案する論文が発表されたのだが、Cuingnetらによるメタアナリシスによれば、アルツハイマー病患者と健康な高齢者をMRI画像から分類する問題では結局はどうやらシンプルな線形カーネルのSVMが良いアルゴリズムだということになった。これは苦労して手法を改良しようとした人間にとってはショッキングな話である。

脳MRI画像は非常に高次元なデータなので、特徴選択に改良の余地があるかもしれないと思われたのであるが、脳の領域を予め選択すると汎化能力が低くなるという問題があり、結局分類の正確さを上げようとすると脳の領域全ての情報を使うに越したことはなかった。それに、訓練データから重要な領域を学習させようとすると、時には数週間の計算を要するほどに計算量がやたらと増えるばかりであった。カーネル関数の選択についても色々と試行錯誤されたのであるが、結局は線形カーネルに落ち着くことになった。

これはショッキングな結果であるが、複雑なアルゴリズムが無力だったのではなく、むしろMRI画像は非常に高次元で多くの情報を含んでおり、また画期的な前処理のアルゴリズムが登場したことで、重症アルツハイマー病と健康な高齢者の識別は全脳の情報を全て使った線形SVMで充分になったと考えるべきである。

しかしあまり重症ではないアルツハイマー病と健康な高齢者の分類となると、どの手法でも途端に感度が落ちてしまった。ましてや
まだ認知症を発症していない人が将来発症するかの予測となると、どの手法もランダムと同程度になってしまった。

何が問題なのか。工夫のしどころはどこなのか。
まず、これらの研究はMRI画像しか使っていないというのが問題である。
MRIだけでなくPETや認知機能検査など多様なモダリティを統合したいと考えるのは必然であろう。
また、一時点のデータしか使っていなかったというのも問題であり、萎縮の経時的変化も考える必要があるだろう。
それから、どうしても感度や特異度の\%を上げることに苦心してしまいがちではあるのだが、
複雑な脳の画像データを全部使うのではなくて、やはり解釈しやすくシンプルであるために、本質的な領域を選び抜きたいのである。

Zhangらの研究はこのような動機に基づいている。つまり、多様な情報源を統合して発症前のアルツハイマー病を見出したい。
%さらに野心的なことは将来の認知機能を推定するというところである。
L2,1ノルム正則化によって
どの時点でも重要になる脳の領域をスパースな解として得る。

画像をｔ（時点）の多項式と考え、直交多項式展開の係数を特徴ベクトルに含めることで
萎縮の速度（ｔの一次式）だけではなく非線形な特徴も抽出する。
多様な情報源の統合
MRIだけでなくPETや認知症検査など、様々なモダリティのデータを最大限に活用するために
モダリティごとにカーネル行列を作り、それらを凸結合したものを分類・回帰に使う
マルチカーネルSVM。これは
それぞれのカーネルの重み付けも最適化するし、凸最適化なので大域的最適解が求まるという良さがある。

MCIと診断された人が将来どうなるかの予測は一時点のMRIだけでは困難だったが、訓練された人間の医師のように、
構造画像だけではなく機能画像やカルテに散らばった様々な情報を統合し、現在までの経時的な変化を辿ることが必要そうであった。
経時的変化の特徴も抽出し、様々なモダリティの検査データを統合することで6ヶ月後に発症するかどうかを80\%近い感度・特異度で予測できた。

発症するかどうかだけでなく将来のスコアの推定もある程度できるようになった。
6ヶ月ではなくもっと早期に発症することが分かれば、早期の薬物療法開始によって更に効果的な予防につながるかもしれないのだ。

%その結果、時系列のデータを使えば、特異度は78%のままでどの手法でも50～60%台だった感度が79%に！
%MRIやPETや認知機能検査の情報は互いに相補的なのでそれぞれの検査で得られる情報を統合することでより正確な予測ができる！
%MRI・PET・認知機能検査・CSFデータなど各モダリティからの情報は相補的なので
%それぞれのカーネル行列を組み合わせれば
%より良いカーネル行列が得られる
%
%\subsubsection{matome}

%
%MRI・PET・認知機能検査だけでなくCSF中のAβなど他のバイオマーカーを使うともっと正確に予測できそう
%全てのモダリティの検査を受けた患者はADNIデータベースにもまだ少ないので今後のさらなる充実に期待
%
%
%
%%Automatic classification of MR scans in Alzheimer's disease
%%Brain (2008), 131, 681-689
%
%
%%とはいえアルツハイマー病の臨床診断は日常的に滞りなく行われていて、殆どの臨床診断は真の答えと一致している。そもそも大抵の専門家は、症状や経過のパターン\footnote{本人を診察したり家族の話を聞いたり認知機能を調べる簡単なテストをして認知機能をスコア化したりする中で明らかになる。}を知った時点でアルツハイマー病かどうかをあらかた察することができるのだが、病態についての生物学的・医学的な研究が進んだ現在、様々な検査によって非常に多くの重要な情報を得ることが出来るようになったからである。中でも、脳というハードウェアの形態を見るMRI、目に見えない脳機能をラジオアイソトープによって画像化するというSPECTやFDG-PET、または脳に溜まった「ゴミ」自体を視覚化するPiB-PETなどによる画像診断研究の進歩は目を見張るものがある。
%
%
%%診断において厄介なのは、認知機能検査の点数が正常とも異常ともつかないような「ボケ気味」の人々であり、こういう白とも黒ともいえないグレーな人々は軽度認知機能障害（MCI）であるのだが、MCIと診断された人々を経時的に追跡してその運命を調べると、決定的に異なる2種類のグループに分かれることが明らかになった。年月を経てもあまり変化がなく少々ボケ気味とはいえ平穏な人生を送ることができるグループと、MCIと診断されてからみるみるうちにアルツハイマー病の診断基準を満たすレベルになり、数ヶ月～数年で変わり果ててしまうグループである。前者はMCI non-converterと呼ばれ、後者はMCI converterと呼ばれる。グレーの中には黒になりつつあるグレーが隠れていたということだ。


\subsection{脳画像統計解析と診断支援ソフトウェア}
実は、自動診断ソフトウェアは日常的に使われており、
日本ではVSRAD\footnote{健常高齢者のデータベースからどのぐらい離れているかということをボクセル毎に検定してZ値を算出し、自動的に診断をする。}という無償のソフトウェアが普及し、1000以上の施設でルーチン化している。
