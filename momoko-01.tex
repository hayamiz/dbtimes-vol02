% -*- coding: utf-8 -*-

\chapter{パターン認識の「構造と力」---逃走論を超えて---}
% * (アスタリスク)付きの \chapter* コマンドは原則不可とする

\begin{flushright}
 早水　桃子 % ペンネーム
\end{flushright}

\begin{spacing}{0.6}
\noindent
{\footnotesize{標題（と傍題\footnote{傍題もやはり浅田彰の『逃走論---スキゾ・キッズの冒険』に因んでいる。}）が示すように、本稿は浅田彰の『構造と力---記号論を超えて』へのオマージュとして書かれたものである。質の悪いパロディと言っておくのが分相応かもしれないが、いずれにせよ1985年生まれの筆者が1983年の浅田彰にやや過剰にインスパイアされてしまったことについては否定のしようもない。しかし本稿で述べる内容は、出典が明記されていない限り全て著者の（現時点での）私見であること、それは他の個人や団体の主張を採用したものではないということ、そして各分野の専門家の見識と一致するとは当然限らないということだけは予め強調しておこう。

\noindent
この試論の構成は、次のようになる。まずは歴史の中から一世を風靡した学問の例を引っ張り出し、本稿のスタンスを明らかにすることから始める（第1節）。そのスタンスでデータマイニングというものを批判的に考察し（第2節）、データ解析以外の領域における機械学習の姿を少しだけ眺めてから（第3章）、人間の脳は壊れるという現実に直面する。最先端の医学研究（第4節）と日常の医療現場（第5節）の問題意識の中で大規模データベース・機械学習・古典統計学の諸概念を捉え直す。その先にあるのはデータベース・機械学習・古典統計学という枠組みを否定したときに何が残るのかという問いであり（第6節）、本稿の目論見はこの問いに対する答えの一つを提示することにある（第7・8節）。
}}
\end{spacing}
 
\section{来し方行く末}
\subsection{無識なる者たち}
『解体新書』は江戸時代の医師たちの試行錯誤によって『ターヘル・アナトミア』というオランダ語の解剖学書から翻訳され、1774年に出版された歴史的書物である。まだオランダ語の辞書すらなかった時代であることを考えると、本文・図表合わせて五冊から成る医学書をわずか数年で翻訳・出版したというだけで大変な偉業である。しかしこの本の偉大さは、当時鎖国政策をとっていた日本に初めて西洋の医学知識を伝導し、それまでの日本における医学の常識を覆したところにある。
『解体新書』以前の日本においては解剖はほとんど行われていなかったため、人体は「五臓六腑」という東洋医学の概念によって捉えられていた。今でこそ我々は人体を色々な臓器の解剖学的な構造と生理学的な機能が作るシステムとして捉えているが、人体の構造を捉える解剖学という枠組みを日本に初めてもたらしたのは、『解体新書』に他ならないのである。

『解体新書』は医学にとどまらず、ヨーロッパで発展した様々な学問から新しい知識や技術を吸収するという「蘭学（洋学）」ブームの契機になり、近代日本科学史に金字塔を打ち立てることとなった。
しかし、『解体新書』の翻訳者の一人である杉田玄白は、彼が85歳で亡くなる2年前に著した『蘭学事始』（1815年）という回想録の冒頭において、このブームを少々醒めた目で眺めている。
%蘭学草創期の出来事を正しく伝えるために、85歳で亡くなる2年前の1815年に『蘭学事始』という回想録を著し、その冒頭でこのように述べている。

\begin{quote}
\ruby{今}{きん}\ruby{時}{じ}、世間に蘭学といふ事専ら\ruby{行}{おこな}はれ、志を立つる人は\ruby{篤}{あつ}く学び、無識なる者は\ruby{漫}{みだ}りにこれを誇張す。
\end{quote}

ここで唐突に、杉田玄白の遺した言葉の「蘭学」を「機械学習」に置き換えるという悪ふざけを試みる。皮肉なことに、このバカバカしい操作により「無識なる者」という言葉が急に現実味を帯びてくる。
%バズワードに煽られる烏合の衆を「無識なる者」と批評することこそ、「無識なる者」という言葉に冷や汗をかいていることの裏返しなのではないだろうか。
人体を切り刻んで臓器や組織ごとに描写する解剖学アトラスの構造は、数多くの理論や方法を網羅的に記述することのようでもある。この文脈で『解体新書』に重なるのは、やはり『パターン認識と機械学習』\footnote{本稿の読者には説明不要であろうが、"Pattern Recognition and Machine Learning"(C. Bishop)を邦訳した教科書である。因みに「ベイズ理論による統計的予測」という副題が添えられている。}だろう。「無識なる者」の外側にいるのだと信じようとするほど、内側に引き摺り込まれる------そんな気分にさせられる。


「情報爆発」や「ビッグデータ」へのソリューションが声高に語られる2012年の状況は、
John Naisbittが"We are drowning in information but starved for knowledge."（我々は情報の海に溺れ、知識に飢えている）\footnote{John Naisbitt "Megatrends"(1982)より引用。}と評した1982年から変わったとはお世辞にも言えないはずだ。そして残念ながら杉田玄白が苦言を呈した1815年からも、多分大きな変化はないのだろう。

言うまでもなく、目新しい分野に魅力を感じるのは当然の知的好奇心である。いただけないのは「無識なる者」が多数派になることである。それは最先端の花形的研究分野の活気ではなくて、ゴールドラッシュ\footnote{金が採掘された場所に一攫千金を目論む採掘者が殺到すること。1848年にカリフォルニアの川で砂金が発見されたことによるカリフォルニア・ゴールドラッシュが有名であるが、金の鉱脈が見つかるたびに世界各地で幾度となく繰り返し見られる現象である。}を彷彿とさせる熱病と呼ぶべきものである。

\subsection{メルトダウンするブームと心中しないために}
もっとも、人並みの繊細さを持ち合わせている人間であればその熱気にのぼせて付和雷同してはいられないはずだ。現在のデータサイエンスをとりまく熱気は単なる空騒ぎに浪費されるかもしれないし、これまでに提案されてきた数々の理論や技法は、現実の問題解決に対する有用性や意義が不明瞭なまま忘れ去られ、それを実装したコードも無用の長物となるかもしれない。データ解析に求められるソフトウェア・ハードウェアに投資しても望む結果が何も得られなかったとしたら、ユーザーの少々無理のある過剰な期待はやがて失望に変わり、それが集団的な絶望へ変わる時にブームは終焉を迎える。最悪の場合は学問分野そのものまで廃れていってしまうかもしれない。

蘭学はそうして廃れていってしまったのだ。一世を風靡した蘭学は蘭方医学と漢方医学の深刻な対立を招き、蘭書翻訳取締令により思想弾圧の憂き目を見ることになったのである。もっとストレートな例を挙げよう。
産業革命は機械による自動化・作業効率化をもたらした一方で、失業を恐れた労働者集団に機械破壊という行動をとらせてしまった\footnote{この機械打ち壊し運動（ラッダイト運動）になぞらえて、雇用を不安定にするIT化・自動化に反対する思想はネオ・ラッダイトと呼ばれている。}し、産業廃棄物という未解決の問題を山積みにしてしまった。新しい考えや技術は受け入れられるとは限らないのだ。優れたものが排除されることもあるし、新たな問題を生み出す元凶になってしまうこともある。
%人間が生み出した新しいものを活かすのも人間であり、殺すのも人間であるということを心に留めておきたい。

とはいえ、ブームに対する危機感に振り回されてもいられない。時代の潮流に乗るための必要条件とは何だろうか。『構造と力』から立て続けに引用するなら、「自ら『濁れる世』の只中をうろつき、危険に身をさらしつつ、しかも、批判的な姿勢を崩さぬこと」である。それは「対象と深くかかわり全面的に没入すると同時に、対照を容赦なく突き放し切って捨てること」で、「簡単に言ってしまえば、シラケつつノリ、ノリつつシラケること、これである」。

スタート地点における我々のスタンスが決まったところで肝に銘じておくべきことがある。理論や技法がどれほど高度なものであろうとも、現実世界では人間の目的を叶える手段に過ぎないということである。目的を設定するのも人間であり、道具を使うのも人間である。だから道具の格付けに一喜一憂したり、道具を使うこと自体が目的化したり、目的なしに道具を理解するだけで満足してしまうという態度はいずれも好ましくないわけだ。
色々な道具を集めた道具箱をうまく使いこなすためには、道具箱をショーケースみたいに扱っていてはいけない。現実の世界に道具をあれこれ持ち込んで、道具の長所と短所、他の道具との相性、適材適所となるような合理的な使い方などを模索する必要がある。

これは当たり前のことなのではあるが、データベースからの知識発見という舞台を観客として眺めていると忘れがちなことでもある。大規模データベース・機械学習・統計学という華やかな役者たちが全てであるかのように錯覚してしまうと、そうした「道具」たちは互いにどのように関わるべきで、どこでどのように応用されるのが良いのかという話は混沌を極めることになる。そうしたストーリーは人間や問題意識や目的の設定なしに始まるはずがなく、もちろん終わるはずもない。

\section{データマイニング再考}
まずはJerome H. Friedmanの"Data mining and Statistics: What's the connection?"(1997)を下敷きにして、データマイニングとは何であるのかを考えてみよう。

\subsection{データマイニングとデータベース}
データから知識を得ようとすること自体に目新しさはないはずだ。では、何が今頃になってデータマイニング「特需」をもたらしたのだろうか。この問いかけに答える鍵の一つは、データベースマネジメントシステム（DBMS）の変遷である。
%Imielinski（1995年）

かつてのDBMSにおいては、銀行のATMで利用されているようなオンライントランザクション処理（OLTP）と呼ばれる情報処理が主流であった。つまり、DBMSに求められていたのはデータを貯蔵し、多くのユーザーからの小規模なデータアクセスを伴う定型クエリーを高速に処理するという能力だった。
%http://otndnld.oracle.co.jp/deploy/performance/pdf/Oracle8i-tuning2.pdf
しかしデータベースに蓄積されたデータを様々な切り口で分析したい人々が
データベースに意思決定支援を求めるにつれて、新しい情報処理が注目を集めるようになった。それが
オンライン分析処理（OLAP）である。

OLAPによってユーザーは対話式の試行錯誤を繰り返して、データウェアハウスに格納された膨大なデータを集計したり、あるいは多次元的に分析したりすることができるようになった。
一見するとこれはデータマイニングツールの原型のように見えるかもしれない。しかし、OLAPによるデータ解析によってデータウェアハウスと「対話」できるのは、的確な仮説を考え続けるような不断の洞察力とクエリーを書き続ける技術力（および時間と体力）を持ち合わせた選ばれし人間だけである。データに潜むパターンやモデルの叩き台を自動的に見つけてくれるものではないし、ビジネスの場における迅速な意思決定に向いているものでもない。だからこれは
データマイニングツールの原型と言うよりはむしろ、人々がデータマイニングツールを求める素地を作ったと考えたほうが良いだろう。
%大規模なデータからパターンを見出す試行錯誤のプロセスが人間の手を煩わせずに実現するならばそれが効率的であるし、data analysis for everyoneみたいなものへの需要は高まっていった。

\subsection{プロパガンダとしてのデータマイニング}
データマイニングもまた他のバズワードの例にもれず定義が曖昧な言葉だが、色々な定義をまとめるとしたら「
複雑で膨大なデータ集合の中に埋もれていて人間の力では見いだせないような有益な知識をコンピューターの力でデータベースの中から探索的に発見し、ビジネスにおける意思決定に役立てるような色々な手法（またはそれらを使ったプロセス）」というところであろう。実際のデータ解析にはIBM Intelligent miner、SAS Enterprise Miner、SPSS Clementineなどの汎用データマイニングツールが使われることが多い。

Knowledge Discovery in Databasesというデータマイニングの目論見は壮大なものである。しかし、
%データから新たな知見を得ようとする試みは全然斬新なものではない。
ユーザーが小難しいデータベースの構造やデータ解析の中身を知らなくてもデータベースに格納されたデータにアクセスし、それを分析したり視覚化したりすることを可能にするというデータマイニングツールというものは
誰のためのものなのだろう。
「データ自身に語らせる」といえば聞こえは良いが、ビジネス戦略のヒントとなるような知識を囁くかどうかは別の話であるし、あまりそういう都合の良い話は聞かない。Friedmanが指摘するように、汎用データマイニングツールは非常に高額な商用ソフトウェアであり、導入するとなれば当然相応の高価なハードウェアやストレージも要求されることになる。その意味で、データマイニングはもはや金脈を掘り当てようとする宝探しのような試みというより、むしろデータマイニングと聞いてソフトウェアやハードウェアに投資してくれるような、データを持て余している企業の意思決定者や潜在的データマイナーを発掘するための商業的プロパガンダと化しているところがある。

\subsection{データマイニングと統計解析の違い}
Friedmanは汎用データマイニングツールが提供する主要な手法の数々を
%\footnote{Decision tree induction・rule induction・nearest neighbors・clustering・association rules・feature extraction・visualisation、neural networks・graphical models（Bayesian belief networks）・遺伝的アルゴリズム・SOM（自己組織化マップ）・neuro-fuzzy systemsと}
を列挙し
%\footnote{Friedmanは定番手法としてdecision tree induction(C4.5, CART, CHAID)・rule induction(AQ, CN2, Reconなど)・nearest neighbors(事例ベース推論)・clustering methods(data segmentation)・association rules(market basket analysis)・feature extraction・visualizationを挙げ、その他の手法としてneural networks・bayesian belief networks(graphical models)・genetic algorithms・self-organizing maps(SOM)・neuro-fuzzy systemsを挙げている。}
、多くのデータマイニングツールにおいて
仮説検定や判別分析など、統計学者にとって見慣れた手法は
殆ど無視されていると評している。一言で「データを解析する」と言っても、データマイニングと統計解析は目的も手段も大きく異なるものであり、両者を混同してはならない。

統計学的なデータ解析は、「データ自身に語らせる」という大らかなものではない。
それは「実験が終わった後に統計学者にコンサルトするのは、死後に解剖を依頼するようなもので、統計学者はその実験の死因を教えることぐらいしかできない」というRonald Fisherの格言にも端的に表れている。
統計学者にとってはデータは綿密な実験デザインに基づいて注意深く収集され、吟味されるべきものであり、
鉱山のように元々そこにあるものではないのだ。
それに、データを説明するための仮説やモデルがなければ仮説検定もモデル選択\footnote{モデルのパラメータを最尤推定する「モデル推定」とともに、どのモデルを選ぶべきかという「モデル選択」は統計学・機械学習においては最も重要なテーマの一つである。AICやBICといった情報量基準が有名だがクロスバリデーションもよく使われる。}も始めようがない。良質なデータと洗練されたモデルこそデータ解析の要なのである。モデルが正しければ良い結果を出し、正しくなければ悪い結果を出すのがアルゴリズム（計算手法）のあるべき姿なのだから、解析結果の善し悪しはアルゴリズムではなくモデルが決めていると考えるのが筋である\footnote{本稿執筆時にインターネット上で配信されていた伊庭幸人氏（統計数理研究所）の「マルコフ連鎖モンテカルロ法の基礎」という講義の中でもモデル・統計手法・アルゴリズムの違いが強調されていた。この段落はそれに共感しながら書かれたものである。}。

これに対しデータマイニングは、良く言えば従来の統計学が扱えないような整理されていない大規模データを何とか使える形にし、色々な切り口で分析して意思決定のヒントになる知識を見出そうとする果敢な取り組みである。悪く言えば産業廃棄物のような膨大なデータの山をゴミだと認めるわけにいかないからといって往生際悪くデータのフォーマットを整え、クレンジング\footnote{データの誤りや重複を洗い出し、使えるデータにするという泥臭い（しかし最も重要な）プロセス。}を施して場当たり的に試行錯誤しながら有象無象の仮説やモデルの叩き台をあれこれ捻り出すという作業でもある。

いずれにせよデータマイニングは統計学とは全く異なる思想を持つものであり、仮説やモデルなしに鉱山の中を探索するデータマイニングにおいてよく登場する手法が、統計学の手法と異なるのは驚くべきことではないということが分かる。データマイニングが語られる文脈においてはテクニックやアルゴリズムありきの話になりがちであるのも、当然の成り行きなのかもしれない。

\subsection{データマイニングの意義}
しかしながら
%データマイニングに失望したり、ベンダーやコンサルティングファームに対して猜疑的になったり、
データマイニングの努力は水泡に帰するのかと悲観する必要はないし、それは本稿における我々のスタンスに反するはずだ。むしろそこでの努力を無にしないために何ができるのかを考えながら進まなくてはならない。

データマイニングに対して批判的になるのをここで一旦やめてみよう。少し楽観的になったら、データマイニングの功績が色々と浮かび上がるに違いない。従来の統計学では難しいような大規模で複雑なデータを解析してきたこと、前処理の重要さを実証し、そのためのプラクティカルな方法を確立してきたこと、データの可視化によって迅速な意思決定を強力に支援していることなど、色々なアドバンテージに気づくはずだ。様々なテクニックを集めたツールとデータ解析の専門家だけでは解析結果は荒唐無稽なものになりがちだったが、それもデータマイニングは役立たないと切り捨てる理由にはならないはずなのだ。むしろデータ解析には現場の知識を持った専門家の知識が必須であるということを改めて思い知らせてくれたと考えることができるし、その方がよほど建設的である。

データマイニングの商業的な側面ばかりを強調したが、大規模データベースを持っているのは企業だけではなく、
後述するように公開されている大規模なデータベースも色々と存在する。
そのようなデータベースから知識を発見する試みにおいてデータマイニングは単なる商業的なプロパガンダに堕したものではなく、真理の探究を目指す学術研究において仮説やモデルのプロトタイプを見つける手段となったり、
蓄積されたデータから合理的に最適な政策を決定するという国家や国際社会の意思決定を支援するためのツールとなり得るものである。

データマイニングは、クラスタリングや決定木学習を筆頭に、多様なパターン認識・機械学習の理論的な枠組みを現実の大規模データに応用してきた。そうした理論と現実の橋渡しをする取り組みは軽視されてはならないし、その価値は商業的な実用性や理論的な厳密さや一般性だけで評価されるべきではない。そうでなければデータマイニングバブル崩壊とともに、データベースから知識を発見することの重要性も失われかねないからだ。

\section{データ解析から離れた機械学習}
データ解析のことばかり考えていると、統計学もパターン認識も機械学習もデータベースを掘り起こす道具にしか見えなくなるかもしれないから、それ以外の側面についても少しだけ触れてから先へ進むことにしよう。
\subsection{医療機器}
統計学や機械学習で登場するアルゴリズムの幾つかは、医療の現場で人々の健康な生活や生命を支えている計算手法と言っても過言ではない。医用画像を得るためには画像再構成という重要なプロセスがあり、そこではバックプロパゲーションやEMアルゴリズムといった機械学習・統計学でよく知られる手法が一般的に用いられている。
また、病気の検査のためだけでなく、
がんの放射線治療においてはモンテカルロ法が古くから線量分布推定に用いられ、
今もなお放射線治療計画に必須の計算手法であり続けている。

医療現場で使われる装置やプログラムに関わるごく一部の例であるが、
統計学や機械学習の、データマイニングで期待される役割とは少し違う一面を垣間見ることができる。

\subsection{神経科学}
もともと機械学習は人工知能研究の一分野であることから分かるように、理論神経科学と深く関係しており、その理論的な枠組みそのものが、人間の脳のメカニズムや、社会における人間の行動などを理解する一助となり得るものである。パーセプトロンは古くから小脳のモデルとして知られ、人間の運動学習の神経基盤を理論的にも実験的にも明らかにし、ロボットの運動制御という工学的な応用にもつながることとなった代表的な例である。
もう少し最近の例としては、強化学習の中でも特にTDモデルが大脳基底核の計算モデルとして注目されている。ドーパミンニューロンが報酬予測誤差によって強化学習を行っているという仮説が提唱され、報酬に基づく人間の意思決定の神経基盤が理論的にも実験的にも精力的に研究されている。こうした研究はパーキンソン病という大脳基底核が侵される神経変性疾患や、薬物・ギャンブル等への依存症などにも密接に関連している。

こうした人間の脳につながる研究はロマンティックで興味深く、また実際に有益でもあるのだが、本稿ではもう少し現実的で差し迫った問題を題材にして、データベース・機械学習・統計学について考察を深める。

\section{認知症の研究：大規模データベースと機械学習の応用}
%日本人の平均寿命は世界の先進国の中でもトップクラスであり、それは高度に発達した現代の医学と、充実した医療サービスを誰でも受けることが出来るようにするための国民皆保険制度の賜物である。しかし
周知の通り、日本を筆頭に世界各国（特に先進国）で高齢化が社会問題化している。認知症患者は増え続けており、医療費の増加を通じて国の財政を圧迫し、若い世代にも大きな負担となっている。

認知症は社会だけでなく個人にとっても人生を脅かす問題である。
厚生労働省によれば日本では現在既に65歳以上の高齢者のうち約10\%が認知症であると見積もられており、しかも今後さらに増加していくと推定されている\footnote{厚生労働省による資料：http://www.mhlw.go.jp/kokoro/speciality/detail\_recog.html
}。
我々のうち少なくとも10人に1人は認知症患者になっていくという事実を突きつけられると、生涯現役が当たり前になりつつある中で自分の本来のパフォーマンスを発揮できなくなってしまう悲劇だとか、周囲の人々の人生への打撃といった生々しいイメージがふと脳裏に浮かんでしまう。

\subsection{認知症：背景知識の要点}
認知症というのは単一の疾患ではなく、認知機能低下を呈する多くの疾患の総称である。名前だけならば誰でも一度は聞いたことがあるように、認知症の中で一番多くて重要なのがアルツハイマー病である。

\subsubsection{アルツハイマー病はハードウェアの異常である}
コンピュータの調子が悪くなったら、まずはハードウェアレベルの異常なのかソフトウェアレベルの異常なのかを考えるのが自然ではないだろうか。それと同様に、長年使ってきた脳がうまく動かなくなる場合も、原因はソフトウェアのレベルとハードウェアのレベルとに分けられる。

認知「機能」の障害と聞くとソフトウェアレベルの問題のように思うかもしれないが、アルツハイマー病は正常な脳の「構造」が壊れることにより機能に支障が出る病気で、ハードウェアの著しい劣化による脳の故障と考えるべきものである。

具体的には、正常な人でも加齢で蓄積するゴミのようなもの\footnote{ゴミの正体について少しだけ詳しく述べれば、アルツハイマー病は脳内に$\beta$アミロイドとよばれるタンパク質が蓄積し、異常リン酸化タウタンパクが蓄積してしまうことで神経細胞の変性・機能不全が起こり、神経細胞死を招く疾患である。それが肉眼的・画像的には脳萎縮という所見となる。}が異常に脳に溜まってしまうために神経細胞がどんどん死んでしまい、脳のボリュームが目に見えて減ってしまうのである。

\subsubsection{アルツハイマー病の根本治療薬とワクチン開発への動き}
ハードウェアの障害であると繰り返したのは、病気が進行して重症化してからでは有効な治療法がないことと、MRI画像で脳の形やボリュームを見ることが診断に有効であることの二点を強調するためである。
壊れかけの初期のうちに食い止めることができたら理想的であるし、もっと言えば最初から認知症を予防できたら完璧である。

そういうわけで現在多くの巨大な製薬会社や医療機器メーカーがベンチャー企業とも協力し、アルツハイマー病の根本治療薬やワクチン開発を目指して研究開発に取り組んでおり、実際に日本でも複数の治験が進行中である。
こうした根本治療薬・ワクチン開発への動きによって、脳の異常の客観的な評価方法や早期診断手法の重要性は急速に高まった。要は完全に脳が壊れてからではなく、なるべく小さいうちに病気の芽を見つけ出すことにより、できるだけ早い段階でその芽を摘み取りたいのである。
\subsection{ADNIデータベース}
2004年にアメリカはUS-ADNI（米国アルツハイマー病神経画像イニシアチブ）をスタートさせた。
これは800人以上の被験者を6ヶ月毎にスキャンして得た
経時的な画像データ
\footnote{
MRIの構造画像だけでなく、FDG-PETという機能画像や発症前診断のためのPiB-PETによるアミロイドイメージング画像も含む。}・認知機能検査の点数・色々な生物学的マーカー・臨床的な因子の有無といった様々な情報をデータベース化し、それをweb上で公開するという試みである。開始当初の予算だけでも6000万ドルということからも、US-ADNIが認知症研究を推進するための国家的な臨床研究プロジェクトであることを窺い知ることができる。

医用画像データの質は施設ごとの撮像方法や装置そのものに大きく影響されるため、多施設から得られたデータをそのまま解析することは出来ない。しかしADNIではその問題を解消するために施設や装置の違いを補正するための前処理を施している。そうして作られた信頼出来る品質のデータは匿名化され、翌日にはwebサイト上で世界中に公開される\footnote{岩坪威「アルツハイマー病の発症前超早期治療に向けてのデータを蓄積」INNERVISION (26・11) 2011}。

因みにこのADNIというイニシアチブはヨーロッパ・オーストラリア・日本・台湾・韓国でもスタートし、日本でもJ-ADNIという名前で2007年からプロジェクトがスタートしている。
US-ADNIの方はさらにアルツハイマー病の早期診断・発症予測に焦点を当てたADNI 2に乗り出しており、また被験者の全ゲノムシークエンスをも公開するビッグデータプロジェクトに変貌しつつある\footnote{もし800人の被験者の全ゲノム配列が得られたらデータベースに追加されるデータのサイズは少なくとも165テラバイトになると見込まれている。http://www.adcs.org/research/PDF/ADNIExclusiveFall2012.pdf}。

\subsection{ADNIデータベースと機械学習}
US-ADNIのデータは誰でも自由に無料でダウンロードすることができる。もっといえば、医療と何ら関係のないデータマイニングや機械学習の研究者であっても、最先端の検査を駆使して得られた大規模な患者データにアクセスすることができる。データはどのように利用しても良いし、「データベースからの知識発見」に成功したならば当然それを自由に論文として発表することが出来る\footnote{ADNIのデータを使った研究の一覧はhttp://adni.loni.ucla.edu/publications/で見ることができる。}
。脳画像への機械学習の応用がさかんに行われている背景にはそうした事情もあるだろう。ADNIデータベースはそうした学際的な研究の基盤になっているのである。

機械学習を応用する側の視点で見れば、確実に正しい教師データが入手できるというのもADNIデータセットの長所である。実は日常的に行われている臨床診断は、厳密には100\%正しいラベル付けとは限らない。それはアルツハイマー病の確定診断をするためには顕微鏡で実際に「ゴミ」が溜まっているのを見なければならないため、手術室で生きている脳から組織のサンプルを採取することが出来た場合を除き、真の答えは死後の解剖なしには確かめようがないからである。ADNIは人間の医師が付けたラベルのみならず、判明している限りの「真のラベル」も公開している。だから訓練データの中にあるかもしれないノイズに悩まされることなく安心して教師あり学習アルゴリズムを使うことが出来る。
\subsection{機械学習による自動診断から発症予測へ}

\subsubsection{SVMによるMRI画像の自動診断}
MRI画像「だけ」でアルツハイマー病を正確に診断するのは人間の医師であっても訓練が必要である。
ところがサポートベクターマシン（SVM）という分類器は認知症症状の程度や年齢といった情報を一切使わず、MRI画像だけで重症なアルツハイマー病の患者と健康な高齢者を95\%ほどの感度・特異度で分類することに成功してしまった\footnote{Kl\"{o}ppel, et al. Automatic classification of MR scans in Alzheimer's disease. Brain (2008), 131, 681-689}。
ここで用いられたSVMは線形SVMというシンプルな教師あり学習アルゴリズムである。おさらいの意味で少しだけ詳しく述べると、非常に高次元なMRI画像を数万次元の長いベクトルとみなし、画像同士の類似度をベクトルの内積として計算し、画像に貼られたラベルから病気か否かの線引きを学習するという方法である。
そんな単純な手法であるにも関わらず、アルツハイマー病患者の脳と健康な高齢者の脳をはっきりと識別し、さらにアルツハイマー病患者の脳とFTLDという別の認知症患者の脳もかなり正確に識別した。勿論この正確さは過学習によるものではない。SVMはトレーニングとテストで異なる施設のデータセットを使っても正しく分類するという高い汎化能力も見せつけたのだった。

\subsubsection{手法の改良と挫折}
MRI画像からアルツハイマー病を自動診断するための手法を改良しようとして、その後も色々な手法が提案されてきた。しかし、後述するDARTEL\footnote{The Diffeomorphic Anatomical Registration Through Exponentiated Lie Algebraという正式名称がある。開発者はJohn Ashburner。}というアルゴリズムによって前処理の方法論は躍進を遂げたものの、特徴選択・特徴抽出・分類に関しては結局あまり大きな手法の改良はなかった。

色々な手法の性能比較は難しい問題だが、Cuingnet\footnote{Cuingnet, et al. Automatic classification of patients with Alzheimer's disease from structural MRI: a comparison of ten methods using the ADNI database. Neuroimage (2011),56(2),766-81 Epub 2010 Jun 11}はADNIデータベースのデータを用いてこれまでに提案されてきた手法を追試し、比較するというメタアナリシスを行った。それによると、アルツハイマー病患者と健康な高齢者のMRI画像診断は大体どのやり方を使ってもそれなりに出来るものだったが、DARTELというアルゴリズムを含む前処理を施した後、脳の全領域の情報を使って線形SVMで識別するのがやはり最善のようであった。

工夫を凝らした複雑な手法をシンプルな手法がせせら笑っているかのような結果である。しかし、MRI画像は非常に高次元で多くの情報を含んでいるために非線形な特徴抽出をする必要がなく、また前処理のアルゴリズムが洗練されていったことでノイズを恐れて脳の領域を一部分に限定するという特徴選択をする必要がなくなり\footnote{脳MRI画像は非常に高次元なデータなので、良い特徴選択ができたら効率的であるが、脳の領域を予め人間が選択すると汎化能力が低くなるという問題がある。訓練データから重要な領域を学習させることも試みられてきたが、時には数週間の計算を要するほどに計算量がやたらと増えるばかりで、結局分類の正確さを向上させようとすると脳の領域全ての情報を使うに越したことはないという結論になってしまった。また、カーネル関数の選択についても色々と試行錯誤されたのであるが、結局は線形カーネルに落ち着くことになった。特徴ベクトルが非常に高次元であるため、線形カーネルで十分なのであろう。}、重症アルツハイマー病患者の脳と健康な高齢者の脳を分類するという程度の問題であれば、全脳のデータを線形SVMに突っ込むだけで十分になったのだと考えることができるだろう。

\subsubsection{目標の再確認}
ここで目標を見失って迷子にならないように、我々のモチベーションを再び明確にしておこう。我々が目指しているのはアルツハイマー病というハードウェアの障害を壊れかけの初期のうちに食い止めることであったはずだ。だからこそ完全に壊れてしまってからではなく、壊れ始めている早期に正常と異常の境界を引くことが出来るような方法を求めているのである。

重症なアルツハイマー病と健康な高齢者の脳の違いは人間には一目瞭然である。だからそれを識別するような2クラス分類問題なんて、線形SVMで事足りるという話である。厄介なのはあまり重症ではないアルツハイマー病と健康な高齢者の2クラス分類問題であり、ましてやまだ認知症を発症していないような「ちょっとボケかけ」の人々\footnote{分かりやすくするためにだいぶ露骨な物言いになってしまったので正しい言葉でも説明しておく。こうした「ちょっとボケかけ」の人々は軽度認知機能障害（MCI）と診断されており、白黒がまだつけられないケースに貼られるグレーなラベルである。このグレーゾーンの人々の中にはやがてアルツハイマー病を発症してクロになる人もいるし、ずっとグレーのままという人もいる（前者はMCI converter、後者はMCI non-converterといわれる）。アルツハイマー病の早期発見とか発症前診断というのは、このグレーな人々の中から将来クロになる人々を見出すというタスクである。}を将来認知症を発症する人としない人に分けるという2クラス分類問題となると人間のエキスパートもしばしば苦慮してしまうほどの難問なのである。

先ほどのCuingnetらの解析によれば、あまり重症ではないアルツハイマー病と健康な高齢者の2クラス分類問題ではどの手法でも途端に感度が落ちてしまった。そして「ちょっとボケかけ」の人々を将来認知症を発症する人としない人に分ける2クラス分類問題となると、どの手法もお手上げだったのである。

\subsubsection{これ以上何ができる？}
これまで見た手法には何が足りなかったのだろうか。
まず気がつくのは、これまでの手法はMRI画像だけで診断をしているということである。
病院で医師が診断をする時に色々な情報源を用いているように、
複数の検査から得られる情報を統合して診断をする枠組みが欲しいと考えるのは自然なことであるはずだ。

別の観点では、一時点のMRIデータしか使っていなかったというところにも改善の余地があるだろう。シンプルに捉えるならば脳が萎縮するスピード、欲を言えば非線形な経時的変化も重要な特徴のはずである。

それから、感度や特異度という数字を上げることだけに拘泥しがちではあるのだが、そればかりが手法の「改良」とは限らないということも、指摘しておく必要がある。多くの説明変数を使えばより正確なpredictionができるかもしれないが、データが複雑になればなるほどシンプルな説明変数が望ましくなるということを忘れてはいけない。
複雑な脳の画像データを全部使う代わりに、出来る限り本質的な領域を選び抜けるならばそれに越したことはない。それができなければどれだけ機械が正確に分類できるようになったところで、人間にとって結果の解釈は難しいままになるからである。

\subsubsection{スパース正則化・マルチカーネル学習}
スパース正則化は観測されたデータをいかに少ない変数で説明するかという問題の近似解法である。また、スパース正則化の特別な場合としてマルチカーネル学習があり、これは多様な情報源からの情報をうまく統合するための枠組みとして注目されている\footnote{詳しい説明は冨岡 亮太・鈴木 大慈・杉山 将の「スパース正則化およびマルチカーネル学習のための最適化アルゴリズムと画像認識への応用」（画像ラボ 2010.4.5）を参照のこと。}。

これは現在我々が直面している問題にどのように役立つのだろうか。スパース正則化を用いた特徴選択は、複雑な脳の画像データから本質的な領域を選び抜くために用いることができる。脳画像をそのまま数万次元のベクトルとして捉えるのではなく、早期診断に必要な幾つかの本質的な領域だけを重要な説明変数として使うのである。

マルチカーネル学習についてはどうか。これはMRIだけでなくPETや認知機能検査などの様々なモダリティのデータを最大限に活用するために使うことができそうだ。これまでに見てきた手法はMRI画像からカーネル行列を1つだけ作ってSVMで識別するというものだったが、モダリティごとにカーネル行列を作って、複数のカーネル行列を凸結合したカーネルを分類・回帰に使うのである。これはそれぞれのカーネルの最適な重み付けを学習できるので、どの検査をどのぐらい重視するのかということも最適化することができる。しかもそれが凸最適化なので、大域的最適解が求まるという強みがある。
\subsubsection{マルチカーネルSVMによるアルツハイマー病の発症予測}
こうした機械学習の先端的な枠組みが現実世界でどのぐらい通用するのかを試そうと思ったら、ADNIデータセットは格好の対象となる。Zhangらはまさにスパース正則化による特徴選択とマルチカーネルSVMをADNIデータセットに応用している
\footnote{Zhang et al. Predicting future clinical changes of MCI patients using longitudinal and multimodal biomarkers. PLoS One (2012),7(3),e33182. Epub 2012 Mar 22}。
%多様な情報源からの情報を結合カーネルという形で統合し、マルチカーネルSVMを用いた分類・回帰を発症前のアルツハイマー病の識別や将来の認知機能の点数予測に応用するという研究を行った。
MRI画像にSVMを適用しただけでは特に感度が低さが目立ち、認知症の早期発見や発症予測という問題には太刀打ちできなかった。しかし経時的な情報を含めたマルチカーネルSVMによってかなり感度が改善され、発症前の診断は随分と正確なものになった。

面白いことに、MRI・PET・脳脊髄液中のバイオマーカーというそれぞれの情報源から得られた情報を使ってカーネル行列を作った方が良いのはカーネル行列を視覚化すれば一目瞭然である。一見したところではMRIのカーネル行列が一番良さそうに見えるのでPETのカーネルや脳脊髄液中のバイオマーカーのカーネルは要らなさそうに思えるのだが、3種類のカーネルを結合して得られたカーネル行列はMRI単独のものよりも確かに見やすいものになっているのだ\footnote{興味があれば、色付けされたカーネル行列を見てみると良いだろう。Multimodal classification of Alzheimer's disease and mild cognitive impairment. Neuroimage. 2011 Apr 1;55(3):856-67. Epub 2011 Jan 12}。つまりそれぞれのモダリティはどれかが優れているとか劣っているとかではなくて、それらが含む情報は互いに相補的なものであるということである。MRIは構造を見る画像検査で、PETは機能を見る画像検査で、脳脊髄液中のバイオマーカーは画像検査に直接現れるものではないのだから、当たり前といえば当たり前である。

Zhangらの研究に使われている特徴選択・特徴抽出アルゴリズムの詳細は本稿では省くが、ADNIデータベースによって6ヶ月毎に撮像された複数時点の脳画像データからL2,1ノルム正則化によってどの時点でも重要になるような重要な脳の領域をスパースな解として得ているというのが特徴選択の要点であり、画像を$t$（時点）の多項式と考えて直交多項式展開した際の係数を特徴ベクトルに含めるというのが特徴抽出の勘所である\footnote{2次以上の多項式で捉えることで、萎縮のスピード（$t$の一次式）だけではなく非線形な変化に関する特徴も抽出している。}。

一時点のMRI画像だけでは困難な問題であっても、現在までの経時的な変化を辿り、訓練された人間の医師のように
構造画像だけではなく機能画像やカルテに散らばった様々な情報を統合することで、半年後に認知症を発症するか否かを80\%ほどの感度と特異度で識別することができるようになった。この解析で使われたのはMRI・FDG-PET・認知機能検査の情報だけであり、血液や脳脊髄液中のバイオマーカーの情報や、現在発症前診断法として最も注目されている[$^{11}$C]PIB-PETという画像の情報は使われていない\footnote{ADNIデータベースといえども、全ての検査を受けている被験者はあまり多くはないからである。}。これらの情報を取り入れればマルチカーネルSVMによる発症予測はより正確なものになると思われる。

\subsubsection{国家レベルの意思決定支援へ}
根本治療薬開発への動きが激化している現在、我々が直面している問題は単に病気の人と健康な人を識別するという問題の範疇を超えて、ハードウェアが壊れてしまう前に劣化を食い止めることができるか否かという人生の明暗を分けるための大きなステップである。単に医師個人の経験やその寄せ集めによる発症予測ではなく、大規模なデータセットに機械学習のアルゴリズムを応用する研究によって発症予測の客観性が高まっていけば、治療薬をどこまで保険適用とするかといった国家レベルの意思決定を支援するという意義も出てくるに違いない。


\section{認知症の臨床：80人のデータベースと統計学の成果}
さて、大規模データベースや機械学習の話に少しページを割き過ぎてしまったのだが、実は実際の日常診療において活躍しているのはもっと小規模なデータベースと古典的な統計解析である。

\subsection{Statistical Parametric Mapping (SPM)}
脳の構造や機能に関する研究の発展はSPMなしにはなかっただろうと思えるほど、SPMというソフトウェアは脳画像統計解析によって神経科学・心理学・精神医学・神経内科学に貢献してきた。SPMはロンドン大学（UCL）のKarl Fristonらが開発した脳画像の統計解析ソフトウェアであり、脳の画像\footnote{SPMはMRIのみならずfMRI・PET・SPECT・EEG・MEGといった多様な脳画像を扱うことができる。}の統計解析をより信頼できるものにするための前処理アルゴリズムに加え、古典的な統計学的仮説検定からベイズ推定に至るまで様々な解析手法が網羅的に実装されている。

SPM自体はオープンソースであり、無償で配布されているのだが、SPMを動かすには残念ながらMATLABという高価な数値計算ソフトウェアが必要である。SPMを用いた研究によって認知症診断にクリティカルな知見がどれだけ解明されようと、脳画像の統計解析が象牙の塔から開放されなかったというのは想像に難くないだろう。大学や研究所などの附属病院などでもない限り、そうした知見は医療の現場へスムーズにおりてくるものではなかったのだ。
\subsection{日常診療のための脳画像統計解析}
幸いなことに、日本では\ruby{VSRAD}{ブイエスラド}というアルツハイマー病早期診断支援ソフトウェアが登場したおかげで、アルツハイマー病早期診断のための脳画像統計解析は医療現場で簡単に使えるようになっている。VSRADはSPMの前処理アルゴリズムや、後述するVBMという脳の形態解析手法をSPMの開発元に許可をとってスタンドアロンアプリケーションにしたものである。WindowsがインストールされたパソコンさえあればMATLABをインストールしなくても動くし、MRI画像解析の専門知識がなくても撮像されたMRI画像を読み込ませて簡単な操作をするだけでMRI画像に前処理を施し、認知症診断のための統計解析結果を出力してくれるという有り難いツールである。
VSRADはエーザイという製薬会社が無償で提供しており、ダウンロードすればすぐ使えるためVSRADは日本では1000以上の施設でルーチン化され、脳ドックでも使われているほどに広く普及している。
因みにMRI画像ではなくSPECT・PET画像を解析するための同様のソフトウェアとしてはeZISやiSSPがある。これらも無償で提供されているソフトウェアであり、日常的にお目にかかることができる。

\subsection{古典統計学の誘惑}
\subsubsection{VSRADに見る古典統計学の強み}
VSRADの統計解析の基盤となっているのは同じくロンドン大学（UCL）のJohn Ashburnerが開発したVBMである。
これはある群の脳MRI画像の各ボクセル\footnote{ボクセルはピクセルが3次元になったようなものを意味する。}の灰白質濃度がある群のそれと比べて有意差があるかを検定\footnote{ボクセルごとにt検定をすると当然多重比較の問題が出てくることになるが、SPMではGaussian random fieldによる補正をしている。何故Bonferroni法ではダメかといえば、各ボクセルは空間的に連続しているから明らかに独立ではないし、しかも大抵前処理の段階でガウシアンフィルタによるスムージングもかけられたりしているからである。}するというものである。例えば、若年者グループと高齢者グループの脳画像データを比べれば、高齢者で有意に萎縮している脳の領域を調べることができる。

VSRADはこのVBMを基礎にしており、調べたい患者のMRI画像と80人の健常者のMRIデータベースとを比較し、Z検定統計量（Zスコア）のカラーマップをMRI画像上に表示してくれる。このカラーマップを見れば有意に萎縮している脳領域が一目で分かるようになっている。
さらに日常診療で使いやすくするために早期アルツハイマー型認知症診断に重要な海馬傍回という領域内のZスコアをざっくりと平均して、萎縮の程度の目安を表示してくれたりもする。
勿論、偏差値だけでは把握できない学力があるように、ざっくりと平均したZスコアだけで早期アルツハイマー病を完璧に診断することができるはずもないので、あくまでもVSRADは診断支援という位置づけのソフトウェアであることを忘れてはならない。それでもこのZスコアだけで早期アルツハイマー病の正診率は87.8\%だったという報告\footnote{Y. Hirata, H. Matsuda et al. : Neuroscience Letters 382（2005）}もあり、『VSRAD適正使用ガイドライン』\footnote{http://www.vsrad.info/general/manual/download/guideline1.pdf}が出されてしまうほど、良くも悪くもポテンシャルの高さが認められている。

\subsubsection{さようなら、ビッグデータ}
保持しておかなければならないデータや検索しなければならないデータのボリュームが爆発的に増える限り、大規模なデータベースのデータを効率的に検索したり集計したりするというアルゴリズムがますます重要になることに疑念の余地はない。
しかしながらデータ解析となると話は別である。これは全ゲノムシークエンスを公開するというUS-ADNIのビッグデータプロジェクトによって人類の運命を大きく左右するような仮説やモデルが発見されるかどうかという話ではなくて、大規模なデータがあったらそれを全部使わなければならないなんてルールはどこにもないという話である。

大きな目標を達成するために必要なものは大規模データベースや技巧的なアルゴリズムとは限らない。むしろサンプル数があまり多くないデータセットや統計学的仮説検定のようなお馴染みの理論かもしれない。
だから少なくともデータ解析の世界においてはビッグデータの幻影に怯える必要はない。
大きければ大きいほど良いなどというちっぽけな固定観念が、偉大な知識発見の妨げになってしまうかもしれないということの方が余程恐ろしいのだ。

\section{知識発見の悪夢}
グレーの中からクロを見出すという難易度の高い2クラス分類問題に挑むアプローチの一つは、ADNIデータセットに対する機械学習の応用であった。しかしVSRADの例を見せられると、800人もの被験者を集めた大規模なデータベースだとか奇を衒った機械学習アルゴリズムなんかより、健常者80人の脳MRIデータベースと古典的な仮説検定の方が医療現場で活躍しているように見えたのではないだろうか。

とはいえ、古典統計学に引き篭もるというのは退行と言わざるを得ないのだ。我々は古典統計学に扱えないようなものを扱うために大規模なデータベースや機械学習に目を向けたのではなかったか。古き良き時代の枠組みに逃げこんだところで、それはほんの一時の雨宿りに過ぎないはずだ。

古典統計学に閉じこもっていては新しい仮説もモデルも見つからず、古典的な統計学的仮説検定の力はやがて色褪せてしまう。パラフレーズを試みるべく、古典統計学を熱心に「木」を見ることによって深い「森」を知ろうとするアプローチだと考えてみよう。このアプローチにしがみつく限り、あなたの視線が「森」に注がれることはない。果たしてあなたは「森」を知らずに「木」を認識できるだろうか？

この逆説のおぞましさに恐れ慄いたあなたは、「木」の呪縛から逃れようとして再び大規模データベースに向かって息つく暇もなく走りだす。辿り着いた先は、古典的な統計学的仮説検定が無力となる樹海である。それでもあなたは機械学習に光を見出し、データベースからの知識発見を自らに言い聞かせるようにしてあてもなく彷徨う。あなたの試行錯誤が報われるかもしれないのは、その樹海から脱出する時だ。あなたは何かを見つけて、命からがら古典統計学に逃げ帰ってくる。見つけたものは探し物かもしれないし、紛い物かもしれない。いずれにせよあなたは古典統計学の「木」に寄り添ってはいられなくなり、再び「森」に向かって走りだす。

終わりのないこの流動に飽きもせず、いや飽きていないふりをして、遭難者であることを認めず冒険家であろうとするあなたは------いや我々は、『構造と力』の言葉でいえば知識発見という遊戯を強いられた「不幸な遊戯者」であり、この延々と続く循環構造の中に閉じ込められている「道化」なのである。これを悪夢と呼ばずして何と呼べば良いのだろう。

データから学ぶという営みをこれほどに魅力的なものにしているのは、我々の精神に根深く存在している罪深い探究心である。この欲望はデータベースとか機械学習とか統計学といった枠の中に行儀よく収まるものではない。「木」や「森」の枠組みから自由であろうとして、我々はその「構造」からの脱却を図った。古い枠組みから逃れた我々はどの枠組みに留まることもできず、この逃走から逃げる道を見いだせないまま、逃走を続けている。
再び『構造と力』から引用するならば、「遊戯の充満が遊戯の喪失でもあるという不幸な逆説。この逆説から逃れる道を探ることこそが、いま問われるべき重要な問題である」ということになる。
%穏やかで幸福なプレイグラウンドは、一体何処にあるのだろう？

\section{「真に喜ばしい遊戯」へ}
我々をこの悪夢から解き放ち、「真に喜ばしい遊戯の場」へ導きうるものとは何だろうか。

ここに"The Future of Data Analysis"という論文がある。最近のベストセラーかと思わせるようなタイトルだが、数学者・統計学者であるJohn W. Tukey\footnote{我々が慣れ親しんでいるビットという言葉も、高速フーリエ変換というアルゴリズムも、統計学でお馴染みの箱ひげ図も、すべてTukeyが生み出したものである。彼の偉大さについて逐一説明するのは不要であろう。}が1962年に書いた論文である。この論文は仮説検定という方法論を偏重した当時の統計学に対する警鐘と言って良い。

『パターン認識と機械学習』の目次を眺めるだけで色々な機械学習のアルゴリズムが開発されてきたことが見て取れるように、基礎的な統計学の教科書をパラパラとめくるだけで面食らうほど様々な手法に出くわすはずだ。しかしTukeyはそうした手法の理論ばかりではなく、現実のデータを扱う探索的データ解析（exploratory data analysis）もまた統計学において重視されなくてはならないということを主張した。

そのために彼が指し示したのは、統計学を「手法の集合」ではなくて「問題の集合」と捉えるというパラダイム転換であった。"Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise."という格言は、脱構築の方向性を示したメッセージであった。意訳をするなら「的はずれな問いに的確に答えようとするぐらいなら、ズレた答えで正しい問いに挑む方がずっといい」といったところだろうか。

もし我々が的はずれな問いに的確に答え続けようとしているなら、それこそ本当に「不幸な道化」である。データベース・機械学習・統計学の間で走り回っても、この苦役は永遠に終わらない。悪夢から逃れ、穏やかで幸福なプレイグラウンドに辿り着くために必要なのは、的確な答え方ではなくて的確な問いである。的確な問いは、我々がまだ全貌を把握していないだけで現実のあちこちに埋まっている。だから最も避けなければならないのは、その宝探しの場が閑散とすることなのだ。

方法論の世界に留まる者たちの覚悟に敬意を払いつつ、我々はいそいそとデータベース・機械学習・統計学から抜け出して、次のプレイグラウンドに移る。大規模データに潜むパターン解析を切り捨てることが、問題意識に潜むパターン解析を始めるための必要条件となる。現実世界の種々の問題を拾い集め、データの集合でも手法の集合でもなく、問題の集合を手に入れる。今求められているのはデータの中の構造を洗い出すエネルギーを、問題の中の構造を浮き彫りにする力に変えることだ。それが我々が知っている手法たちに明確な目的を与え、それらが活き活きとした形で応用されるような新しい秩序を作り出す。データの外側、かつ認識の内側の場所で遊戯が始まるとき、我々はもう苦役を課せられた「不幸な道化」ではなくなっているはずだ。

\section{おわりに}
「おわりに」と書いてはみたものの、一体何が終わったのかと詰問されているような気分になっている。
本稿の主張を思い切って標語的に纏めてしまえば、《data analysisからproblem analysisへ》ということになるだろう。この方向性は1962年にTukeyが"The Future of Data Analysis"の中で論じたもののはずだが、2012年の終わりを迎える今日になっても悪夢が終わる気配はない。

本稿では、仮説検定からビッグデータに至るまで網羅的にキーワードをかき集め、それらを医学・医療の文脈の中で整理し直すという試みを行った。特に第4節・第5節には最先端の医学的テーマや機械学習の高度な手法も遠慮なく盛り込んだので非常に込み入った内容に見えたかもしれない。しかし《data analysisからproblem analysisへ》という考えを強調するために、問題の構造としては高次元ベクトルに対する2クラス分類問題という一種類のパターンに絞った。

現実世界の問題を集めて問題のパターンを知るという、データや手法を超えたパターン認識。それが未来から現在のものになり、本稿が未来への布石から過去の石ころになるようなとき、我々は悪夢からの逃走すらも悪夢だと笑い飛ばす存在になっているのだろうと思う。だから本当は「おわりに」ではなく、「おわりのはじまりへ」と書くべきだったのかもしれない。そんなことを考えながら、2012年が過ぎていく。

%統計学・機械学習の応用研究においてこの考え方が目新しくなくなり、もはや論ずる価値を持たなくなるような未来を思い描きながら本稿を執筆した。



%、手法に固執しすぎては現実の問題をいつまでも解決できないかもしれない。それぞれの道具が現実世界でその真価を発揮して人間を幸せにするために、手法ではなくて問題を重視する発想にそろそろ切り替えなければならないのだと思う。
%\begin{quote}
%"Those who ignore Statistics are condemned to reinvent it." --- Bradley Efron
%\end{quote}