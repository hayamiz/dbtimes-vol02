% -*- coding: utf-8 -*-

\chapter{パターン認識の「構造と力」---逃走論を超えて---}
% * (アスタリスク)付きの \chapter* コマンドは原則不可とする

\begin{flushright}
 {\headfont 早水 桃子}% ペンネーム
\end{flushright}

\begin{spacing}{0.7}
\normalsize{
\noindent
標題（と傍題\footnote{傍題もやはり浅田彰の『逃走論---スキゾ・キッズの冒険』に因む。}）が示すように、本稿は『構造と力---記号論を超えて』へのオマージュとして書かれている。質の悪いパロディと言っておくのが分相応かもしれないが、いずれにせよ1985年生まれの私が1983年の浅田彰の「力」に今も引き摺られているということは、告白するまでもなさそうだ。勿論、出典が記されている部分を除き、本稿は私という一個人の私見を述べたものに過ぎない。それは各分野の専門家の意見と一致するとは当然限らないし、またそれを期待したわけでもない。

\noindent
まずは歴史の中から一世を風靡した学問の例を引っ張り出し、この場違いな試論のスタンスを明らかにすることから始めたい（第1節）。そのスタンスでデータマイニングというものを批判的に考察する（第2節）。データ解析以外の領域における機械学習の姿を少しだけ眺めてから（第3章）、最先端の医学研究（第4節）と日常的な医療現場（第5節）に進んでいく。そこで直面するのは人間の脳は壊れるという現実であり、一貫した問題意識の中で大規模データベース・機械学習・古典統計学の諸概念を捉え直す。その先にあるのはデータベース・機械学習・古典統計学という枠組みを否定したときに何が残るのかという問いであり（第6節）、本稿の目論見はこの問いに対する答えの一つを提示することにある（第7・8節）。
}
\end{spacing}
 
\section{来し方行く末}
\subsection{無識なる者たち}
『解体新書』は江戸時代の医師たちの試行錯誤によって『ターヘル・アナトミア』というオランダ語の解剖学書から翻訳され、1774年に出版された歴史的書物である。まだオランダ語の辞書すらなかった時代であることを考えると、本文・図表合わせて五冊から成る医学書をわずか数年で翻訳・出版したというだけで大変な偉業である。しかしこの本の偉大さは、当時鎖国政策をとっていた日本に初めて西洋の医学知識を伝導し、それまでの日本の常識を覆したところにある。
『解体新書』以前の日本においては解剖はほとんど行われていなかったため、人体は「五臓六腑」という東洋医学の概念によって捉えられていた。今でこそ我々は人体を色々な臓器の解剖学的な構造と生理学的な機能が作るシステムとして捉えているが、人体の構造を捉える解剖学という枠組みを日本に初めてもたらしたのは、『解体新書』に他ならないのである。

『解体新書』は医学にとどまらず、ヨーロッパで発展した様々な学問から新しい知識や技術を吸収するという「蘭学（洋学）」ブームの契機になり、近代日本科学史に金字塔を打ち立てることとなった。
しかし、『解体新書』の翻訳者の一人である杉田玄白は、彼が85歳で亡くなる2年前に著した『蘭学事始』（1815年）という回想録の冒頭において、このブームを少々醒めた目で眺めている。
%蘭学草創期の出来事を正しく伝えるために、85歳で亡くなる2年前の1815年に『蘭学事始』という回想録を著し、その冒頭でこのように述べている。

\begin{quote}
\ruby{今}{きん}\ruby{時}{じ}、世間に蘭学といふ事専ら\ruby{行}{おこな}はれ、志を立つる人は\ruby{篤}{あつ}く学び、無識なる者は\ruby{漫}{みだ}りにこれを誇張す。
\end{quote}

唐突に、「蘭学」を「機械学習」に置き換えるという悪ふざけを試みる。この操作のバカバカしさとは裏腹に、「無識なる者」が我々に冷たく突き刺さる言葉に化ける。
%バズワードに煽られる烏合の衆を「無識なる者」と批評することこそ、「無識なる者」という言葉に冷や汗をかいていることの裏返しなのではないだろうか。
人体を切り刻んで臓器や組織ごとに描写する解剖学アトラスの構造は、数多くの理論や方法を網羅的に記述することのようでもある。この文脈で『解体新書』に重なるのは、やはり『パターン認識と機械学習』\footnote{本稿の読者には説明不要であろうが、``Pattern Recognition and Machine Learning''(C. Bishop)を邦訳した教科書である。因みに「ベイズ理論による統計的予測」という副題が添えられている。}だろう。自分は「無識なる者」の外側にいるのだと信じようとするほど、内側に引き摺り込まれる------そんな気分にさせられる。


「情報爆発」や「ビッグデータ」へのソリューションが声高に語られる2012年の状況は、
John Naisbittが ``We are drowning in information but starved for knowledge.''（我々は情報の海に溺れ、知識に飢えている）\footnote{John Naisbitt ``Megatrends''(1982)より引用。}と評した1982年から変わったとはお世辞にも言えないはずだ。そして残念ながら杉田玄白が苦言を呈した1815年からも、多分大きな変化はないのだろう。

新しいものに魅力を見出すこと自体は、無論健全な知的好奇心である。いただけないのは「無識なる者」が多数派になることである。それは最先端の花形的研究分野の活気ではなくて、ゴールドラッシュ\footnote{金が採掘された場所に一攫千金を目論む採掘者が殺到すること。1848年にカリフォルニアの川で砂金が発見されたことによるカリフォルニア・ゴールドラッシュが有名であるが、金の鉱脈が見つかるたびに世界各地で幾度となく繰り返し見られる現象である。}を彷彿とさせる熱病と呼ぶべきものだからだ。

\subsection{メルトダウンするブームと心中しないために}
もっとも、人並みの繊細さを持ち合わせている人間であればその熱気にのぼせて付和雷同してはいられないだろう。現在のデータサイエンスをとりまく熱気は単なる空騒ぎに浪費されるかもしれないし、これまでに提案されてきた数々の理論や技法は、現実の問題解決に対する有用性や意義が不明瞭なまま忘れ去られ、それを実装したコードも無用の長物となるかもしれない。データ解析に求められるソフトウェア・ハードウェアに投資しても望む結果が何も得られなかったとしたら、ユーザーの少々無理のある過剰な期待はやがて失望に変わり、それが集団的な絶望へ変わる時にブームは終焉を迎える。最悪の場合は学問分野そのものまで廃れていってしまうかもしれない。

そして蘭学は亡骸になった。一世を風靡し、蘭方と漢方の深刻な医学的対立を招き、蘭書翻訳取締令により思想弾圧の憂き目を見ることになった。19世紀の英国に目を向けてみれば、もっとストレートな例を見出だせる。
産業革命は機械による自動化・作業効率化をもたらした一方で、失業を恐れた労働者集団に機械破壊という行動をとらせてしまい\footnote{この機械打ち壊し運動（ラッダイト運動）になぞらえて、雇用を不安定にするIT化・自動化に反対する思想はネオ・ラッダイトと呼ばれている。}、産業廃棄物という未解決の問題を山積みにしてしまった。どれほど優れたものだと言われてきたとしても、人間はそれを排除することもあるし、新たな問題の元凶と見做すこともある。持て囃されているものが、定着するとは限らない。そんなことはもう幾度となく思い知らされてきたはずだ。
%人間が生み出した新しいものを活かすのも人間であり、殺すのも人間であるということを心に留めておきたい。

とはいえ、「無識なる者」を見下して優越に浸るのは、自意識過剰な批評家だけでいい。ならば時代の潮流に乗るために必要なものとは何なのだろうか。
------もしも思考が淀みかけたなら、今すぐキューを手に取って、ビリヤードの練習がてらブレイクショットを打とう。キューボールは「自ら『濁れる世』の只中をうろつき、危険に身をさらしつつ、しかも、批判的な姿勢を崩さぬこと」。それは「対象と深くかかわり全面的に没入すると同時に、対照を容赦なく突き放し切って捨てること」で、まさしく「シラケつつノリ、ノリつつシラケること、これである」\footnote{あまりにも有名なフレーズ。『構造と力』から立て続けに引用。}。

スタート地点における我々のスタンスが決まったところで肝に銘じておくべきことがある。理論や技法がどれほど高度なものであろうとも、現実世界では人間の目的を叶える手段に過ぎないということである。目的を設定するのも人間であり、道具を使うのも人間である。だから道具の格付けに一喜一憂するとか道具を使うこと自体が目的化するというのは狂信的と言わねばなるまい。道具を「学習」したら終わりという態度だって、望ましいはずがない。「急いで狭苦しい枠組を作り、その中に閉じ込もってチマチマと空白を埋めていくという、一見勤勉そのものの『学習』態度、その実、これ以上の怠惰はない」わけである。目的の見えない虚しい学習は義務教育で卒業しよう。さもなくば人工知能に笑われる。

データベースからの知識発見という舞台を観客として眺めていると、大規模データベース・機械学習・統計学という役者たちの華やかさに惑わされそうになる。「道具」たちは何ができて、何ができないのか。互いにどのように関わり、どこでどのように応用されるべきなのか。それが混沌を極めているとしたら、おそらく何かとてつもなく単純なことを見落としている。それは目的が存在しなければ道具箱もショーケースになってしまう、ということだ。カオスの中から浮かび上がるストーリーは、人間の問題意識や目的なしに始まるはずがなく、そして終わるはずもない。

\section{データマイニング再考}
まずはJerome H. Friedmanの``Data mining and Statistics: What's the connection?''(1997)を下敷きにして、データマイニングとは何であるのかを掘り下げてみよう。

\subsection{データマイニングとデータベース}
データから知識を得ようとすること自体に目新しさはないはずだ。では、何が今頃になってデータマイニング「特需」をもたらしたのだろうか。この問いかけに答える鍵の一つは、データベースマネジメントシステム（DBMS）の変遷である。
%Imielinski（1995年）

かつてのDBMSにおいては、銀行のATMで利用されているようなオンライントランザクション処理（OLTP）と呼ばれる情報処理が主流であった。つまり、DBMSに求められていたのはデータを貯蔵し、多くのユーザーからの小規模なデータアクセスを伴う定型クエリーを高速に処理するという能力だった。
%http://otndnld.oracle.co.jp/deploy/performance/pdf/Oracle8i-tuning2.pdf
しかしデータベースに蓄積されたデータを様々な切り口で分析したい人々が
データベースに意思決定支援を求めるにつれて、新しい情報処理が注目を集めるようになった。それが
オンライン分析処理（OLAP）である。

OLAPによってユーザーは対話式の試行錯誤を繰り返して、データウェアハウスに格納された膨大なデータを集計したり、あるいは多次元的に分析したりといったことができるようになった。
一見するとこれはデータマイニングツールの原型のように見えるかもしれない。しかし、OLAPによるデータ解析によってデータウェアハウスと「対話」できるのは、的確な仮説を考え続けるような不断の洞察力とクエリーを書き続ける技術力（および時間と体力）を持ち合わせた選ばれし人間だけである。データに潜むパターンやモデルの叩き台を自動的に見つけてくれるものではないし、ビジネスの場における迅速な意思決定に向いているものでもない。だからこれは
データマイニングツールの原型と言うよりはむしろ、人々がデータマイニングツールを求める素地を作ったと考えたほうが良いだろう。
%大規模なデータからパターンを見出す試行錯誤のプロセスが人間の手を煩わせずに実現するならばそれが効率的であるし、data analysis for everyoneみたいなものへの需要は高まっていった。

\subsection{プロパガンダとしてのデータマイニング}
データマイニングもまた他のバズワードの例にもれず定義が曖昧な言葉だが、色々な定義をまとめるとしたら「
複雑で膨大なデータ集合の中に埋もれていて人間の力では見いだせないような有益な知識をコンピューターの力でデータベースの中から探索的に発見し、ビジネスにおける意思決定に役立てるような色々な手法（またはそれらを使ったプロセス）」というところであろう。実際のデータ解析にはIBM Intelligent miner、SAS Enterprise Miner、SPSS Clementineなどの汎用データマイニングツールが使われることが多い。

Knowledge Discovery in Databasesというデータマイニングの目論見は壮大なものである。しかし、
%データから新たな知見を得ようとする試みは全然斬新なものではない。
ユーザーが小難しいデータベースの構造やデータ解析の中身を知らなくてもデータベースに格納されたデータにアクセスすることを可能にし、それを分析したり視覚化したりしてくれるデータマイニングツールというものは
誰のためのものなのだろう。
「データ自身に語らせる」といえば聞こえは良いが、ビジネス戦略のヒントとなるような知識を囁くかどうかは別の話であるし、そういう都合の良い話はあまり聞かない。Friedmanが指摘するように、汎用データマイニングツールは非常に高額な商用ソフトウェアであり、導入するとなれば当然相応の高価なハードウェアやストレージも要求されることになる。その意味で、データマイニングはもはや金脈を掘り当てようとする宝探しのような試みというより、むしろデータマイニングと聞いてソフトウェアやハードウェアに投資してくれるような、データを持て余している企業の意思決定者や潜在的データマイナーを発掘するための商業的プロパガンダと化しているところがある。

\subsection{データマイニングと統計解析の違い}
Friedmanは汎用データマイニングツールが提供する主要な手法の数々を
%\footnote{Decision tree induction・rule induction・nearest neighbors・clustering・association rules・feature extraction・visualisation、neural networks・graphical models（Bayesian belief networks）・遺伝的アルゴリズム・SOM（自己組織化マップ）・neuro-fuzzy systemsと}
を列挙し
%\footnote{Friedmanは定番手法としてdecision tree induction(C4.5, CART, CHAID)・rule induction(AQ, CN2, Reconなど)・nearest neighbors(事例ベース推論)・clustering methods(data segmentation)・association rules(market basket analysis)・feature extraction・visualizationを挙げ、その他の手法としてneural networks・bayesian belief networks(graphical models)・genetic algorithms・self-organizing maps(SOM)・neuro-fuzzy systemsを挙げている。}
、多くのデータマイニングツールにおいて
仮説検定や判別分析など、統計学者にとって見慣れた手法は
殆ど無視されていると評している。一言で「データを解析する」と言っても、データマイニングと統計解析は目的も手段も大きく異なるものであり、両者を混同してはならない。

統計学的なデータ解析は、「データ自身に語らせる」という大らかなものではない。
それは「実験が終わった後に統計学者にコンサルトするのは、死後に解剖を依頼するようなもので、統計学者はその実験の死因を教えることぐらいしかできない」というRonald Fisherの格言にも端的に表れている。
統計学者にとってはデータは綿密な実験デザインに基づいて注意深く収集され、吟味されるべきものであり、
鉱山のように元々そこにあるものではないのだ。

それに、データを説明するための仮説やモデルがなければ仮説検定もモデル選択\footnote{モデルのパラメータを最尤推定する「モデル推定」とともに、どのモデルを選ぶべきかという「モデル選択」は統計学・機械学習においては最も重要なテーマの一つである。AICやBICといった情報量基準が有名だがクロスバリデーションもよく使われる。}も始めようがない。良質なデータと洗練されたモデルこそデータ解析の要なのである。モデルが正しければ良い結果を出し、正しくなければ悪い結果を出すのがアルゴリズム（計算手法）のあるべき姿なのだから、解析結果の善し悪しはアルゴリズムではなくモデルが決めていると考えるのが筋である\footnote{本稿執筆時にインターネット上で配信されていた伊庭幸人氏（統計数理研究所）の「マルコフ連鎖モンテカルロ法の基礎」という講義の中でもモデル・統計手法・アルゴリズムの違いが強調されていた。}。

これに対しデータマイニングは、良く言えば従来の統計学が扱えないような整理されていない大規模データを何とか使える形にし、色々な切り口で分析して意思決定のヒントになる知識を見出そうとする果敢な取り組みである。悪く言えば産業廃棄物のような膨大なデータの山をゴミだと認めるわけにいかないからといって往生際悪くデータのフォーマットを整え、クレンジング\footnote{データの誤りや重複を洗い出し、使えるデータにするという泥臭い（しかし最も重要な！）プロセス。}を施して場当たり的に試行錯誤しながら有象無象の仮説やモデルの叩き台をあれこれ捻り出すという作業でもある。

いずれにせよデータマイニングは統計学とは全く異なる思想を持つものであり、仮説やモデルなしに鉱山の中を探索するデータマイニングにおいてよく登場する手法が、統計学の手法と異なるのは驚くべきことではないということが分かる。データマイニングが語られる文脈においてはテクニックやアルゴリズムありきの話になりがちであるのも、当然の成り行きなのかもしれない。

\subsection{データマイニングの意義}
しかしながら
%データマイニングに失望したり、ベンダーやコンサルティングファームに対して猜疑的になったり、
データマイニングの努力は水泡に帰するのかと悲観する必要はないし、それは本稿における我々のスタンスに反するはずだ。そこでの努力を無にしないためにできることがある、と考える程度の希望は持っていたい。

データマイニングに対して批判的になるのをここで一旦やめてみよう。少し楽観的になれば、データマイニングの功績が色々と浮かび上がるに違いない。従来の統計学では難しいような大規模で複雑なデータの解析も、プラクティカルな前処理の方法論も、複雑なデータの可視化による迅速な意思決定支援も、データマイニングなしには語れない。様々なテクニックを集めたツールとデータ解析の専門家だけでは解析結果が荒唐無稽なものになりがちだったが、それはデータマイニングは役立たないと切り捨てる理由にはならないはずだ。むしろデータ解析には現場の知識を持った専門家の関与が必須であるということを実証したといっていい。その教訓を\ruby{採掘}{マイニング}できなければ、ちょっとお粗末である。

データマイニングの商業的な側面ばかりを強調したが、大規模データベースを持っているのは企業だけではない。
後述するが、公開されている大規模なデータベースも色々と存在する。
そういうデータベースから知識を発見する試みにおいてデータマイニングは単なる商業的なプロパガンダに堕したものではなく、真理の探究を目指す学術研究において仮説やモデルのプロトタイプを見つける手段となったり、蓄積されたデータから合理的に最適な政策を決定するという国家や国際社会の意思決定を支援するためのツールとなり得る。

データマイニングは、クラスタリングや決定木学習に代表されるように、多様なパターン認識・機械学習の理論的な枠組みを現実の大規模データに応用してきた。そうした理論と現実の橋渡しをする取り組みは軽視されてはならないし、その価値は商業的な実用性や理論的な厳密さや一般性だけで評価されるべきではない。そうでなければデータマイニングバブル崩壊とともに、データベースから知識を発見することの重要性も失われかねないからだ。

\section{データベースの外側}
データ解析のことばかり考えていると、統計学もパターン認識も機械学習もデータベースを掘り起こす道具にしか見えなくなるかもしれない。このまま先へ進む前に、ほんの少しだけデータベースの外側に目を向けてみる。
\subsection{医療機器}
統計学や機械学習で登場するアルゴリズムの幾つかは、医療の現場で人々の健康な生活や生命を支えている計算手法と言っても過言ではない。医用画像を得るためには画像再構成という重要なプロセスがあり、そこではバックプロパゲーションやEMアルゴリズムといった機械学習・統計学でよく知られる手法が一般的に用いられている。
また、病気の検査のためだけでなく、
がんの放射線治療においてはモンテカルロ法が古くから線量分布推定に用いられ、
今もなお放射線治療計画に必須の計算手法であり続けている。

\subsection{神経科学}
もともと機械学習は人工知能研究の一分野であることから分かるように、理論神経科学と深く関係しており、その理論的な枠組みそのものが、人間の脳のメカニズムや、社会における人間の行動などを理解する一助となり得るものである。パーセプトロンは古くから小脳のモデルとして知られ、人間の運動学習の神経基盤を理論的にも実験的にも明らかにし、ロボットの運動制御という工学的な応用にもつながることとなった代表的な例である。
もう少し最近の例としては強化学習（特にTD学習）によるモデルがあり、これは大脳基底核の計算モデルとして注目されている。ドーパミンニューロンが報酬予測誤差によって強化学習を行っているという仮説が提唱され、報酬に基づく人間の意思決定の神経基盤が理論的にも実験的にも精力的に研究されている。こうした研究はパーキンソン病という大脳基底核が侵される神経変性疾患や、薬物・ギャンブル等への依存症などにも密接に関連している。

氷山の一角を一瞥しただけでも、これまでとは違った
統計学や機械学習の顔を垣間見ることができる。
とりわけ人間の脳につながる研究は興味深くて実際に有益でもある。しかしロマンティックな物語は別の機会に譲り、もう少しリアルで差し迫った話をしよう。

\section{認知症の研究：大規模データベースと機械学習の応用}
%日本人の平均寿命は世界の先進国の中でもトップクラスであり、それは高度に発達した現代の医学と、充実した医療サービスを誰でも受けることができるようにするための国民皆保険制度の賜物である。しかし
周知の通り、日本を筆頭に世界各国（特に先進国）で高齢化が社会問題化している。認知症患者は増え続けており、医療費の増加を通じて国の財政を圧迫し、若い世代にも大きな負担となっている。

認知症は社会だけでなく個人にとっても人生を脅かす問題である。
厚生労働省によれば日本では現在既に65歳以上の高齢者のうち約10\%が認知症であると見積もられており、しかも今後さらに増加していくと推定されている\footnote{厚生労働省による資料：http://www.mhlw.go.jp/kokoro/speciality/detail\_recog.html
}。
これは10人のうち誰か1人が生贄になるというロシアン・ルーレットではない。むしろ海の中に浮かぶ島の10分の1が水没している、そんなイメージである。じわじわと満ちてくる潮に抗えない無力さが、生々しい光景となり脳裏を掠める。

\subsection{認知症：背景知識の要点}
認知症というのは単一の疾患ではなく、認知機能低下を呈する多くの疾患の総称である。認知症の中で一番多くて重要なのがアルツハイマー病である。

\subsubsection*{アルツハイマー病はハードウェアの異常である}
コンピュータの調子が悪くなったら、まずはハードウェアレベルの異常なのかソフトウェアレベルの異常なのかを考えるのが自然ではないだろうか。それと同様に、長年使ってきた脳がうまく動かなくなる場合も、原因はソフトウェアのレベルとハードウェアのレベルとに分けられる。

認知「機能」の障害と聞くとソフトウェアレベルの問題のように思うかもしれないが、アルツハイマー病は正常な脳の「構造」が壊れることにより機能に支障が出る病気で、ハードウェアの著しい劣化による脳の故障と考えるべきものである。

具体的には、正常な人でも加齢で蓄積するゴミのようなもの\footnote{ゴミの正体について少しだけ詳しく述べれば、アルツハイマー病は脳内に$\beta$アミロイドとよばれるタンパク質が蓄積し、異常リン酸化タウタンパクが蓄積してしまうことで神経細胞の変性・機能不全が起こり、神経細胞死を招く疾患である。それが肉眼的・画像的には脳萎縮という所見となる。}が異常に脳に溜まってしまうために神経細胞がどんどん死んでしまい、脳のボリュームが目に見えて減ってしまうのである。

\subsubsection*{アルツハイマー病の根本治療薬とワクチン開発への動き}
ハードウェアの障害であると繰り返したのは、病気が進行して重症化してからでは有効な治療法がないことと、MRI画像で脳の形やボリュームを見ることが診断に有効であることの二点を強調するためである。
壊れかけの初期のうちに食い止めるのが望ましいし、壊れないように予防するのが究極的な理想といえる。

そういうわけで現在多くの巨大な製薬会社や医療機器メーカーがベンチャー企業とも協力し、アルツハイマー病の根本治療薬やワクチン開発を目指して研究開発に取り組んでおり、実際に日本でも複数の治験が進行中である。
根本治療薬・ワクチン開発への動きによって、脳の異常の客観的な評価方法や早期診断手法の重要性は急速に高まった。要は完全に脳が壊れてからではなく、なるべく小さいうちに病気の芽を見つけ出し、できるだけ早い段階でその芽を摘み取りたいのであり、その芽を見つける方法が求められているのである。
\subsection{ADNIデータベース}
2004年に米国では、6000万ドルを投じた\ruby{US-ADNI}{ユーエスアドニ}（米国アルツハイマー病神経画像イニシアチブ）がスタートした。開始当初の予算の規模を一瞥しただけでも、これが国家的な臨床研究プロジェクトであることを窺い知ることができるだろう。
US-ADNIは800人以上の被験者を6ヶ月毎にスキャンして得た
経時的な画像データ
\footnote{
MRIの形態画像だけでなく、FDG-PETという機能画像やPiB-PETによる発症前診断のためのアミロイドイメージング画像も含む。}や認知機能検査の点数・色々な生物学的マーカー・臨床的な因子の有無といった情報をデータベース化するだけでなく、認知症研究を推進するために全データをインターネット上で公開するという試みである。
それを支える技術もまた高く評価されている。例えば医用画像データの質は施設ごとの撮像方法や装置そのものに大きく影響されるため、多施設から得られたデータをそのまま解析することはできないのだが、ADNIではデータに施設や装置の違いを補正するための前処理を施しており、不揃いなデータを信頼できる品質にしてからデータベースに収めている。勿論データは全て匿名化され、撮像翌日にはwebサイトに上がってくる\footnote{岩坪威「アルツハイマー病の発症前超早期治療に向けてのデータを蓄積」INNERVISION (26・11) 2011}。

因みにこのプロジェクトはヨーロッパ・オーストラリア・日本・台湾・韓国にも存在し、日本でも2007年にJ-ADNIが始まった。
US-ADNIの方はさらにアルツハイマー病の早期診断・発症予測に焦点を当てたADNI 2に乗り出しており、今や被験者の全ゲノムシークエンスをも公開するビッグデータプロジェクトに変貌しつつある\footnote{もし800人の被験者の全ゲノム配列が得られたらデータベースに追加されるデータのサイズは少なくとも165テラバイトになると見込まれている。http://www.adcs.org/research/PDF/ADNIExclusiveFall2012.pdf}。

\subsection{学際的研究の基盤として}
US-ADNIのデータは誰でも自由に無料でダウンロードすることができる。もっとあからさまな言い方をしてもいい。最先端の検査を駆使して得られた大規模な患者データを手に入れるために、医師免許は要らないのである。手を動かして実験するのが億劫というならそれでも構わない。データはどのように利用しても良いし、解析した結果を論文として発表することは咎められるどころか推奨されているのである\footnote{ADNIのデータを使った研究の一覧はhttp://adni.loni.ucla.edu/publications/で見ることができる。}
。認知症の画像研究を漁ってみると実に多様な機械学習の応用研究を目にすることになるが、こういった事情を鑑みれば何ら不思議はない。US-ADNIという大規模な公開データベースは学際的な研究の場を作り、それを発展させている。

機械学習を応用する側の視点で見れば、確実に正しい教師データが入手できるというのもADNIデータセットの長所である。実は病院で日常的に行われている臨床診断は、厳密には100\%正しいラベル付けとは限らない。それはアルツハイマー病の確定診断をするためには顕微鏡で実際に「ゴミ」が溜まっているのを見なければならないため、手術室で生きている脳から組織のサンプルを採取することができた場合を除き、真の答えは死後の解剖なしには確かめようがないからである。ADNIは人間の医師が付けたラベルのみならず、判明している限りの「真のラベル」も公開している。だから訓練データの中にあるかもしれないノイズに悩まされることなく、安心して教師あり学習アルゴリズムを使うことができる。
\subsection{機械学習による自動診断から発症予測まで}

\subsubsection*{MRI画像の自動診断}
MRI画像「だけ」でアルツハイマー病を正確に診断するのは人間の医師であっても訓練が必要である。
ところがサポートベクターマシン（SVM）という分類器は認知症症状の程度や年齢といった情報を一切使わず、MRI画像だけで重症なアルツハイマー病の患者と健康な高齢者を95\%ほどの感度・特異度で分類することに成功してしまった\footnote{Kl\"{o}ppel, et al. Automatic classification of MR scans in Alzheimer's disease. Brain (2008), 131, 681-689}。
ここで用いられたSVMは線形SVMというシンプルな教師あり学習アルゴリズムである。おさらいの意味で少しだけ詳しく述べると、非常に高次元なMRI画像を数万次元の長いベクトルとみなし、画像同士の類似度をベクトルの内積として計算し、画像に貼られたラベルから病気か否かの線引きを学習するという方法である。
そんな単純な手法であるにも関わらず、アルツハイマー病患者の脳と健康な高齢者の脳をはっきりと識別し、さらにアルツハイマー病患者の脳とFTLDという別の認知症患者の脳もかなり正確に識別した。勿論この正確さは過学習によるものではない。米国のデータで訓練されたSVMは英国のデータを正しく分類し、その逆もまた然りであった。我々は「所変われば品変わる」をあまり疑わずに暮らしているし、英語にも``So many countries, so many customs.''という表現がある。ところがSVMは所が変わろうと品が変わろうと安定した識別を行い、高い汎化能力を見せつけたのだった。

\subsubsection*{線形SVMを越えられない？}
MRI画像からアルツハイマー病を自動診断するための手法を改良しようとして、その後も色々な手法が提案されてきた。しかし、後述するDARTEL\footnote{The Diffeomorphic Anatomical Registration Through Exponentiated Lie Algebraという正式名称がある。開発者はJohn Ashburner。}というアルゴリズムによって前処理の方法論は躍進を遂げたものの、特徴選択・特徴抽出・分類に関しては結局あまり大きな手法の改良はなかった。

色々な手法の性能比較は難しい問題だが、Cuingnet\footnote{Cuingnet, et al. Automatic classification of patients with Alzheimer's disease from structural MRI: a comparison of ten methods using the ADNI database. Neuroimage (2011),56(2),766-81 Epub 2010 Jun 11}はADNIデータベースのデータを用いてこれまでに提案されてきた手法を追試し、比較するというメタアナリシスを行った。それによると、アルツハイマー病患者と健康な高齢者のMRI画像診断は大体どのやり方を使ってもそれなりにできるものだったが、DARTELというアルゴリズムを含む前処理を施した後、脳の全領域の情報を使って線形SVMで識別するのがやはり最善のようであった。

工夫を凝らした複雑なテクニックをシンプルな手法がせせら笑っているかのような結果である。しかし、MRI画像は非常に高次元で多くの情報を含んでいるために非線形な特徴抽出をする必要がなく、また前処理のアルゴリズムが洗練されていったことでノイズを恐れて脳の領域を一部分に限定するという特徴選択をする必要がなくなり\footnote{脳MRI画像は非常に高次元なデータなので、良い特徴選択ができたら効率的であるが、脳の領域を予め人間が選択すると汎化能力が低くなるという問題がある。訓練データから重要な領域を学習させることも試みられてきたが、時には数週間の計算を要するほどに計算量がやたらと増えるばかりで、結局分類の正確さを向上させようとすると脳の領域全ての情報を使うに越したことはないという結論になってしまった。また、カーネル関数の選択についても色々と試行錯誤されたのであるが、結局は線形カーネルに落ち着くことになった。特徴ベクトルが非常に高次元であるため、線形カーネルで十分なのであろう。}、重症アルツハイマー病患者の脳と健康な高齢者の脳を分類するという程度の問題であれば、全脳のデータを線形SVMに突っ込むだけで十分になったのだと考えることができるだろう。

\subsubsection*{問題意識と問題パターン}
迷子にならないように論点を整理しよう。我々が目指しているのはアルツハイマー病というハードウェアの障害を壊れかけの初期のうちに食い止めることであったはずだ。だからこそ完全に壊れてしまってからではなく、壊れ始めている早期に正常と異常の境界を引くことができるような方法を求めているのである。

重症なアルツハイマー病と健康な高齢者の脳の違いは人間には一目瞭然である。だからそれを識別するような2クラス分類問題なんて、線形SVMで事足りるという話である。厄介なのはあまり重症ではないアルツハイマー病と健康な高齢者の2クラス分類問題であり、ましてやまだ認知症を発症していないような「ちょっとボケかけ」の人々\footnote{分かりやすくするためにだいぶ露骨な物言いになってしまったので正しい言葉でも説明しておく。こうした「ちょっとボケかけ」の人々は軽度認知機能障害（MCI）と診断されており、白黒がまだつけられないケースに貼られるグレーなラベルである。このグレーゾーンの人々の中にはやがてアルツハイマー病を発症してクロになる人もいるし、ずっとグレーのままという人もいる（前者はMCI converter、後者はMCI non-converterといわれる）。アルツハイマー病の早期発見とか発症前診断というのは、このグレーな人々の中から将来クロになる人々を見出すというタスクである。}を将来認知症を発症する人としない人に分けるという2クラス分類問題となると人間のエキスパートもしばしば苦慮してしまうほどの難問なのである。

先ほどのCuingnetらの解析によれば、あまり重症ではないアルツハイマー病と健康な高齢者の2クラス分類問題ではどの手法でも途端に感度が落ちてしまった。そして「ちょっとボケかけ」の人々を将来認知症を発症する人としない人に分ける2クラス分類問題となると、どの手法もお手上げだったのである。

\subsubsection*{現実世界からのヒント}
理論の世界で現実を見下す者の頭上で、雲のような現実が泳いでいることがある。
これまでの手法はMRI画像だけで診断をしている。まずはこれに気づきたい。
医者が日頃何をしているかを想像できたなら、
複数の検査情報を統合して診断する枠組みが自然と必要になる。
それに加えて、複数時点の検査データも扱いたくなってくる。患者を診るには一時点のMRIだけではなくて、もっと経時的な情報が欲しいのだ。脳が萎縮するスピード、欲を言えば非線形の変化も特徴ベクトルに含めたい。

それから、感度や特異度という数字を上げることだけに拘泥しがちではあるのだが、そればかりが手法の「改良」とは限らないということも指摘しておく必要がある。多くの説明変数を使えばより正確なpredictionができるかもしれないが、データが複雑になればなるほどシンプルな説明変数が望ましくなるということを忘れてはいけない。
複雑な脳の画像データを全部使う代わりに、できる限り本質的な領域を選び抜けるならばそれに越したことはない。それができなければどれだけ機械が正確に分類できるようになったところで、人間にとって結果の解釈は難しいままになるからである。

\subsubsection*{スパース正則化・マルチカーネル学習}
スパース正則化は観測されたデータをいかに少ない変数で説明するかという問題の近似解法である。また、スパース正則化の特別な場合としてマルチカーネル学習があり、これは多様な情報源からの情報をうまく統合するための枠組みとして注目されている\footnote{詳しい説明は冨岡 亮太・鈴木 大慈・杉山 将の「スパース正則化およびマルチカーネル学習のための最適化アルゴリズムと画像認識への応用」（画像ラボ 2010.4.5）を参照。}。

これは現在我々が直面している問題にどのように役立つのだろうか。スパース正則化を用いた特徴選択は、複雑な脳の画像データから本質的な領域を選び抜くために用いることができる。脳画像をそのまま数万次元のベクトルとして捉えるのではなく、早期診断に必要な幾つかの本質的な領域だけを重要な説明変数として使うのである。

マルチカーネル学習についてはどうか。これはMRIだけでなくPETや認知機能検査などの様々なモダリティのデータを最大限に活用するために使うことができそうだ。MRI画像からカーネル行列を1つだけ作って分類器にかけるのではなく、モダリティごとに作ったカーネルを凸結合したカーネル行列を分類・回帰に使うのである。これはそれぞれのカーネルの最適な重み付けを学習できるので、どの検査をどのぐらい重視するのかということも最適化することができる。しかもそれが凸最適化なので、大域的最適解が求まるという強みがある。
\subsubsection*{マルチカーネルSVMによるアルツハイマー病の発症予測}
こうした機械学習の先端的な枠組みが現実世界でどのぐらい通用するのかを試そうと思ったら、ADNIデータベースに向かおう。Zhangらはまさにスパース正則化による特徴選択とマルチカーネルSVMをADNIデータセットに応用している
\footnote{Zhang et al. Predicting future clinical changes of MCI patients using longitudinal and multimodal biomarkers. PLoS One (2012),7(3),e33182. Epub 2012 Mar 22}。
%多様な情報源からの情報を結合カーネルという形で統合し、マルチカーネルSVMを用いた分類・回帰を発症前のアルツハイマー病の識別や将来の認知機能の点数予測に応用するという研究を行った。
MRI画像にSVMを適用しただけでは特に感度が低さが目立ち、認知症の早期発見や発症予測という問題には太刀打ちできなかった。しかし経時的な情報を含めたマルチカーネルSVMによってかなり感度が改善され、発症前の診断は随分と正確なものになった。

実は、MRI・PET・脳脊髄液中のバイオマーカーというそれぞれの情報源から得られた情報を使ってカーネル行列を作った方が良いのはカーネル行列を視覚化すれば一目瞭然である。一見したところではPETや脳脊髄液中のバイオマーカーのカーネル行列はMRIのカーネル行列に比べて「使えない」もののように見える。しかし3種類のカーネルを結合して得られたカーネル行列は、MRI単独のものよりも確かに見やすい構造になっているのだ\footnote{カーネル行列をカラーの図として見ることができる。Multimodal classification of Alzheimer's disease and mild cognitive impairment. Neuroimage. 2011 Apr 1;55(3):856-67. Epub 2011 Jan 12}。つまりそれぞれのモダリティはどれかが優れているとか劣っているとかではなくて、それぞれに含まれる情報は互いに相補的なものであるということである。MRIは形態を見る画像検査で、PETは機能を見る画像検査で、脳脊髄液中のバイオマーカーは画像検査に直接現れるものではないのだから、当たり前といえば当たり前である。

Zhangらの研究に使われている特徴選択・特徴抽出アルゴリズムの詳細は本稿では省く。特徴選択の勘所は、ADNIで6ヶ月毎に撮像された複数時点の脳画像データから、L2,1ノルム正則化によってどの時点でも重要になるような重要な脳の領域をスパースな解として得ているということにある。また、画像を$t$（時点）の多項式と考えて直交多項式展開した際の係数を特徴ベクトルに含めるというのが経時的な特徴の抽出の要点である\footnote{2次以上の多項式で捉えることで、萎縮のスピード（$t$の一次式）だけではなく非線形な変化に関する特徴も抽出している。}。

一時点のMRI画像だけでは困難な問題であっても、現在までの経時的な変化を辿り、訓練された人間の医師のように
構造画像だけではなく機能画像やカルテに散らばった様々な情報を統合することで、半年後に認知症を発症するか否かを80\%ほどの感度と特異度で識別することができるようになった。この解析で使われたのはMRI・FDG-PET・認知機能検査の情報だけであり、血液や脳脊髄液中のバイオマーカーの情報や、現在発症前診断法として最も注目されている[$^{11}$C]PiB-PETという画像の情報は使われていない\footnote{ADNIデータベースといえども、全ての検査を受けている被験者はあまり多くはないからである。}。これらの情報を取り入れればマルチカーネルSVMによる発症予測はより正確なものになると思われる。

\subsubsection*{国家レベルの意思決定支援へ}
根本治療薬開発への動きが激化している現在、我々の直面する問題は単に病気の人と健康な人の識別というレベルを超えるものになっている。ハードウェアが壊れてしまう前に劣化を食い止めることができるか否か。それは人生の明暗を分け、人類の運命の分かれ道となる。

大規模なデータベースと機械学習の応用研究によって発症予測の客観性が高まっていけば、それは医療政策という国家レベルの意思決定を支援するものになる。
治療薬をどこまで保険適用とするかという意思決定が合理的であるためには、
単に医師個人の経験やその寄せ集めによる発症予測では不十分なのである。


\section{認知症の臨床：80人のデータベースと統計学の成果}
さて、大規模データベースや機械学習の話に少しページを割き過ぎてしまったのだが、実は実際の日常診療において活躍しているのはもっと小規模なデータベースと古典的な統計解析である。

\subsection{Statistical Parametric Mapping (SPM)}
脳の構造や機能に関する研究の発展はSPMなしにはなかっただろうと思えるほど、SPMというソフトウェアは脳画像統計解析によって神経科学・心理学・精神医学・神経内科学に貢献してきた。SPMはロンドン大学（UCL）のKarl Fristonらが開発した脳画像の統計解析ソフトウェアであり、脳の画像\footnote{SPMはMRIのみならずfMRI・PET・SPECT・EEG・MEGといった多様な脳画像を扱うことができる。}の統計解析をより信頼できるものにするための前処理アルゴリズムに加え、古典的な統計学的仮説検定からベイズ推定に至るまで様々な解析手法が網羅的に実装されている。

SPM自体はオープンソースであり、無償で配布されているのだが、SPMを動かすには残念ながらMATLABという高価な数値計算ソフトウェアが必要である。SPMを用いた研究によって認知症診断にクリティカルな知見がどれだけ解明されようと、脳画像の統計解析が象牙の塔から開放されなかったというのは想像に難くないだろう。大学や研究所などの附属病院などでもない限り、そうした知見は医療の現場へスムーズにおりてくるものではなかったのだ。
\subsection{日常診療のための脳画像統計解析}
幸いなことに、日本では\ruby{VSRAD}{ブイエスラド}というアルツハイマー病早期診断支援ソフトウェアが登場したおかげで、アルツハイマー病早期診断のための脳画像統計解析は医療現場で簡単に使えるようになっている。VSRADはSPMの前処理アルゴリズムや、後述するVBMという脳の形態解析手法をSPMの開発元に許可をとってスタンドアロンアプリケーションにしたものである。WindowsがインストールされたパソコンさえあればMATLABをインストールしなくても動くし、MRI画像解析の専門知識がなくても撮像されたMRI画像を読み込ませて簡単な操作をするだけでMRI画像に前処理を施し、認知症診断のための統計解析結果を出力してくれるという有り難いツールである。
VSRADはエーザイという製薬会社が無償で提供しており、ダウンロードすればすぐ使えるためVSRADは日本では1000以上の施設でルーチン化され、脳ドックでも使われているほどに広く普及している。
因みにMRI画像ではなくSPECT・PET画像を解析するための同様のソフトウェアとしてはeZISやiSSPがある。これらも無償で提供されているソフトウェアであり、日常的にお目にかかることができる。

\subsection{古典統計学の誘惑}
\subsubsection*{VSRADに見る古典統計学の強み}
VSRADの統計解析の基盤となっているのは同じくロンドン大学（UCL）のJohn Ashburnerが開発したVBMである。
これはある群の脳MRI画像の各ボクセル\footnote{ボクセルはピクセルが3次元になったようなものである。}の灰白質濃度がある群のそれと比べて有意差があるかを検定\footnote{ボクセルごとにt検定をすると当然多重比較の問題が出てくることになるが、SPMではGaussian random fieldによる補正をしている。何故Bonferroni法ではダメかといえば、各ボクセルは空間的に連続しているから明らかに独立ではないし、しかも大抵前処理の段階でガウシアンフィルタによるスムージングもかけられたりしているからである。}するというものである。例えば、若年者グループと高齢者グループの脳画像データを比べれば、高齢者で有意に萎縮している脳の領域を調べることができる。

VSRADはこのVBMを基礎にしており、調べたい患者のMRI画像と80人の健常者のMRIデータベースとを比較し、Z検定統計量（Zスコア）のカラーマップをMRI画像上に表示してくれる。このカラーマップを見れば有意に萎縮している脳領域が一目で分かるようになっている。
さらに日常診療で使いやすくするために早期アルツハイマー型認知症診断に重要な海馬傍回という領域内のZスコアをざっくりと平均して、萎縮の程度の目安を表示してくれたりもする。
勿論、偏差値だけでは把握できない学力があるように、Zスコアだけで早期アルツハイマー病を完璧に診断することができるはずもないので、あくまでもVSRADは診断支援という位置づけのソフトウェアであることを忘れてはならない。それでもこのZスコアだけで早期アルツハイマー病の正診率は87.8\%だったという報告\footnote{Y. Hirata, H. Matsuda et al. : Neuroscience Letters 382（2005）}もあり、『VSRAD適正使用ガイドライン』\footnote{http://www.vsrad.info/general/manual/download/guideline1.pdf}が出されてしまうほど、良くも悪くもポテンシャルの高さが認められている。

\subsubsection*{さようなら、ビッグデータ}
保持しておかなければならないデータや検索しなければならないデータのボリュームが爆発的に増える限り、大規模なデータベースのデータを効率的に検索したり集計したりするアルゴリズムがますます重要になることに疑念の余地はない。
しかしながらデータ解析となると話は別である。これは全ゲノムシークエンスを公開するというUS-ADNIのビッグデータプロジェクトは良いか悪いかという話ではなくて、大規模なデータがあったらそれを全部使わなければならないなんてルールはどこにもないという話である。

大きな目標を達成するために必要なものは大規模データベースや技巧的なアルゴリズムとは限らない。むしろサンプル数があまり多くないデータセットや統計学的仮説検定のようなお馴染みの理論かもしれない。
だから少なくともデータ解析の世界においてはビッグデータの幻影に怯える必要はない。
大きければ大きいほど良いなどというちっぽけな固定観念が、偉大な知識発見の妨げになってしまうかもしれないということの方が余程恐ろしいのだ。

\section{知識発見の「悪夢」}
灰色らしい灰色と、黒に近い灰色を分けるという問題。その難問に挑み続ける大規模データベースと機械学習の姿は勇ましいものだった。しかしVSRADを目にすると、800人もの被験者を集めた大規模なデータベースや奇を衒った機械学習アルゴリズムなんかよりも、健常者80人の脳MRIデータベースと仮説検定の方が眩しく輝いているように見えてくる。

しかし当然、古典統計学に引き篭もるというのは退行と言わざるを得ない。我々が大規模なデータベースや機械学習に目を向けた理由は何だったのだろうか。我々は古典統計学に扱えないようなものを扱いたかったのではなかっただろうか。古き良き時代の枠組みに逃げこんだところで、それはほんの一時の雨宿りに過ぎないはずだ。
古典統計学に閉じこもっていては新しい仮説もモデルも見つからず、古典的な統計学的仮説検定の力はやがて色褪せてしまうに違いない。

パラフレーズを試みるべく、古典統計学を熱心に「木」を見ることによって深い「森」を知ろうとするアプローチだと考えてみよう。このアプローチにしがみつく限り、あなたの視線が「森」に注がれることはない。ならばあなたは「森」を知らずに「木」を認識できるのだと思っていることになる。しかしあなたが熱心に見ているものは本当に「木」なのだろうか。「木」の構造が、かたちが、パターンが、ゲシュタルトが、崩壊し始める。

この逆説のおぞましさに恐れ慄いたあなたは、「木」の呪縛から逃れようとして再び大規模データベースに向かって息つく暇もなく走りだす。辿り着いた先は、古典統計学が無力となる樹海である。それでもあなたは機械学習に光を見出し、知識を求めてあてもなく彷徨う。あなたの試行錯誤が報われるかもしれないのは、その樹海から脱出する時だ。あなたは何かを見つけて、命からがら古典統計学に逃げ帰ってくる。見つけたものは探し物かもしれないし、紛い物かもしれない。いずれにせよあなたは古典統計学の「木」に寄り添ってはいられなくなり、再び「森」に向かって走りだす。
これを悪夢と呼ばずして何と呼べば良いのだろう。

終わりのないこの流動に飽きもせず、いや飽きていないふりをして、遭難者であることを認めず冒険家であろうとするあなたは------いや我々は、『構造と力』の言葉でいえば、知識発見という遊戯を強いられた「不幸な遊戯者」、つまりはこの延々と続く循環構造の中に閉じ込められている「道化」に他ならない。

データから学ぶという営みをこれほどに魅力的なものにしているのは、我々の精神に根深く存在している罪深い探究心である。肥大化した欲望はもはや枠の中に行儀よく収められるものではない。「木」や「森」の枠組みから自由であろうとして、我々はその「構造」からの脱却を図った。そこにあったのは自由だろうか？答えは言うまでもなく否である。古い枠組みから逃れた我々はどの枠組みに留まることもできず、この逃走から逃げる道を見いだせないまま、悪夢のような逃走を続けている。
再び『構造と力』から引用するなら、「遊戯の充満が遊戯の喪失でもあるという不幸な逆説。この逆説から逃れる道を探ることこそが、いま問われるべき重要な問題である」ということになる。
%穏やかで幸福なプレイグラウンドは、一体何処にあるのだろう？

\section{「真に喜ばしい遊戯」へ}
我々をこの逃走から解き放ち、「真に喜ばしい遊戯の場」へ導きうるものとは何だろうか。

ここに``The Future of Data Analysis''という論文がある。最近のベストセラーかと思わせるタイトルだが、数学者・統計学者であるJohn W. Tukey\footnote{我々が慣れ親しんでいるビットという言葉も、高速フーリエ変換というアルゴリズムも、統計学でお馴染みの箱ひげ図も、すべてTukeyが生み出したものである。彼の偉大さについては説明不要であろう。}が1962年に書いた論考である。この論文は仮説検定という方法論を偏重した当時の統計学に対する警鐘と言っていい。

『パターン認識と機械学習』の目次を眺めるだけで色々な機械学習のアルゴリズムが開発されてきたことが見て取れるように、基礎的な統計学の教科書をパラパラとめくるだけで面食らうほど様々な手法に出くわすはずだ。しかしTukeyはそうした手法の理論ばかりではなく、現実のデータを扱う探索的データ解析（exploratory data analysis）もまた統計学において重視されなくてはならないということを主張した。

そのために彼が指し示したのは、統計学を「手法の集合」ではなくて「問題の集合」と捉えるというパラダイム転換であった。``Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.''という格言は、そんな脱構築の方向性を示したメッセージであった。意訳をするなら「的はずれな問いに的確に答えようとするぐらいなら、ズレた答えで正しい問いに挑む方がずっといい」といったところだろうか。

もし我々が的はずれな問いに的確に答え続けようとしているなら、本当に虚しく救いようのない道化である。データベース・機械学習・統計学の間で走り回っても、この苦役は永遠に終わらない。悪夢から逃れ、穏やかで幸福なプレイグラウンドに辿り着くために必要なもの。それは的確な答え方ではなく、的確な問いである。的確な問いは、我々がまだ全貌を把握していないだけで現実のあちこちに埋まっている。だから最も避けなければならないのは、その宝探しの場が閑散とすることなのだ。

方法論の世界に留まる者たちの覚悟に敬意を表したら、データベース・機械学習・統計学という構造から抜け出し、いそいそと次のプレイグラウンドに移ろう。現実世界の中で問題を拾い集め、データの集合でも手法の集合でもない、問題の集合を手に入れる。今求められているのはデータの中の構造を洗い出すエネルギーを、問題の中の構造を浮き彫りにする力に変えることだ。「データに潜むパターン解析」というゲシュタルトを壊すこと。それが問題のパターンを認識する鍵となる。この認識が既存の方法たちに目的を与え、それらが幸福な形で使われるような新しい秩序を作り出す。データの外側、かつ認識の内側の場所で遊戯が始まるとき、我々はもう苦役を課せられた「不幸な道化」ではなくなっている。

\section{おわりに}
「おわりに」と書いてはみたものの、一体何が終わったのかと詰問されているような気分になってくる。
本稿の主張を標語的に纏めてしまえば、《data analysisからproblem analysisへ》ということになるだろう。1962年にTukeyが見つめた未来は、今でも``future''のままである。

本稿では、仮説検定からビッグデータに至るまで網羅的にキーワードをかき集め、それらを医学・医療の中で再構成した。第4節・第5節は複雑な話に見えたかもしれないが、問題の構造としては高次元ベクトルに対する2クラス分類問題というパターンに絞ってある。

現実世界の問題を集めて問題のパターンを知るという、データや手法を超えたパターン認識。それが未来から現在のものになり、本稿が未来への布石から過去の石ころになるようなとき、我々は悪夢からの逃走すらも悪夢だと笑って穏やかに遊戯する存在になっているだろう。だから本当は「おわりに」ではなく、「おわりのはじまりへ」と書くべきだったのかもしれない。溢れ出る時間は外へ向かう思いの外さえ閉じ込めて、止め処なくどこかへと流れて行く。

%統計学・機械学習の応用研究においてこの考え方が目新しくなくなり、もはや論ずる価値を持たなくなるような未来を思い描きながら本稿を執筆した。


%、手法に固執しすぎては現実の問題をいつまでも解決できないかもしれない。それぞれの道具が現実世界でその真価を発揮して人間を幸せにするために、手法ではなくて問題を重視する発想にそろそろ切り替えなければならないのだと思う。
%\begin{quote}
%"Those who ignore Statistics are condemned to reinvent it." --- Bradley Efron
%\end{quote}
