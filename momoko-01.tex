% -*- coding: utf-8 -*-

\chapter{データと認識の間で知る現実のパターン}
% * (アスタリスク)付きの \chapter* コマンドは原則不可とする

\begin{flushright}
 早水　桃子 % ペンネーム
\end{flushright}

\begin{spacing}{0.6}
\noindent
{\footnotesize{本稿は、データサイエンスや統計学の専門家ではない筆者が
The Database Times vol.2のために気儘に執筆し、無責任に寄稿したものである。
本稿で述べる内容は、出典が明記されていない限りは全て
筆者の私見であり、それは他の個人や団体の主張を採用したものではなく、また
各分野の専門家の見識と一致するとは当然限らないということを予め宣言しておく。}}
\end{spacing}
 
\section{来し方行く末}
\subsection{ブームとしての学問}
『解体新書』は、江戸時代の医師たちの試行錯誤によって『ターヘル・アナトミア』というオランダ語の解剖学書から翻訳され、1774年に出版された歴史的書物である。まだオランダ語の辞書すらなかった時代であることを考えると、本文・図表合わせて五冊から成る医学書をわずか数年で翻訳・出版したというだけで大変な偉業である。しかし、この本の功績は、当時鎖国政策をとっていた日本に初めて西洋の医学知識を伝導し、それまでの日本の医学の常識を覆したところにある。
『解体新書』以前の日本においては解剖はほとんど行われていなかったため、人体は「五臓六腑」という東洋医学の概念によって捉えられていた。今でこそ我々は人体を色々な臓器の解剖学的な構造と生理学的な機能が作るシステムとして捉えているが、この枠組みを日本に初めてもたらしたのは、『解体新書』に他ならない。

『解体新書』は医学にとどまらず、ヨーロッパで発展した様々な学問から新しい知識や技術を吸収するという「蘭学（洋学）」ブームの契機になり、近代日本科学史に金字塔を打ち立てることとなった。
しかし、『解体新書』の翻訳者の一人である杉田玄白は、彼が85歳で亡くなる2年前に著した『蘭学事始』（1815年）という回想録の冒頭において、このブームを少々冷めた視線で眺めている。
%蘭学草創期の出来事を正しく伝えるために、85歳で亡くなる2年前の1815年に『蘭学事始』という回想録を著し、その冒頭でこのように述べている。

\begin{quote}
今時（きんじ）、世間に蘭学といふ事専ら行（おこな）はれ、志を立つる人は篤く学び、無識なる者は漫（みだ）りにこれを誇張す。
\end{quote}

時代を問わず、ブームというのはそういうものなのかもしれない。昨今のデータサイエンスをとりまく流行も、恐らく例外ではない。『解体新書』が現代医学への道を切り拓き、やがて蘭学が社会現象の域にまで達したように、
初の網羅的な解剖学アトラスのような『パターン認識と機械学習』の大ヒットもまた、機械学習・データマイニングといった分野の大流行を物語る。
ジョン・ネイスビッツは『メガトレンド』（1982年）という著作の中で「我々は情報の海に溺れ、知識に飢えている」と見事に評したが、
%We are drowning in information but starved for knowledge. 
「情報爆発」や「ビッグデータ」へのソリューションが声高に語られ、それに煽られるかのようにバズワードに群がる烏合の衆は今日も騒がしいツイートをやめない。

無論、目新しい分野に期待を寄せるのは当然あるべき知的好奇心に違いないのだが、
パターン認識と機械学習の学習\footnote{特定の書籍に言及しているわけではない、と白々しくトボケておく。}をしなければ何となく乗り遅れたような気分にさせられたり、とりあえず機械学習で何かをすればそれが何であっても人目を引くことが出来たり、データの海に溺れる者にデータマイニングを謳った藁を差し伸べる有象無象の輩が現れるのは、最先端の花形的研究分野の活気と言うよりはむしろ、ゴールドラッシュ\footnote{金が採掘された場所に一攫千金を目論む採掘者が殺到すること。1848年にカリフォルニアの川で砂金が発見されたことによるカリフォルニア・ゴールドラッシュが有名であるが、金の鉱脈が見つかるたびに世界各地で幾度となく繰り返し見られる現象である。}を彷彿とさせる熱病と呼ぶべきものだろう。

\subsection{メルトダウンするブームと心中しないために}
人並みの繊細さを持ち合わせている人間であれば、その熱気にのぼせて付和雷同してはいられないはずだ。現在のデータサイエンスをとりまく熱気は単なる空騒ぎに浪費されるかもしれないし、これまでに提案されてきた数々の理論や技法は、現実の問題解決に対する有用性や意義が不明瞭なまま忘れ去られ、それを実装したコードも無用の長物となるかもしれない。データ解析に求められるソフトウェア・ハードウェアに投資しても望む結果が何も得られなかった場合、ユーザーの少々無理のある過剰な期待はやがて失望に変わり、それが集団的な絶望へ変わる時にブームは終焉を迎え、最悪の場合は学問分野そのものが廃れていくかもしれない。

また、新しい考えや技術は受け入れられるどころか脅威と見なされて排除されたり、新たな問題を生み出すことも少なくない。実際、蘭学ブームは蘭方医学と漢方医学の深刻な対立を招き、蘭学は蘭書翻訳取締令などによって思想弾圧の憂き目を見ることになったし、産業革命は機械による自動化・作業効率化をもたらした一方で、失業を恐れた労働者集団は機械を破壊するという行動に出た\footnote{この機械打ち壊し運動（ラッダイト運動）になぞらえて、雇用を不安定にするIT化・自動化に反対する思想はネオ・ラッダイトと呼ばれている。}し、産業廃棄物の処理という新たな問題を生じさせた。

だからといって、ブームに対する危機感に振り回されて時代の潮流に乗ることを拒絶するのも賢明ではないだろう。したがって、本稿において貫きたいスタンスはまさしく「自ら『濁れる世』の只中をうろつき、危険に身をさらしつつ、しかも、批判的な姿勢を崩さぬことである。対象と深くかかわり全面的に没入すると同時に、対照を容赦なく突き放し切って捨てること。同化と異化のこの鋭い緊張こそ、真に知と呼ぶに値するすぐれてクリティカルな体験の境位であることは、いまさら言うまでもない。簡単に言ってしまえば、シラケつつノリ、ノリつつシラケること、これである」\footnote{浅田彰の『構造と力―記号論を超えて』（1983年）のあまりにも有名な一節。}。

持て囃されている道具を信仰したり、道具を使うこと自体が目的になってしまったり、道具を数式やプログラムの上だけで理解したり、あるいは実装して再生産しただけで満足してしまったり、どの道具が優れているのかという格付けに一喜一憂したりするような態度は全てこのスタンスに反する。肝に銘じておくべきは、どんなに高尚な理論や技法も、現実世界の問題に挑む人間にとっては目的を叶えるために使える道具の一つにすぎず、目的のために道具を使う人間があくまでも主役ということである。道具の長所や短所や使い道、他の道具とどのように関わると有用か、どのような道具がどのような局面で必要とされているのかは、ショーケースの中の道具を鑑賞しているだけではちっとも分からず、実際に使ってみて漸く見えてくるものである。

これはあまりにも当然なことであるのだが、データベースからの知識発見\footnote{Knowledge discovery in databasesの頭文字を取ってKDDとも呼ばれる。}という舞台を観客として眺めていると、大規模データベース・機械学習・統計学という華やかな役者たちが全てであるかのように錯覚しがちである。しかしそうした「道具」たちは何ができて何ができないのか、どのように互いに関わり、どこでどのように応用すれば現実世界で真価を発揮するものになるのかというストーリーは、
人間や社会の問題意識やゴール設定なしに始まるはずがないし、もちろん終わるはずもない。

\section{データマイニング再考}
まずはJerome H. Friedmanの"Data mining and Statistics: What's the connection?"を下敷きにして、データマイニングとは何であるのかを
批判的に捉え直してみよう。

\subsection{データマイニングとデータベース}
データから知識を得ようとするというのは、全然目新しくない。では、何が今更データマイニング「特需」をもたらしたのだろうか。この問いかけに答える鍵の一つは、データベースマネジメントシステム（DBMS）の変遷である。
%Imielinski（1995年）

かつてのDBMSにおいては、銀行のATMで利用されているようなオンライントランザクション処理（OLTP）と呼ばれる情報処理が主流であった。つまり、DBMSに求められていたのはデータを貯蔵し、多くのユーザーからの小規模なデータアクセスを伴う定型クエリーを高速に処理するという能力だった。
%http://otndnld.oracle.co.jp/deploy/performance/pdf/Oracle8i-tuning2.pdf
しかしデータベースに蓄積されたデータを様々な切り口で分析したい人々が
データベースに意思決定支援を求めるにつれて、
オンライン分析処理（OLAP）が注目を集めるようになった。OLAPは、ユーザーが対話式の試行錯誤を繰り返すことによって、データウェアハウスに格納された膨大なデータを集計したり、多次元的に分析したりすることを可能にした。

一見するとこれはデータマイニングツールの原型のように見えるかもしれない。しかし、OLAPによるデータ解析によってデータウェアハウスと「対話」できるのは、的確な仮説を考え続けるような不断の洞察力とクエリーを書き続ける技術力（および時間と体力）を持ち合わせた選ばれし人間だけである。データに潜むパターンやモデルの叩き台を自動的に見つけてくれるものではないし、ビジネスの場における迅速な意思決定に向いているものでもない。だからこれは
データマイニングツールの原型と言うよりはむしろ、人々がデータマイニングツールを求める素地を作ったと考えたほうが良いだろう。
%大規模なデータからパターンを見出す試行錯誤のプロセスが人間の手を煩わせずに実現するならばそれが効率的であるし、data analysis for everyoneみたいなものへの需要は高まっていった。

\subsection{プロパガンダとしてのデータマイニング}
データマイニングもまた他のバズワードの例にもれず定義が曖昧な言葉であるが、
複雑で膨大なデータ集合の中に埋もれていて人間の力では見いだせないような有益な知識をコンピューターの力でデータベースの中から探索的に発見し、ビジネスにおける意思決定に役立てるような色々な手法（ないし、それらを使ったプロセス）という意味で専ら使われており、
多くの場合においてはIBM Intelligent miner、SAS Enterprise Miner、SPSS Clementineなどの汎用データマイニングツールを使うことによって実践されるものである。

データマイニングの目論見は壮大なものであるが、
%データから新たな知見を得ようとする試みは全然斬新なものではない。
ユーザーが小難しいデータベースの構造やデータ解析の中身を知らなくてもデータベースに格納されたデータにアクセスし、それを分析したり視覚化することを可能にするというデータマイニングツールというものは、
誰のためのものなのだろうか。
「データ自身に語らせる」といえば聞こえは良いが、ビジネス戦略のヒントとなるような知識を囁くかどうかは別の話であるし、あまりそういううまい話は聞かない。Friedmanが指摘するように、汎用データマイニングツールは非常に高額な商用ソフトウェアであり、導入するとなるれば当然相応の高価なハードウェアやストレージも要求する。その意味で、データマイニングはもはや金脈を掘り当てようとする宝探しのような試みと言うよりはむしろ、データマイニングと聞いてソフトウェアやハードウェアに投資してくれるような、データを持て余している企業の意思決定者や潜在的データマイナーを発掘するための商業的プロパガンダと化しているところがある。

\subsection{データマイニングと統計解析の違い}
Friedmanは汎用データマイニングツールが提供する主要な手法の数々を
%\footnote{Decision tree induction・rule induction・nearest neighbors・clustering・association rules・feature extraction・visualisation、neural networks・graphical models（Bayesian belief networks）・遺伝的アルゴリズム・SOM（自己組織化マップ）・neuro-fuzzy systemsと}
を列挙し、多くのデータマイニングツールにおいて
仮説検定や判別分析など、統計学者にとって見慣れた手法は
殆ど無視されていると評している。一言で「データを解析する」と言っても、データマイニングと統計解析は目的も手段も大きく異なるものであり、両者を混同してはならない。

統計学的なデータ解析は、「データ自身に語らせる」という大らかなものではない。
「実験が終わった後に統計学者にコンサルトするのは、死後に解剖を依頼するようなもので、統計学者はその実験の死因を教えることぐらいしかできない」というロナルド・フィッシャーの言葉の通り、
そもそも統計学者にとってはデータは綿密な実験デザインに基づいて注意深く収集されたり吟味されるべきものであり、
鉱山のように元々そこにあるものではないのだ。
それに、データを説明するための仮説やモデルがなければ、仮説検定\footnote{帰無仮説を棄却するか否かという仮説検定は無味乾燥なものだと思うかもしれないが、少なくとも学術研究の分野においては最もよく使われる意思決定アルゴリズムである。}もモデル選択\footnote{モデルのパラメータを最尤推定する「モデル推定」とともに、どのモデルを選ぶべきかという「モデル選択」は機械学習（特に教師あり学習）においては最も重要なテーマの一つである。モデル選択基準はAICやBICなど色々なものがあり、クロスバリデーションもよく使われる。}も始めようがない。だから、良質なデータと洗練されたモデルこそ、統計学的なデータ解析の要であるのだ。その営みにおいてアルゴリズムというのはそのモデルを採用するか否かの基準を計算するための方法にすぎない。だから、モデルが正しければ良い結果を出し、正しくなければ悪い結果を出すのがここでのアルゴリズムのあるべき姿であるし、解析結果の善し悪しはアルゴリズムではなく、モデルが決めていると考えるのが筋である。

これに対しデータマイニングは、良く言えば従来の統計学が扱えないような整理されていない大規模データを何とか使える形にし、色々な切り口で分析して意思決定のヒントになる知識を見出そうとする果敢な取り組みであり、悪く言えば産業廃棄物のような膨大なデータの山をゴミだと認めるわけにいかないからといって、往生際悪くデータのフォーマットを整え、クレンジング\footnote{データの誤りや重複を洗い出し、使えるデータにするという泥臭い（しかし最も重要な）プロセス。}を施して場当たり的に試行錯誤することにより、有象無象の仮説やモデルの叩き台をあれこれ捻り出すという作業である。

いずれにせよデータマイニングは統計学とは全く異なる思想を持つものであり、仮説やモデルなしに鉱山の中を探索するデータマイニングにおいてよく登場する手法が、統計学の手法と異なるのは驚くべきことではないし、データマイニングが語られる文脈においてはテクニックやアルゴリズムありきの話になりがちであるのも、当然の成り行きなのかもしれない。

\subsection{データマイニングの意義}
しかしながら
%データマイニングに失望したり、ベンダーやコンサルティングファームに対して猜疑的になったり、
データマイニングの努力は水泡に帰するのかと悲観する必要はないし、それは本稿における我々のスタンスではない。むしろそこでの努力を無にしないために、試行錯誤の中に埋もれた教訓を探し、データマイニングが人間にとって有用であるためにはどうあるべきかを考えなければならない。

データマイニングは大規模なデータセットになるほど前処理が重要であることを実証してきたし、そのためのプラクティカルな方法を確立してきた。データの可視化は迅速な意思決定の取っ掛かりとして確かに強力であるし、現にそういったツールが多くの人々に求められているということは、人間が扱わなければならないデータが確かに複雑になっているということを物語っている。
また、様々なテクニックを集めたツールとデータ解析の専門家だけでは解析結果は荒唐無稽なものになりがちであったが、それはデータマイニングは役立たないと切り捨てる理由にはならず、むしろデータ解析には現場の知識を持った専門家の知識が必須であるということを改めて思い知らせてくれたと考えるべきであるし、その方がよほど建設的である。

また、大規模データベースを持っているのは企業だけではないし、
後述するように公開されている大規模なデータベースも数多く存在する。
そのようなデータベースから知識を発見する試みにおいて、データマイニングは単なる商業的なプロパガンダに堕したものではなく、真理の探求を目指す学術研究において仮説やモデルのプロトタイプを見つける手段となったり、
蓄積されたデータから合理的に最適な政策を決定するという国家や国際社会の意思決定を支援するためのツールとなり得るものである。

データマイニングは、アソシエーションルール・クラスタリング・決定木など、実に様々なパターン認識・機械学習の理論的な枠組みを現実の大規模データに応用してきた。そうした理論と現実の橋渡しをする取り組みは軽視されてはならないし、その価値は商業的な実用性や理論的な厳密さや一般性だけで評価されるべきではない。そうでなければデータマイニングバブル崩壊とともに、データベースから知識を発見することの重要性も失われかねないからだ。

\section{データ解析から離れた機械学習}
データ解析のことばかり考えていると、統計学もパターン認識も機械学習もデータベースを掘り起こす道具にしか見えなくなるかもしれないから、それ以外の側面についても少しだけ触れてから先へ進むことにしよう。
\subsection{医療機器}
統計学や機械学習で登場するアルゴリズムの幾つかは、医療の現場で人々の健康な生活や生命を支えている計算手法と言っても過言ではない。医用画像を得るためには画像再構成という重要なプロセスがあり、そこではバックプロパゲーションやEMアルゴリズムといった機械学習・統計学でよく知られる手法が一般的に用いられている。
また、病気の検査のためだけでなく、
がんの放射線治療においてはモンテカルロ法が古くから線量分布推定に用いられ、
今もなお放射線治療計画に必須の計算手法であり続けている。

医療現場で使われる装置やプログラムに関わるごく一部の例であるが、
統計学や機械学習の、データマイニングで期待される役割とは少し違う一面を窺い知ることが出来るだろう。

\subsection{神経科学}
もともと機械学習は人工知能研究の一分野であることから分かるように、理論神経科学と深く関係しており、その理論的な枠組みそのものが、人間の脳のメカニズムや、社会における人間の行動などを理解する一助となり得るものである。パーセプトロンは古くから小脳のモデルとして知られ、人間の運動学習の神経基盤を理論的にも実験的にも明らかにし、ロボットの運動制御という工学的な応用にもつながることとなった代表的な例である。
より最近の例としては、やや趣味的になるが、ドーパミンニューロンが報酬予測誤差によって強化学習を行っているのではないかという仮説が提唱され、強化学習の中でも特にTDモデルが大脳基底核の計算モデルとして注目を集め、報酬に基づく意思決定の神経基盤が理論的にも実験的にも精力的に研究されている\footnote{そしてその実験のためには医用画像処理・画像の統計解析が必要不可欠なのである。}。これらは薬物やギャンブルなどの依存症と密接に関係し、今後は人間の衝動制御にもつながる研究がますます進むと思われる。

こうした人間の脳につながる研究はロマンティックで興味深く、また実際に有益でもあるのだが、本稿ではもう少し現実的で差し迫った問題を題材にして、データベースや機械学習・統計学について考察を深める。

\section{人生の明暗を分ける機械学習}
%日本人の平均寿命は世界の先進国の中でもトップクラスであり、それは高度に発達した現代の医学と、充実した医療サービスを誰でも受けることが出来るようにするための国民皆保険制度の賜物である。しかし
少々勢いに任せた見出しをつけてしまったが、周知の通り、日本を筆頭に世界各国（特に先進国）では高齢化が社会問題となっており、増え続ける認知症患者は医療費の増加により国の財政を圧迫し、若い世代に大きな負担を強いるだけでなく個人にとっても人生を脅かす問題になっている。

日本では現在既に65歳以上の高齢者のうち約10\%が認知症であると見積もられており、しかも今後さらに増加していくと推定されている。
我々のうち少なくとも10人に1人は認知症患者になっていくのかと物思いに沈んでいると、生涯現役が当たり前になりつつある中で自分の本来のパフォーマンスを発揮できなくなっていく個人にとっての悲劇や、その個人を取りまく人々の人生が受ける打撃を少し生々しく想像してしまう。

\subsection{認知症：背景知識の要点}
認知症というのは単一の疾患ではなく、認知機能低下を呈する多くの疾患の総称である。その中でも一番多くて重要なのが、誰でも一度は聞いたことがあるであろうアルツハイマー病である。

\subsubsection{アルツハイマー病はハードウェアの異常である}
コンピュータの調子が悪くなったら、まずはハードウェアレベルの異常なのかソフトウェアレベルの異常なのかを考えるのが自然ではないだろうか。それと同様に、長年使ってきた脳がうまく動かなくなる場合も、原因はソフトウェアのレベルとハードウェアのレベルとに分けられる。

認知「機能」の障害と聞くとソフトウェアレベルの問題のように思うかもしれないが、アルツハイマー病は正常な脳の「構造」が壊れることにより機能に支障が出る病気で、ハードウェアの著しい劣化による脳の故障と考えるべきものである。

具体的には、正常な人でも加齢で蓄積するゴミのようなもの\footnote{ゴミの正体について少しだけ詳しく述べれば、アルツハイマー病は脳内に$\beta$アミロイドとよばれるタンパク質が蓄積し、異常リン酸化タウタンパクが蓄積してしまうことで神経細胞の変性・機能不全が起こり、神経細胞死を招く疾患である。それが肉眼的・画像的な脳萎縮という所見となる。}が異常に脳に溜まってしまうために神経細胞がどんどん死んでしまい、脳のボリュームが目に見えて減ってしまうのである。

\subsubsection{アルツハイマー病の根本治療薬とワクチン開発への動き}
ハードウェアの障害であると繰り返したのは、病気が進行して重症化してからでは有効な治療法がないことと、MRI画像で脳の形やボリュームを見ることが診断に有効であることの二点を強調するためである。
壊れかけの初期のうちに食い止めることが出来るなら幸せなことであるし、もっと言えば最初から認知症症状が出ないように出来たら理想的である。

現在多くの巨大な製薬会社や医療機器メーカーがベンチャー企業とも協力し、アルツハイマー病の根本治療薬やワクチン開発を目指して研究開発に取り組んでおり、実際に日本でも複数の治験が進行中である。
こうした根本治療薬・ワクチン開発への動きによって、脳の異常の客観的な評価方法や早期診断手法の重要性は急速に高まった。完全に壊れてしまってからではなく、なるべく異常が小さいうちに病気の芽を見つけ出し、なるべく早い段階で摘み取りたいのである。
\subsection{ADNIデータベース}
2004年にアメリカはUS-ADNI（米国アルツハイマー病神経画像イニシアチブ）をスタートさせた。
これは800人以上の被験者を6ヶ月毎にスキャンして得た
経時的な画像データ
\footnote{
MRIの構造画像だけでなく、FDG-PETという機能画像や発症前診断のためのPiB-PETによるアミロイドイメージングも含む}のみならず、認知機能検査の点数・色々な生物学的マーカー・臨床的な因子などの様々な情報をデータベース化し、それをweb上で公開するという試みである。US-ADNIは開始当初の予算だけでも6000万ドルという規模であり、認知症研究を推進するための国家的な臨床研究プロジェクトである。

医用画像データの質は施設ごとの撮像方法や装置そのものに大きく影響されるため、多施設から得られたデータをそのまま解析することは出来ない。しかしADNIではそのような違いを補正するための前処理を施し、データを信頼出来る品質にしている。そうして作られたデータは匿名化され、翌日にはwebサイト上で世界中に公開される。

因みにこのADNIというイニシアチブはヨーロッパ・オーストラリア・日本・台湾・韓国でもスタートし、日本でもJ-ADNIという名前で2007年からプロジェクトがスタートしている。
US-ADNIの方はさらにアルツハイマー病の早期診断・発症予測に焦点を当てたADNI 2に乗り出しており、また被験者の全ゲノムシークエンスをも公開するビッグデータプロジェクトに変貌しつつある\footnote{もし800人の被験者の全ゲノム配列が得られたらデータベースに追加されるデータのサイズは少なくとも165テラバイトになると見込まれている。}。

\subsection{ADNIデータベースと機械学習}
US-ADNIのデータは医師や研究者ではなくても、誰でも自由に無料でダウンロードすることができる。
つまり、医療と何ら関係のないデータマイニングや機械学習の研究者であっても、最先端の検査を駆使して得られた大規模な患者データを得ることが出来るのである。データはどのように利用しても良いし、「データベースからの知識発見」に成功したならば当然それを自由に論文として発表することが出来る\footnote{ADNIのデータを使った研究の一覧はhttp://adni.loni.ucla.edu/publications/で見ることができる。}
。ADNIデータベースはそうした学際的な研究の基盤になっており、認知症をターゲットにした脳画像に対して機械学習の手法がさかんに応用されている背景にはそうした事情もあると思う。

機械学習を応用するという視点で見れば、確実に正しい教師データが入手できるというのもADNIデータセットの長所である。実は日常的に行われている臨床診断は、厳密には100\%正しいラベル付けとは限らない。それはアルツハイマー病を確定診断をするためには顕微鏡で実際に「ゴミ」が溜まっているのを見なければならないため、手術室で生きている脳から組織のサンプルを採取することが出来た場合を除き、真の答えは死後の解剖なしには確かめようがないからである。ADNIは人間の医師が付けたラベルだけでなく、判明している限りの「真のラベル」も公開している。だから訓練データの中にあるかもしれないノイズに悩まされることなく、安心して教師あり学習アルゴリズムを使うことが出来る。
\subsection{機械学習による自動診断から発症予測へ}

\subsubsection{SVMによるMRI画像の自動診断}
MRI画像「だけ」でアルツハイマー病を正確に診断するのは人間の医師であっても訓練が必要である。
ところがサポートベクターマシン（SVM）という分類器は、認知症の症状の度合いや年齢といった情報を一切使わず、MRI画像だけで重症なアルツハイマー病の患者と健康な高齢者を95\%ほどの感度・特異度で分類することに成功してしまったのである\footnote{Automatic classification of MR scans in Alzheimer's disease. Brain (2008), 131, 681-689}。
ここで用いられたSVMは線形SVMというもので、簡単に言うと非常に高次元なMRI画像を数万次元の長いベクトルとみなし、その内積を計算して得られる類似度から病気か否かを判断するというシンプルな教師あり学習アルゴリズムである。
しかも、アルツハイマー病患者の脳と健康な高齢者の脳を識別するだけでなく、アルツハイマー病患者の脳とFTLDという別の認知症患者の脳を識別するということも可能であった。また、ある施設のデータセットで訓練し、全く別の施設で撮像されたデータセットでテストしても正しく分類するという高い汎化能力を持っていることも大きな長所であった。

\subsubsection{手法の改良と挫折}
MRI画像からアルツハイマー病を自動診断するための手法をより良いものにするために、その後も色々な研究がなされ、色々な手法が提案されてきた。しかし、結局あまり大きな改良はなかった。

色々な手法の性能比較は難しい問題だが、Cuingnet\footnote{Cuingnet et al. Automatic classification of patients with Alzheimer's disease from structural MRI: a comparison of ten methods using the ADNI database. Neuroimage. 2011 May 15;56(2):766-81. Epub 2010 Jun 11.}はADNIデータベースのデータを用いてこれまでに提案されてきた手法を追試し、比較するというメタアナリシスを行った。それによると、アルツハイマー病患者と健康な高齢者のMRI画像診断は大体どのやり方を使ってもそれなりに出来るものだったが、DARTELというアルゴリズムを含む前処理を施した後、脳の全領域の情報を使って線形SVMで識別するのがやはり最善のようであった。

工夫を凝らした複雑な手法をシンプルな手法がせせら笑っているかのような、ちょっと絶望的な結果である。しかし、MRI画像は非常に高次元で多くの情報を含んでいるために非線形な特徴抽出をする必要がなく、また前処理のアルゴリズムが洗練されていったことでノイズを恐れて脳の領域を一部分に限定するという特徴選択をする必要がなくなり\footnote{脳MRI画像は非常に高次元なデータなので、良い特徴選択ができたら効率的であるが、脳の領域を予め人間が選択すると汎化能力が低くなるという問題がある。訓練データから重要な領域を学習させることも試みられてきたが、時には数週間の計算を要するほどに計算量がやたらと増えるばかりで、結局分類の正確さを向上させようとすると脳の領域全ての情報を使うに越したことはないという結論になってしまった。また、カーネル関数の選択についても色々と試行錯誤されたのであるが、結局は線形カーネルに落ち着くことになった。特徴ベクトルが非常に高次元であるため、線形カーネルで十分なのであろう。}、重症アルツハイマー病患者の脳と健康な高齢者の脳を分類するという程度の問題であれば、全脳のデータを線形SVMに突っ込むだけで十分になったのだと考えることができるだろう。

\subsubsection{目標の再確認}
ここで目標を見失って迷子にならないように、我々のモチベーションを再び明確にしておこう。我々が目指しているのはアルツハイマー病というハードウェアの障害を壊れかけの初期のうちに食い止めることであったはずだ。だからこそ完全に壊れてしまってからではなく、壊れ始めている早期に正常と異常の境界を引くことが出来るような方法を求めているのである。

重症なアルツハイマー病と健康な高齢者の脳の違いは人間には一目瞭然である。だからそれを識別するような2クラス分類問題なんて、線形SVMで事足りるという話である。厄介なのはあまり重症ではないアルツハイマー病と健康な高齢者の2クラス分類問題であり、ましてやまだ認知症を発症していないような「ちょっとボケかけ」の人々\footnote{だいぶ露骨な物言いになってしまったが、こうした「ちょっとボケかけ」の人々は軽度認知機能障害（MCI）と診断されており、白黒がまだつけられないケースに貼られるグレーなラベルである。このグレーゾーンの人々の中にはやがてアルツハイマー病を発症してクロになる人もいるし、ずっとグレーのままという人もいる（前者はMCI converter、後者はMCI non-converterといわれる）。アルツハイマー病の早期発見とか発症前診断というのは、このグレーな人々の中から将来クロになる人々を見出すというタスクである。}を将来認知症を発症する人としない人に分けるという2クラス分類問題となると人間のエキスパートもしばしば苦慮してしまうほどの難問なのである。

先ほどのCuingnetらの解析によれば、あまり重症ではないアルツハイマー病と健康な高齢者の2クラス分類問題ではどの手法でも途端に感度が落ちてしまった。そして「ちょっとボケかけ」の人々を将来認知症を発症する人としない人に分ける2クラス分類問題となると、どの手法もお手上げだったのである。

\subsubsection{これ以上何ができる？}
これまで見た手法には何が足りなかったのだろうか。
まずすぐに気づくのは、これまでの手法はMRI画像だけで診断をしているということである。
病院で医師が診断をする時に色々な情報源を用いているように、
複数の検査から得られる情報を統合して診断をする枠組みが欲しいと考えるのは自然なことであるはずだ。

また、一時点のMRIデータしか使っていなかったというところにも改善の余地があるだろう。シンプルに捉えるならば脳が萎縮するスピード、欲を言えば非線形な経時的変化も重要な特徴のはずである。

それから、手法の「改良」を目指して感度や特異度という数字を上げることだけに拘泥してはいけないというのも指摘しておくべきだろう。多くの説明変数を使えばより正確なpredictionができるかもしれないが、データが複雑になればなるほどシンプルな説明変数が望ましくなるということを忘れてはいけない。
つまりは複雑な脳の画像データを全部使うのではなく、出来る限り本質的な領域を選び抜きたいのである。そうでなければ、データを人間が解釈するのは難しいままになるからである。

\subsubsection{スパース正則化・マルチカーネル学習}
スパース正則化は観測されたデータをいかに少ない変数で説明するかという問題の近似解法である。また、スパース正則化の特別な場合としてマルチカーネル学習があり、これは多様な情報源からの情報をうまく統合するための枠組みとして注目されている\footnote{詳しい説明は冨岡 亮太・鈴木 大慈・杉山 将の「スパース正則化およびマルチカーネル学習のための最適化アルゴリズムと画像認識への応用」を参照のこと。}。

これは現在我々が直面している問題にどのように役立つのだろうか。スパース正則化を用いた特徴選択は、「複雑な脳の画像データを全部使うのではなく、出来る限り本質的な領域を選び抜」くために用いることが出来る。脳画像をそのまま数万次元のベクトルとして捉えるのではなく、早期診断に必要な幾つかの本質的な領域だけを重要な説明変数として使うのである。

また、MRIだけでなくPETや認知機能検査など、様々なモダリティのデータを最大限に活用するために、マルチカーネル学習を使うことができそうだ。これまでに見てきた手法はMRI画像からカーネル行列を作ってSVMで識別するというものだったが、モダリティごとのカーネル行列を作り、複数のカーネル行列を凸結合したカーネルを分類・回帰に使うことを可能にするのがマルチカーネル学習である。これはそれぞれのカーネルの重み付けも最適化できるので、どの検査をどのぐらい重視するのかも最適化することができ、しかも凸最適化なので大域的最適解が求まるという強みがある。
\subsubsection{マルチカーネルSVMによるアルツハイマー病の発症予測}
こうした機械学習の先端的な枠組みが現実世界でどのぐらい通用するのかを試すのにADNIデータセットは格好の場所を提供してくれる。Zhangら
\footnote{
Predicting future clinical changes of MCI patients using longitudinal and multimodal biomarkers. Zhang D, Shen D; Alzheimer's Disease Neuroimaging Initiative. PLoS One. 2012;7(3):e33182. Epub 2012 Mar 22}はまさにADNIデータベースに蓄積された多様な情報源からの情報を結合カーネルという形で統合して、マルチカーネルSVMを用いた分類・回帰を発症前のアルツハイマー病の識別や将来の認知機能の点数予測に応用している。認知症の早期発見や発症予測はMRI画像にSVMを適用しただけでは特に感度が低くて太刀打ちできなかったが、こうして経時的な情報を含めたマルチカーネルSVMによってかなり感度が改善され、発症前の診断手法の正確さはかなり向上した。

MRI・PET・脳脊髄液中のバイオマーカーというそれぞれの情報源から得られた情報を使ってそれぞれのカーネル行列を作るとMRIのカーネル行列が一番良さそうに見えるのだが、3種類のカーネルを結合して得られたカーネル行列は実際、MRI単独のものよりも目に見えてはっきりと良いものになっている\footnote{Multimodal classification of Alzheimer's disease and mild cognitive impairment.Neuroimage. 2011 Apr 1;55(3):856-67. Epub 2011 Jan 12. Zhang D, Wang Y, Zhou L, Yuan H, Shen D; Alzheimer's Disease Neuroimaging Initiative.}。つまりそれぞれのモダリティはどれかが優れているとか劣っているとかではなくて、それらが含む情報は互いに相補的なものであるということである。MRIは構造を見る画像検査である一方でPETは機能を見る画像検査であり、脳脊髄液中のバイオマーカーは画像に直接現れるものではないということからも、これは納得のいく結果である。

Zhangらの研究に使われている特徴選択・特徴抽出アルゴリズムの詳細は本稿では省くが、ADNIデータベースによって6ヶ月毎に撮像された複数時点の脳画像データからL2,1ノルム正則化によってどの時点でも重要になるような重要な脳の領域をスパースな解として得ているというのが特徴選択の要点であり、画像をt（時点）の多項式と考えて直交多項式展開した際の係数を特徴ベクトルに含めるというのが特徴抽出の勘所である\footnote{2次以上の多項式で捉えることで、萎縮のスピード（ｔの一次式）だけではなく非線形な変化に関する特徴も抽出している。}。

こうして一時点のMRI画像だけでは困難な問題であっても、訓練された人間の医師のように
構造画像だけではなく機能画像やカルテに散らばった様々な情報を統合し、現在までの経時的な変化を辿ることによって、半年後に認知症を発症するか否かを感度79\%・特異度78\%で識別することができた。このZhangらの解析で使われたのはMRI・FDG-PET・認知機能検査の情報だけであり、血液や脳脊髄液中のバイオマーカーの情報や、現在発症前診断法として最も注目されている[$^{11}$C]PIB-PETという画像の情報は使われていない\footnote{ADNIデータベースにおいても全ての検査を受けている被験者は少ないからである。}。これらの情報を取り入れればマルチカーネルSVMによる発症予測はより正確なものになると思われる。

\subsubsection{国家レベルの意思決定支援へ}
根本治療薬開発への動きが激化している現在、我々が直面している問題は単に病気の人と健康な人を識別するという問題の範疇を超えて、ハードウェアが壊れてしまう前に劣化を食い止めることができるか否かという人生の明暗を分けるための大きなステップである。単に医師個人の経験やその寄せ集めによる発症予測ではなく、大規模なデータセットに機械学習のアルゴリズムを応用する研究によって発症予測の客観性が高まっていけば、治療薬をどこまで保険適用とするかといった国家レベルの意思決定を支援するという意義も出てくるに違いない。

%
%
%%とはいえアルツハイマー病の臨床診断は日常的に滞りなく行われていて、殆どの臨床診断は真の答えと一致している。そもそも大抵の専門家は、症状や経過のパターン\footnote{本人を診察したり家族の話を聞いたり認知機能を調べる簡単なテストをして認知機能をスコア化したりする中で明らかになる。}を知った時点でアルツハイマー病かどうかをあらかた察することができるのだが、病態についての生物学的・医学的な研究が進んだ現在、様々な検査によって非常に多くの重要な情報を得ることが出来るようになったからである。中でも、脳というハードウェアの形態を見るMRI、目に見えない脳機能をラジオアイソトープによって画像化するというSPECTやFDG-PET、または脳に溜まった「ゴミ」自体を視覚化するPiB-PETなどによる画像診断研究の進歩は目を見張るものがある。
%
%
%%診断において厄介なのは、認知機能検査の点数が正常とも異常ともつかないような「ボケ気味」の人々であり、こういう白とも黒ともいえないグレーな人々は軽度認知機能障害（MCI）であるのだが、MCIと診断された人々を経時的に追跡してその運命を調べると、決定的に異なる2種類のグループに分かれることが明らかになった。年月を経てもあまり変化がなく少々ボケ気味とはいえ平穏な人生を送ることができるグループと、MCIと診断されてからみるみるうちにアルツハイマー病の診断基準を満たすレベルになり、数ヶ月～数年で変わり果ててしまうグループである。前者はMCI non-converterと呼ばれ、後者はMCI converterと呼ばれる。グレーの中には黒になりつつあるグレーが隠れていたということだ。

\section{古典統計学 --- 木を見て森を知る ---}
さて、大規模データベースや機械学習の話に少しページを割き過ぎてしまったのだが、実は実際の日常診療において活躍しているのはもっと小規模なデータベースと古典的な統計解析である。

\subsection{Statistical Parametric Mapping (SPM)}
脳の構造や機能に関する研究の発展はSPMなしには無かっただろうと思えるほど、SPMというソフトウェアは脳画像統計解析によって神経科学・心理学・精神医学・神経内科学に貢献してきた。SPMはロンドン大学（UCL）のKarl Fristonらが開発した脳画像の統計解析ソフトウェアであり、脳の画像\footnote{SPMはMRIのみならずfMRI・PET・SPECT・EEG・MEGといった多様な脳画像を扱うことができる。}の統計解析をより信頼できるものにするための前処理アルゴリズムや、古典的な統計学的仮説検定からベイズ推定に至るまで様々な解析手法が実装されている。

SPM自体はオープンソースであり、無償で配布されているが、残念ながらSPMを動かすにはMATLABという高価なソフトウェアが必要である。だから大学や研究所などの附属病院などでもない限り、SPMを用いた研究によって認知症診断にクリティカルな脳の領域がどれだけ解明されたところで、そうした知見を使った脳画像の統計解析は象牙の塔から開放されないというのは想像に難くないだろう。
\subsection{日常診療のための脳画像統計解析}
アルツハイマー病早期診断のための脳画像統計解析を医療現場で簡単に生かせるようにしたものが、日本ではもはや一般的になっているVSRADというアルツハイマー病早期診断支援ソフトウェアである。VSRADはSPMの前処理アルゴリズムや、後述するVBMという脳の形態解析手法を（勿論SPMの開発元に許可をとって）スタンドアロンアプリケーションにしたものである。WindowsがインストールされたパソコンさえあればMATLABをインストールしなくても動くし、MRI画像解析の専門知識がなくても撮像されたMRI画像を読み込ませて簡単な操作をするだけでMRI画像に前処理を施し、認知症診断のための統計解析結果を出力してくれるという有り難い道具である。

VSRADはエーザイという製薬会社が無償で提供しており、ダウンロードすればすぐ使えるためVSRADは日本では1000以上の施設でルーチン化され、脳ドックでも使われているほどに広く普及している。
因みにMRI画像ではなくSPECT・PET画像を解析するための同様のソフトウェアとしてはeZISやiSSPがある。これらも無償で提供されているソフトウェアであり、日常的にお目にかかることができる。

\subsection{古典統計学の誘惑}
\subsubsection{VSRADに見る古典統計学の強み}
VSRADの統計解析の基盤となっているのは同じくロンドン大学（UCL）のJohn Ashburnerが開発したVBMである。
これはある群の脳MRI画像の各ボクセル\footnote{ボクセルはピクセルが3次元になったようなものを意味する。}の灰白質濃度がある群のそれと比べて有意差があるかを検定\footnote{各ボクセルごとにt検定をすると当然多重比較の問題が出てくることになるが、SPMではGaussian random fieldによる補正をしている。何故Bonferroni法ではダメかといえば、各ボクセルは空間的に連続しているから明らかに独立ではないし、しかも大抵前処理の段階でガウシアンフィルタによるスムージングもかけられたりしているからである。}するというものである。解析の例としては若年者と高齢者の脳画像を比べることで、高齢者で有意に萎縮している脳の領域を調べるというものである。

VSRADはこのVBMを基礎にして、調べたい患者のMRI画像と80人の健常者のMRIデータベースとを比較して有意に萎縮している脳領域が一目で分かるように、Z検定統計量（Zスコア）のカラーマップをMRI画像上に表示してくれる。
さらに日常診療で使いやすくするために早期アルツハイマー型認知症診断に重要な海馬傍回という領域内のZスコアをざっくりと平均して、萎縮の程度の目安を表示してくれたりもする。
偏差値だけでは把握できない学力があるように、ざっくりと平均したZスコアだけで早期アルツハイマー病を完璧に診断することができるはずもないので、あくまでもVSRADは診断支援という位置づけのソフトウェアであることを忘れてはならない。それでもこのZスコアだけで早期アルツハイマー病の正診率は87.8\%だったという報告\footnote{Y. Hirata, H. Matsuda et al. : Neuroscience Letters 382（2005）}もあり、『VSRAD適正使用ガイドライン』\footnote{http://www.vsrad.info/general/manual/download/guideline1.pdf}が出されてしまうのも頷けてしまうほどに、良くも悪くもポテンシャルの高さが認められている。

\subsubsection{さようなら、ビッグデータ}
保持しておかなければならないデータや検索しなければならないデータのボリュームが爆発的に増える限り、大規模なデータベースのデータを効率的に検索したり集計するというアルゴリズムがますます重要になることに疑念の余地はない。
しかしながらデータ解析となると話は別である。全ゲノムシークエンスを公開するというUS-ADNIのビッグデータプロジェクトによって人類の運命を大きく左右するような仮説やモデルが発見されるかどうかという話ではなくて、大規模なデータがあったらそれを全部使わなければならないなんてルールはどこにもないという話である。

大きな目標を達成するために必要なものは大規模データベースや技巧的なアルゴリズムとは限らず、むしろサンプル数があまり多くないデータセットや統計学的仮説検定のようなお馴染みの理論かもしれない。
だから少なくともデータ解析の世界においてはビッグデータに怯える必要はない。そんなものは幻影だ。
大きければ大きいほど良いなどというちっぽけな固定観念が、偉大な知識発見の妨げになってしまうかもしれないということの方が余程恐ろしいのだ。

\section{知識発見という悪夢}
グレーの中からクロを見出すという難易度の高い2クラス分類問題に対して、ADNIデータセットに対して機械学習でアプローチするというのは一つの方法であった。しかしVSRADの例を見せられると、800人もの被験者を集めた大規模なデータベースだとか奇を衒った機械学習アルゴリズムなんかよりも、健常者80人の脳MRIデータセットと古典的でシンプルな統計学的仮説検定の方が医療現場で人間と共に活き活きと仕事をしているように見えたのではないだろうか。

本当にそうだろうか。古典統計学に逃げこんだところで、それはほんの一時の雨宿りに過ぎないはずだ。
古典統計学を熱心に「木」を見ることによって深い「森」を知ろうとするアプローチのようなものだと考えたとしよう。その時あなたの視線は「森」に注がれていないはずだが、果たして「森」を見ずに「木」を捉えることは出来るのだろうか？

古典統計学に閉じこもっていては新しい仮説もモデルも見つからず、古典的な統計学的仮説検定の力はやがて色褪せてしまう。そしてこの逆説のおぞましさに恐れ慄いたあなたは再び大規模データベースに向かって息つく暇もなく走りだす。それはプレイグラウンドではない。樹海である。あなたは辿り着いた先でいよいよ古典的な統計学的仮説検定が無力な場所に来てしまったことに気づく。そしてパターン認識とか機械学習に光を見出し、データベースからの知識発見を自らに言い聞かせるようにしてあてもなく彷徨う。あなたの試行錯誤が報われるかもしれないのは、その樹海から脱出する時だ。あなたは何らかの仮説とかモデルを携えて、古典統計学に命からがら逃げ帰る。しかしあなたが拾った仮説やモデルが棄却されるか否かに関わらず、結局あなたはやがて再び古典統計学の「木」に寄り添ってはいられなくなる。

終わりのないこの流動に飽きもせず、いや飽きていないふりをして、遭難者であることを認めず冒険家であろうとするあなたは、いや我々は、『構造と力』で浅田彰が用いている言葉でいえば「不幸な遊戯者」ないし「道化」に他ならない。我々は知識発見という苦役を自らに化し、この延々と続く循環構造の中に閉じ込められている。これを悪夢と呼ばずして何と呼べば良いのだろう。

観察されたデータから学ぶということは素朴な営みを堪らなく魅力的にしているのは、観察されるものの大小に依らず我々の精神に根深く存在している罪深い探究心である。この知的な欲望が求めているものが、データベースとか機械学習とか統計学といった枠の中に行儀よく収まっているようなものではないことについてはもうこれ以上語る必要はないだろう。本稿の最後に語るべきことは、我々はどうすればこの悪夢から逃れられるのだろうということだ。再び『構造と力』から引用するならば、「学習と遊戯の区分が抹消される時一様な時空がひろがる。そこでの遊戯の充満が遊戯の喪失でもあるという不幸な逆説。この逆説から逃れる道を探ることこそが、いま問われるべき重要な問題である」ということになる。
%穏やかで幸福なプレイグラウンドは、一体何処にあるのだろう？

\section{真に喜ばしい遊戯への誘い}
我々を悪夢から解き放ち、「真に喜ばしい遊戯の場」へ導きうるものとは何か。

ここに"The Future of Data Analysis"という論文がある。最近話題の本かと思わせるようなタイトルだが、これは数学者・統計学者であるJohn W. Tukey\footnote{我々が慣れ親しんでいるビットという言葉も、高速フーリエ変換というアルゴリズムも、統計学でお馴染みの箱ひげ図も、すべてTukeyが生み出したものである。彼の偉大さについてはほんの一瞥で十分であろう。}が1962年に書いた論文である。この論文は、仮説検定という方法論を偏重した当時の統計学に対する警鐘と言って良い。

『パターン認識と機械学習』の目次を眺めるだけで、これまでに色々な機械学習のアルゴリズムが開発されてきたことが見て取れるように、基礎的な統計学の教科書をパラパラとめくるだけで、面食らうほど様々な統計学的仮説検定に出くわすはずだ。しかしTukeyはそうした手法やツールを編み出す理論ばかりではなく、現実のデータを扱う探索的データ解析（exploratory data analysis）もまた統計学において重視されなくてはならないということを主張した。

そのために彼が指し示したのは、統計学を「手法の集合」ではなくて「問題の集合」と捉えるというパラダイム転換であった。"Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise."というメッセージが脱構築の引き金を引く。意訳をするとしたら、「的はずれな問いに的確に答えようとするぐらいなら、ズレた答えで正しい問いに挑む方がずっといい」といったところだろうか。

もし我々が的はずれな問いに的確に答え続けようとしているなら、我々はまさに知識発見という苦役を課された「不幸な道化」なのである。データベース・機械学習・統計学の間で走り回っても、この苦役は永遠に終わらない。悪夢から逃れ、穏やかで幸福なプレイグラウンドに辿り着くために必要なのは、的確な答え方ではなくて的確な問いなのである。的確な問いというのは、我々がまだ全貌を把握していないだけで現実のあちこちに埋まっている。だから、最も避けなければならないのはその宝探しの場が閑散とすることだ。

方法論の世界に留まる者たちの才能と覚悟に敬意を払いつつ、我々はいそいそとデータベース・機械学習・統計学から抜け出して、新たなプレイグラウンドへ向かう。大規模データに潜むパターン解析を切り捨てることが、問題意識に潜むパターン解析を始めるための必要条件だ。「真の喜ばしいの遊戯の場」はデータの外側、かつ認識の内側にある。そこで我々は現実世界の種々の問題を拾い集め、データの集合でも手法の集合でもなく、問題の集合を手に入れる。データの中のパターンを洗い出す代わりに、複数の問題に潜むパターンを浮き彫りにする。そうして我々は問題解決手法に生き生きとした明確な目的を与え、それらが望ましい形で応用されるような秩序を作り出す。そのとき我々はもう苦役を課せられた「不幸な道化」ではなくなっているはずだ。

\section{おわりに}
本稿の主張を思い切って標語的に纏めてしまえば、《data analysisからproblem analysisへ》ということになるだろう。この方向性は1962年にTukeyが"The Future of Data Analysis"の中で描いたもののはずだが、2012年の終わりを迎える今日になっても実現されていない。

本稿はproblem pattern analysisという方向性に向けた一つの試論である。データベース・機械学習・統計学を超えたパターン解析が「未来」から「現在」のものになり、この試論が完全に「過去」のものになるとき、きっと我々は悪夢を笑い飛ばせるような存在になっている。

%、手法に固執しすぎては現実の問題をいつまでも解決できないかもしれない。それぞれの道具が現実世界でその真価を発揮して人間を幸せにするために、手法ではなくて問題を重視する発想にそろそろ切り替えなければならないのだと思う。
%\begin{quote}
%"Those who ignore Statistics are condemned to reinvent it." --- Bradley Efron
%\end{quote}