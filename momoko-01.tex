% -*- coding: utf-8 -*-

\chapter{パターン認識と機械学習の「応用」}
% * (アスタリスク)付きの \chapter* コマンドは原則不可とする

\begin{flushright}
 早水　桃子 % ペンネーム
\end{flushright}

\begin{spacing}{0.6}
\noindent
{\footnotesize{本稿は、データサイエンスや統計学の専門家ではない筆者が
The Database Times vol.2のために気儘に執筆し、無責任に寄稿したものである。
本稿で述べる内容は、出典が明記されていない限りは全て
筆者の私見であり、それは他の個人や団体の主張を採用したものではなく、また
各分野の専門家の見識と一致するとは当然限らないということを予め宣言しておく。}}
\end{spacing}
 
\section{来し方行く末}
\subsection{ブームとしての学問}
『解体新書』は、江戸時代の医師たちの試行錯誤によって『ターヘル・アナトミア』というオランダ語の解剖学書から翻訳され、1774年に出版された歴史的書物である。まだオランダ語の辞書すらなかった時代であることを考えると、本文・図表合わせて五冊から成る医学書をわずか数年で翻訳・出版したというだけで大変な偉業である。しかし、この本の功績は、当時鎖国政策をとっていた日本に初めて西洋の医学知識を伝導し、それまでの日本の医学の常識を覆したところにある。
『解体新書』以前の日本においては解剖はほとんど行われていなかったため、人体は「五臓六腑」という東洋医学の概念によって捉えられていた。今でこそ我々は人体を色々な臓器の解剖学的な構造と生理学的な機能が作るシステムとして捉えているが、この枠組みを日本に初めてもたらしたのは、『解体新書』に他ならない。

『解体新書』は医学にとどまらず、ヨーロッパで発展した様々な学問から新しい知識や技術を吸収するという「蘭学（洋学）」ブームの契機になり、近代日本科学史に金字塔を打ち立てることとなった。
しかし、『解体新書』の翻訳者の一人である杉田玄白は、彼が85歳で亡くなる2年前に著した『蘭学事始』（1815年）という回想録の冒頭において、このブームを少々冷めた視線で眺めている。
%蘭学草創期の出来事を正しく伝えるために、85歳で亡くなる2年前の1815年に『蘭学事始』という回想録を著し、その冒頭でこのように述べている。

\begin{quote}
今時（きんじ）、世間に蘭学といふ事専ら行（おこな）はれ、志を立つる人は篤く学び、無識なる者は漫（みだ）りにこれを誇張す。
\end{quote}

時代を問わず、ブームには功罪がある。昨今のデータサイエンスをとりまく流行も、恐らく例外ではない。『解体新書』が現代医学への道を切り拓き、やがて蘭学が社会現象の域にまで達したように、
初の網羅的な解剖学アトラスのような『パターン認識と機械学習』の大ヒットもまた、機械学習・データマイニングといった分野の大流行を物語る。
ジョン・ネイスビッツは『メガトレンド』（1982年）という著作の中で「私たちは情報の海に溺れ、知識に飢えている」と見事に評したが、
%We are drowning in information but starved for knowledge. 
「情報爆発」や「ビッグデータ」へのソリューションが声高に語られ、それに煽られるかのようにバズワードに群がる烏合の衆は今日も騒がしいツイートをやめない。

無論、目新しい分野に期待を寄せるのは当然あるべき知的好奇心に違いないのだが、
パターン認識と機械学習の学習\footnote{特定の書籍に言及しているわけではない、と白々しくトボケておく。}をしなければ何となく乗り遅れたような気分にさせられたり、とりあえず機械学習で何かをすればそれが何であっても人目を引くことが出来たり、データの海に溺れる者にデータマイニングを謳った藁を差し伸べる有象無象の輩が現れるのは、最先端の花形的研究分野の活気と言うよりはむしろ、ゴールドラッシュ\footnote{金が採掘された場所に一攫千金を目論む採掘者が殺到すること。1848年にカリフォルニアの川で砂金が発見されたことによるカリフォルニア・ゴールドラッシュが有名であるが、金の鉱脈が見つかるたびに世界各地で幾度となく繰り返し見られる現象である。}を彷彿とさせる熱病と呼ぶべきものだろう。

\subsection{メルトダウンするブームと心中しないために}
多少の良識を持ち合わせているのなら、その熱気にのぼせて付和雷同してはいられないはずだ。現在のデータサイエンスをとりまく熱気は単なる空騒ぎに浪費されるかもしれないし、これまでに提案されてきた数々の理論や技法は、現実の問題解決に対する有用性や意義が不明瞭なまま忘れ去られ、それを実装したコードも無用の長物となるかもしれない。データ解析に求められるソフトウェア・ハードウェアに投資しても望む結果が何も得られなかった場合、ユーザーの少々無理のある過剰な期待はやがて失望に変わり、それが集団的な失望へ変わる時にブームは終焉を迎え、最悪の場合は学問分野そのものが廃れていくかもしれない。
また、新しい考えや技術は受け入れられるどころか、脅威と見なされて排除されることも珍しくない。実際、蘭学ブームは蘭方医学と漢方医学の深刻な対立を招き、蘭学は蘭書翻訳取締令などによって思想弾圧の憂き目を見ることになったし、産業革命は機械による自動化・作業効率化をもたらした一方で、失業を恐れた労働者集団は機械を破壊するという行動に出た\footnote{この機械打ち壊し運動（ラッダイト運動）になぞらえて、雇用を不安定にするIT化・自動化に反対する思想はネオ・ラッダイトと呼ばれている。}し、産業廃棄物の処理という新たな問題を生じさせた。

だからといって、危機感に振り回されて時代の潮流に乗ることを拒絶するのも賢明ではないだろう。したがって、本稿において貫きたいスタンスはまさしく「自ら『濁れる世』の只中をうろつき、危険に身をさらしつつ、しかも、批判的な姿勢を崩さぬことである。対象と深くかかわり全面的に没入すると同時に、対照を容赦なく突き放し切って捨てること。同化と異化のこの鋭い緊張こそ、真に知と呼ぶに値するすぐれてクリティカルな体験の境位であることは、いまさら言うまでもない。簡単に言ってしまえば、シラケつつノリ、ノリつつシラケること、これである」\footnote{浅田彰の『構造と力―記号論を超えて』（1983年）のあまりにも有名な一節。}。

持て囃されている道具を信仰したり、道具を使うこと自体が目的になってしまったり、道具を数式やプログラムの上だけで理解したり、あるいは実装して再生産しただけで満足してしまったり、どの道具が優れているのかという格付けに一喜一憂したりするような態度は全てこのスタンスに反する。肝に銘じておくべきは、どんなに高尚な理論や技法も、現実世界の問題に挑む人間にとっては目的を叶えるために使える道具の一つにすぎず、目的のために道具を使う人間があくまでも主役ということである。道具の長所や短所や使い道、他の道具とどのように関わると有用か、どのような道具がどのような局面で必要とされているのかは、ショーケースの中の道具を鑑賞しているだけではちっとも分からず、実際に使ってみて漸く見えてくるものである。

あまりにも当然なことであるのだが、データベースからの知識発見\footnote{Knowledge discovery in databasesの頭文字を取ってKDDとも呼ばれる。}という舞台を観客として見ていると、大規模データベース・統計学・機械学習という華やかな役者たちが全てであるかのように錯覚しがちである。しかしそうした「道具」たちは何ができて何ができないのか、どのように互いに関わり、どこでどのように応用すれば現実世界で真価を発揮するものになるのかというストーリーは、
人間や社会の問題意識やゴール設定なしに始まるはずがないし、終わるはずもない。

\section{データマイニング再考}
まずはJerome H. Friedmanの"Data mining and Statistics: What's the connection?"を下敷きにして、データマイニングとは何であるのかを
批判的に捉え直してみよう。

\subsection{データマイニングとデータベース}
データから知識を得ようとするというのは、全然目新しくない。では、何が今更データマイニング「特需」をもたらしたのだろうか。この問いかけに答える鍵の一つは、データベースマネジメントシステム（DBMS）の変遷である。
%Imielinski（1995年）

かつてのデータベースマネジメントシステムにおいては、銀行のATMで利用されているようなオンライントランザクション処理（OLTP）と呼ばれる情報処理が主流であった。つまり、データベースマネジメントシステムに求められていたのはデータを貯蔵し、多くのユーザーからの定型クエリーを高速に処理し、小さなデータを高速に読み書きして更新するという能力だった。
%http://otndnld.oracle.co.jp/deploy/performance/pdf/Oracle8i-tuning2.pdf
しかしデータベースに蓄積されたデータを様々な切り口で分析したいエンドユーザーたちが
データベースに意思決定支援を求めるにつれて、データベースマネジメントシステムにおいて
オンライン分析処理（OLAP）が注目を集めるようになった。OLAPは、ユーザーが対話式の試行錯誤を繰り返すことによって、データウェアハウスに格納された膨大なデータを集計したり、多次元的に分析したりすることを可能にした。

一見するとこれはデータマイニングツールの原型のように見えるかもしれない。しかし、OLAPによるデータ解析によってデータウェアハウスと「対話」できるのは、的確な仮説を考え続けるような不断の洞察力とクエリーを書き続ける技術力（および時間と体力）を持ち合わせた選ばれし人間だけである。データに潜むパターンやモデルの叩き台を自動的に見つけてくれるものではないし、ビジネスの場における迅速な意思決定に向いているものでもない。だからこれは
データマイニングツールの原型と言うよりはむしろ、人々がデータマイニングツールを求める素地を作ったと考えたほうが良いだろう。
%大規模なデータからパターンを見出す試行錯誤のプロセスが人間の手を煩わせずに実現するならばそれが効率的であるし、data analysis for everyoneみたいなものへの需要は高まっていった。

\subsection{プロパガンダとしてのデータマイニング}
データマイニングもまた他のバズワードの例にもれず定義が曖昧な言葉であるが、
複雑で膨大なデータ集合の中に埋もれていて人間の力では見いだせないような有益な知識をコンピューターの力でデータベースの中から探索的に発見し、ビジネスにおける意思決定に役立てるような色々な手法（ないし、それらを使ったプロセス）という意味で専ら使われており、
多くの場合においてはIBM Intelligent miner、SAS Enterprise Miner、SPSS Clementineなどの汎用データマイニングツールを使うことによって実践されるものである。

データマイニングの目論見は壮大なものであるが、
%データから新たな知見を得ようとする試みは全然斬新なものではない。
ユーザーが小難しいデータベースの構造やデータ解析の中身を知らなくてもデータベースに格納されたデータにアクセスし、それを分析したり視覚化することを可能にするというデータマイニングツールというものは、
誰のためのものなのだろうか。
「データ自身に語らせる」といえば聞こえは良いが、ビジネス戦略のヒントとなるような知識を囁くかどうかは別の話であるし、あまりそういううまい話は聞かない。Friedmanが指摘するように、汎用データマイニングツールは非常に高額な商用ソフトウェアであり、導入するとなるれば当然相応の高価なハードウェアやストレージも要求する。その意味で、データマイニングはもはや金脈を掘り当てようとする宝探しのような試みと言うよりはむしろ、データマイニングと聞いてソフトウェアやハードウェアに投資してくれるような、データを持て余している企業の意思決定者や潜在的データマイナーを発掘するための商業的プロパガンダと化しているところがある。

\subsection{データマイニングと統計解析の違い}
Friedmanは汎用データマイニングツールが提供する主要な手法の数々を
%\footnote{Decision tree induction・rule induction・nearest neighbors・clustering・association rules・feature extraction・visualisation、neural networks・graphical models（Bayesian belief networks）・遺伝的アルゴリズム・SOM（自己組織化マップ）・neuro-fuzzy systemsと}
を列挙し、多くのデータマイニングツールにおいて
仮説検定や判別分析など、統計学者にとって見慣れた手法は
殆ど無視されていると評している。一言で「データを解析する」と言っても、データマイニングと統計解析は目的も手段も大きく異なるものであり、両者を混同してはならない。

統計学的なデータ解析は、「データ自身に語らせる」という大らかなものではない。
「実験が終わった後に統計学者にコンサルトするのは、死後に解剖を依頼するようなもので、統計学者はその実験の死因を教えることぐらいしかできない」というロナルド・フィッシャーの言葉の通り、
そもそも統計学者にとってはデータは綿密な実験デザインに基づいて注意深く収集されたり吟味されるべきものであり、
鉱山のように元々そこにあるものではないのだ。
それに、データを説明するための仮説やモデルがなければ、仮説検定\footnote{帰無仮説を棄却するか否かという仮説検定は無味乾燥なものだと思うかもしれないが、少なくとも学術研究の分野においては最もよく使われる意思決定アルゴリズムである。}もモデル選択\footnote{モデルのパラメータを最尤推定する「モデル推定」とともに、どのモデルを選ぶべきかという「モデル選択」は機械学習（特に教師あり学習）においては最も重要なテーマの一つである。モデル選択基準はAICやBICなど色々なものがあり、クロスバリデーションもよく使われる。}も始めようがない。だから、良質なデータと洗練されたモデルこそ、統計学的なデータ解析の要であるのだ。その営みにおいてアルゴリズムというのはそのモデルを採用するか否かの基準を計算するための方法にすぎない。だから、モデルが正しければ良い結果を出し、正しくなければ悪い結果を出すのがここでのアルゴリズムのあるべき姿であるし、解析結果の善し悪しはアルゴリズムではなく、モデルが決めていると考えるのが筋である。

これに対しデータマイニングは、良く言えば従来の統計学が扱えないような整理されていない大規模データを何とか使える形にし、色々な切り口で分析して意思決定のヒントになる知識を見出そうとする果敢な取り組みであり、悪く言えば産業廃棄物のような膨大なデータの山をゴミだと認めるわけにいかないからといって、往生際悪くデータのフォーマットを整え、クレンジング\footnote{データの誤りや重複を洗い出し、使えるデータにするという泥臭い（しかし最も重要な）プロセス。}を施して場当たり的に試行錯誤することにより、有象無象の仮説やモデルの叩き台をあれこれ捻り出すという作業である。

いずれにせよデータマイニングは統計学とは全く異なる思想を持つものであり、仮説やモデルなしに鉱山の中を探索するデータマイニングにおいてよく登場する手法が、統計学の手法と異なるのは驚くべきことではないし、データマイニングが語られる文脈においてはテクニックやアルゴリズムありきの話になりがちであるのも、当然の成り行きなのかもしれない。

\subsection{データマイニングの意義}
しかしながら
%データマイニングに失望したり、ベンダーやコンサルティングファームに対して猜疑的になったり、
データマイニングの努力は水泡に帰するのかと悲観する必要はないし、それは本稿における我々のスタンスではない。むしろそこでの努力を無にしないために、試行錯誤の中に埋もれた教訓を探し、データマイニングが人間にとって有用であるためにはどうあるべきかを考えなければならない。

データマイニングは大規模なデータセットになるほど前処理が重要であることを実証してきたし、そのためのプラクティカルな方法を確立してきた。データの可視化は迅速な意思決定の取っ掛かりとして確かに強力であるし、現にそういったツールが多くの人々に求められているということは、人間が扱わなければならないデータが確かに複雑になっているということを物語っている。
また、様々なテクニックを集めたツールとデータ解析の専門家だけでは解析結果は荒唐無稽なものになりがちであったが、それはデータマイニングは役立たないと切り捨てる理由にはならず、むしろデータ解析には現場の知識を持った専門家の知識が必須であるということを改めて思い知らせてくれたと考えるべきであるし、その方がよほど建設的である。

また、大規模データベースを持っているのは企業だけではないし、
大規模なオープンデータベースも数多く存在する。
そのようなデータベースから知識を発見する試みにおいて、データマイニングは単なる商業的なプロパガンダに堕したものではなく、真理の探求を目指す学術研究において仮説やモデルのプロトタイプを見つける手段となったり、
蓄積されたデータから合理的に最適な政策を決定するという国家や国際社会の意思決定を支援するためのツールとなり得るものである。

データマイニングは、特徴選択・特徴抽出・クラスタリング・決定木学習など、実に様々なパターン認識・機械学習の理論的な枠組みを現実の大規模データに応用してきた。そうした理論と現実の橋渡しをする取り組みは軽視されてはならないし、その価値は商業的な実用性や理論的な厳密さや一般性だけで評価されるべきではない。そうでなければデータマイニングバブル崩壊とともに、データベースから知識を発見することの重要性も失われかねないからだ。

\section{データベースの外の世界}
データベースのことばかり考えていると、統計学もパターン認識も機械学習もデータベースを掘り起こす道具にしか見えなくなるかもしれないから、それ以外の側面についても少しだけ触れてから先へ進むことにしよう。
\subsection{医療機器}
統計学や機械学習で登場するアルゴリズムの幾つかは、医療の現場で人々の健康な生活や生命を支えている計算手法と言っても過言ではない。医用画像を得るためには画像再構成という重要なプロセスがあり、そこではバックプロパゲーションやEMアルゴリズムといった機械学習・統計学でよく知られる手法が一般的に用いられている。
また、病気の検査のためだけでなく、
がんの放射線治療においてはモンテカルロ法が古くから線量分布推定に用いられ、
今もなお放射線治療計画に必須の計算手法であり続けている。

医療現場で使われる装置やプログラムに関わるごく一部の例であるが、
統計学や機械学習の、データマイニングで期待される役割とは少し違う一面を窺い知ることが出来るだろう。

\subsection{神経科学}
もともと機械学習は人工知能研究の一分野であることから分かるように、理論神経科学と深く関係しており、その理論的な枠組みそのものが、人間の脳のメカニズムや、社会における人間の行動などを理解する一助となり得るものである。パーセプトロンは古くから小脳のモデルとして知られ、人間の運動学習の神経基盤を理論的にも実験的にも明らかにし、ロボットの運動制御という工学的な応用にもつながることとなった代表的な例である。
より最近の例としては、やや趣味的になるが、ドーパミンニューロンが報酬予測誤差によって強化学習を行っているのではないかという仮説が提唱され、強化学習の中でも特にTDモデルが大脳基底核の計算モデルとして注目を集め、報酬に基づく意思決定の神経基盤が理論的にも実験的にも精力的に研究されている\footnote{そしてその実験のためには医用画像処理・画像の統計解析が必要不可欠なのである。}。これらは薬物やギャンブルなどの依存症やパーキンソン病という神経変性疾患の病態解明もに密接に関係し、今後人間の衝動制御についての研究はますます進むと思われる。

こうした研究は非常に興味深く、また有益でもあるのだが、本稿ではもう少し現実的で差し迫った問題を題材にして考察を深めることにする。

\section{脳は壊れるという現実}
日本人の平均寿命は
世界の先進国の中でもトップクラスであり、それは高度に発達した現代の医学と、充実した医療サービスを誰でも受けることが出来るようにするための国民皆保険制度の賜物である。しかし日本を筆頭に世界各国（特に先進国）で社会の高齢化が問題になっていることは周知の通りであるし、増え続ける認知症患者は医療費の増加により国の財政を圧迫し、若い世代にも大きな負担を強いている。また、生涯現役が当たり前になりつつある中で、認知症によって本来のパフォーマンスを発揮できなくなり、生活の質が下がっていくのは
個人の人生にとっても悲劇的であるし、その個人を取りまく人々の生活にもかなりの打撃を与える。

\subsection{背景知識の要点}
認知症というのは認知機能低下を呈する多くの疾患の総称であり、単一の疾患を指すものではない。その中でも一番多くて重要なのは誰でも一度は聞いたことがあるであろうアルツハイマー病である。

\subsubsection{アルツハイマー病はハードウェアの異常である}
コンピュータの調子が悪くなったら、まずはハードウェアレベルの異常なのかソフトウェアレベルの異常なのかを考えるのが自然な流れであるように、長年使ってきた脳がうまく動かなくなる場合も、ソフトウェアレベルの原因と、ハードウェアレベルの原因とがある。

認知「機能」の障害と聞くとソフトウェアの障害のように思うかもしれないが、アルツハイマー病は正常な脳の「構造」が壊れることにより機能に支障が出る病気で、ハードウェアの著しい劣化による脳の故障と考えるべきものである。具体的には正常な人でも加齢で蓄積するゴミのようなもの\footnote{ゴミの正体について少しだけ詳しく述べれば、アルツハイマー病は脳内に$\beta$アミロイドとよばれるタンパク質が蓄積し、異常リン酸化タウタンパクが蓄積してしまうことで神経細胞の変性・機能不全が起こり、神経細胞死を招く疾患である。それが肉眼的・画像的な脳萎縮という初見となる。}が異常に脳に溜まってしまうために神経細胞がどんどん死んでしまい、脳のボリュームが目に見えて減ってしまうのである。

\subsubsection{アルツハイマー病の根本治療薬とワクチン開発への動き}
ハードウェアの障害であると繰り返したのは、病気が進行して重症化してからでは有効な治療法がないことと、MRI画像で脳の形やボリュームを見ることが診断に有効であることの二点を強調するためである。
壊れかけの初期のうちに食い止めることが出来るなら画期的であるし、もっと言えば最初から認知症症状が出ないように出来たら理想的である。現在多くの巨大な製薬会社や医療機器メーカーがベンチャー企業とも協力し、アルツハイマー病の根本治療薬やワクチン開発を目指して研究開発に取り組んでおり、実際に日本でも複数の治験が進行中である。

\subsection{ADNIデータベース}
根本治療薬・ワクチン開発への動きとともに、脳の異常の客観的な評価方法や、早期診断手法の重要性は急速に高まった。
2004年にアメリカはUS-ADNI（米国アルツハイマー病神経画像イニシアチブ）という国家的な臨床研究プロジェクトをスタートさせた。これは800人以上の被験者を6ヶ月毎にスキャンして集めた画像データ\footnote{
MRIの構造画像だけでなく、FDG-PETという機能画像や発症前診断のためのPiB-PETによるアミロイドイメージングも含む}をはじめ、色々な生物学的マーカーや臨床的な様々な因子の情報を集めて認知症研究のための大規模データベースを作り、web上で公開するという壮大な\footnote{初期の予算だけで6000万ドル。}認知症研究プロジェクトである。画像データの質は施設ごとの撮像方法や装置そのものに大きく影響されるため、多施設から得られたデータをそのまま解析することは出来ないのだが、ADNIでは
撮像された画像データはそのような違いを補正するための前処理を施され、匿名化された後、翌日にはwebサイトにアップロードされる。

このADNIというイニシアチブはヨーロッパ・オーストラリア・日本・台湾・韓国でもスタートし、日本でもJ-ADNIという名前で2007年からプロジェクトがスタートしている。
US-ADNIはさらにアルツハイマー病の早期診断・発症予測に焦点を当てたADNI 2に乗り出し、また被験者の全ゲノムシークエンスをも公開するビッグデータプロジェクトに変貌しつつある\footnote{もし800人の被験者の全ゲノム配列が得られたらデータベースに追加されるデータのサイズは少なくとも165テラバイトになる。}。

もちろんUS-ADNIのデータは研究者ではなくても世界中の誰でも自由に無料でダウンロードすることができる。研究者たちはこのデータをいかなる形で学術利用しても良く、実際に多くの論文が既に発表されている。

\subsubsection{ADNIデータベースと機械学習}



%アルツハイマー病の確定診断をするためには顕微鏡で実際に「ゴミ」が溜まっているのを見なければならないので、手術室で生きている脳から組織のサンプルを採取することが出来た場合を除けば、真の答えは死後の解剖なしには厳密には分からない。
とはいえアルツハイマー病の臨床診断は日常的に滞りなく行われていて、殆どの臨床診断は真の答えと一致している。そもそも大抵の専門家は、症状や経過のパターン\footnote{本人を診察したり家族の話を聞いたり認知機能を調べる簡単なテストをして認知機能をスコア化したりする中で明らかになる。}を知った時点でアルツハイマー病かどうかをあらかた察することができるのだが、
病態についての生物学的・医学的な研究が進んだ現在、様々な検査によって非常に多くの重要な情報を得ることが出来るようになったからである。中でも、脳というハードウェアの形態を見るMRI、目に見えない脳機能をラジオアイソトープによって画像化するというSPECTやFDG-PET、または脳に溜まった「ゴミ」自体を視覚化するPiB-PETなどによる画像診断研究の進歩は目を見張るものがある。


%診断において厄介なのは、認知機能検査の点数が正常とも異常ともつかないような「ボケ気味」の人々であり、こういう白とも黒ともいえないグレーな人々は軽度認知機能障害（MCI）であるのだが、MCIと診断された人々を経時的に追跡してその運命を調べると、決定的に異なる2種類のグループに分かれることが明らかになった。年月を経てもあまり変化がなく少々ボケ気味とはいえ平穏な人生を送ることができるグループと、MCIと診断されてからみるみるうちにアルツハイマー病の診断基準を満たすレベルになり、数ヶ月～数年で変わり果ててしまうグループである。前者はMCI non-converterと呼ばれ、後者はMCI converterと呼ばれる。グレーの中には黒になりつつあるグレーが隠れていたということだ。


\subsection{脳画像統計解析と診断支援ソフトウェア}
自動診断ソフトウェアは日常的に使われており、
日本ではVSRAD\footnote{健常高齢者のデータベースからどのぐらい離れているかということをボクセル毎に検定してZ値を算出し、自動的に診断をする。}という無償のソフトウェアが普及し、1000以上の施設でルーチン化している。
